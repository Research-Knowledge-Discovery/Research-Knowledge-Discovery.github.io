<h3 id="bib:year-2020" class="bibsonomy_quicknav_group"><a name="2020">2020</a></h3>
<div style="margin-bottom:1em">
<b>Linguistic vs encyclopaedic knowledge : classification of MWEs from Wikipedia articles</b>. <br/>
<i>CYBERNETICS AND INFORMATION TECHNOLOGIES</i>, 20(4):125-140, 2020.

<br/>
Z. Kancheva and I. Radev.
<br/>

<a onclick="toggleAbstract('lepsky', '904fdad4a7dd9e18975a20c3d3800eb3'); return false;" href="https://www.bibsonomy.org/bibtex/2904fdad4a7dd9e18975a20c3d3800eb3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '904fdad4a7dd9e18975a20c3d3800eb3', 'https://www.bibsonomy.org/bibtex/2904fdad4a7dd9e18975a20c3d3800eb3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2904fdad4a7dd9e18975a20c3d3800eb3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_904fdad4a7dd9e18975a20c3d3800eb3lepsky" style="display:none;border:1px dotted grey;">
This paper reports on the first steps in the creation of linked data through the mapping between the synsets of BTB-WordNet and the articles in Bulgarian Wikipedia. The task of expanding the BTB-WordNet with encyclopaedic knowledge is done by mapping its synsets to Wikipedia articles with many MWEs found in the articles and subjected to further analysis. We look for a way to filter the Wikipedia MWEs in the effort of selecting the ones most beneficial to the enrichment of BTB-WN.
</div>
<div style="position:relative">						
	<div id="bib_904fdad4a7dd9e18975a20c3d3800eb3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Detecting multiword expression type helps lexical complexity assessment</b>. <br/>
<i>arXiv:2005.05692 [cs]</i>, 2020.
arXiv: 2005.05692
<br/>
Ekaterina Kochmar, Sian Gooding and Matthew Shardlow.
<br/>
<a href="http://arxiv.org/abs/2005.05692">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '9091605c7c01cedb86328d71e57809bb'); return false;" href="https://www.bibsonomy.org/bibtex/29091605c7c01cedb86328d71e57809bb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '9091605c7c01cedb86328d71e57809bb', 'https://www.bibsonomy.org/bibtex/29091605c7c01cedb86328d71e57809bb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/29091605c7c01cedb86328d71e57809bb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_9091605c7c01cedb86328d71e57809bblepsky" style="display:none;border:1px dotted grey;">
Multiword expressions (MWEs) represent lexemes that should be treated as single lexical units due to their idiosyncratic nature. Multiple NLP applications have been shown to benefit from MWE identification, however the research on lexical complexity of MWEs is still an under-explored area. In this work, we re-annotate the Complex Word Identification Shared Task 2018 dataset of Yimam et al. (2017), which provides complexity scores for a range of lexemes, with the types of MWEs. We release the MWE-annotated dataset with this paper, and we believe this dataset represents a valuable resource for the text simplification community. In addition, we investigate which types of expressions are most problematic for native and non-native readers. Finally, we show that a lexical complexity assessment system benefits from the information about MWE types.
</div>
<div style="position:relative">						
	<div id="bib_9091605c7c01cedb86328d71e57809bblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Computational phraseology</b>.<br/>
2020. Google-Books-ID: 7qndDwAAQBAJ.
<br/>Gloria Corpas Pastor and Jean-Pierre Colson.
<br/>

<a onclick="toggleAbstract('lepsky', '42024a1372705b49d6aeb9b4637d37ae'); return false;" href="https://www.bibsonomy.org/bibtex/242024a1372705b49d6aeb9b4637d37ae/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '42024a1372705b49d6aeb9b4637d37ae', 'https://www.bibsonomy.org/bibtex/242024a1372705b49d6aeb9b4637d37ae/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/242024a1372705b49d6aeb9b4637d37ae/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_42024a1372705b49d6aeb9b4637d37aelepsky" style="display:none;border:1px dotted grey;">
Whether you wish to deliver on a promise, take a walk down memory lane or even on the wild side, phraseological units (also often referred to as phrasemes or multiword expressions) are present in most communicative situations and in all world’s languages. Phraseology, the study of phraseological units, has therefore become a rare unifying theme across linguistic theories.In recent years, an increasing number of studies have been concerned with the computational treatment of multiword expressions: these pertain among others to their automatic identification, extraction or translation, and to the role they play in various Natural Language Processing applications. Computational Phraseology is a comparatively new field where better understanding and more advances are urgently needed. This book aims to address this pressing need, by bringing together contributions focusing on different perspectives of this promising interdisciplinary field.
</div>
<div style="position:relative">						
	<div id="bib_42024a1372705b49d6aeb9b4637d37aelepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Building wordnets with multi-word expressions from parallel corpora</b>. <br/>
<i>Procesamiento del Lenguaje Natural</i>, 64(03):45-52, 2020.
Accepted: 2020-03-29T15:04:33ZPublisher: Sociedad Española para el Procesamiento del Lenguaje Natural
<br/>
Alberto Manuel Simões and Xavier Gómez Guinovart.
<br/>
<a href="http://rua.ua.es/dspace/handle/10045/104713">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '0da8a00a4708cd79ac3f8566b633fa37'); return false;" href="https://www.bibsonomy.org/bibtex/20da8a00a4708cd79ac3f8566b633fa37/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0da8a00a4708cd79ac3f8566b633fa37', 'https://www.bibsonomy.org/bibtex/20da8a00a4708cd79ac3f8566b633fa37/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20da8a00a4708cd79ac3f8566b633fa37/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0da8a00a4708cd79ac3f8566b633fa37lepsky" style="display:none;border:1px dotted grey;">
In this paper we present a method for enlarging wordnets focusing on multi-word terms and utilising data from parallel corpora. Our approach is validated using the Galician and Portuguese wordnets. The multi-word candidates obtained in this experiment were manually validated, obtaining a 73.2%accuracy for the Galician language and a 75.5%for the Portuguese language.
</div>
<div style="position:relative">						
	<div id="bib_0da8a00a4708cd79ac3f8566b633fa37lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A study of semantic projection from single word terms to multi-word terms in the environment domain</b>.<br/>
In: 

<i>LREC 2020 Workshop Language Resources and Evaluation Conference 11–16 May 2020</i>, pages 50.
2020.

<br/>
Yizhe Wang, Béatrice Daille and Nabil Hathout.
<br/>


<a onclick="toggleAbstract('lepsky', '6dde5e5e23c4509fad1cee8f4031b141'); return false;" href="https://www.bibsonomy.org/bibtex/26dde5e5e23c4509fad1cee8f4031b141/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6dde5e5e23c4509fad1cee8f4031b141', 'https://www.bibsonomy.org/bibtex/26dde5e5e23c4509fad1cee8f4031b141/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26dde5e5e23c4509fad1cee8f4031b141/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6dde5e5e23c4509fad1cee8f4031b141lepsky" style="display:none;border:1px dotted grey;">
The semantic projection method is often used in terminology structuring to infer semantic relations between terms. Semantic projectionrelies upon the assumption of semantic compositionality:  the relation that links simple term pairs remains valid in pairs of complexterms built from these simple terms. This paper proposes to investigate whether this assumption commonly adopted in natural languageprocessing is actually valid. First, we describe the process of constructing a list of semantically linked multi-word terms (MWTs) relatedto the environmental field through the extraction of semantic variants.  Second, we present our analysis of the results from the semanticprojection. We find that contexts play an essential role in defining the relations between MWTs.
</div>
<div style="position:relative">						
	<div id="bib_6dde5e5e23c4509fad1cee8f4031b141lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2019" class="bibsonomy_quicknav_group"><a name="2019">2019</a></h3>
<div style="margin-bottom:1em"><b>Representation and parsing of multiword expressions : current trends</b>.<br/>
2019. 
<br/>Yannick Parmentier and Jakub Waszczuk.
<br/>

<a onclick="toggleAbstract('lepsky', '75e16f82389da838e95dd0d64fc6e4d8'); return false;" href="https://www.bibsonomy.org/bibtex/275e16f82389da838e95dd0d64fc6e4d8/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '75e16f82389da838e95dd0d64fc6e4d8', 'https://www.bibsonomy.org/bibtex/275e16f82389da838e95dd0d64fc6e4d8/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/275e16f82389da838e95dd0d64fc6e4d8/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_75e16f82389da838e95dd0d64fc6e4d8lepsky" style="display:none;border:1px dotted grey;">
Deep parsing is the fundamental process aiming at the representation of the syntacticstructure of phrases and sentences. In the traditional methodology this process isbased on lexicons and grammars representing roughly properties of words and interactionsof words and structures in sentences. Several linguistic frameworks, such as HeaddrivenPhrase Structure Grammar (HPSG), Lexical Functional Grammar (LFG), Tree AdjoiningGrammar (TAG), Combinatory Categorial Grammar (CCG), etc., offer differentstructures and combining operations for building grammar rules. These already containmechanisms for expressing properties of Multiword Expressions (MWE), which, however,need improvement in how they account for idiosyncrasies of MWEs on the onehand and their similarities to regular structures on the other hand. This collaborativebook constitutes a survey on various attempts at representing and parsing MWEs in thecontext of linguistic theories and applications.
</div>
<div style="position:relative">						
	<div id="bib_75e16f82389da838e95dd0d64fc6e4d8lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2018" class="bibsonomy_quicknav_group"><a name="2018">2018</a></h3>
<div style="margin-bottom:1em">
<b>Evaluating distributional features for multiword expression recognition</b>. <br/>
In: P. Sojka, A. Horák, I. Kopeček and K. Pala, editors, <i>Text, Speech, and Dialogue</i>, series Lecture Notes in Computer Science, pages 126-134.
Springer International Publishing, 2018.

<br/>
Natalia Loukachevitch and Ekaterina Parkhomenko.
<br/>


<a onclick="toggleAbstract('lepsky', '4fb44e9a8bc6f0914f2ca19214a89d14'); return false;" href="https://www.bibsonomy.org/bibtex/24fb44e9a8bc6f0914f2ca19214a89d14/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4fb44e9a8bc6f0914f2ca19214a89d14', 'https://www.bibsonomy.org/bibtex/24fb44e9a8bc6f0914f2ca19214a89d14/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24fb44e9a8bc6f0914f2ca19214a89d14/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4fb44e9a8bc6f0914f2ca19214a89d14lepsky" style="display:none;border:1px dotted grey;">
In this paper we consider the task of extracting multiword expression for Russian thesaurus RuThes, which contains various types of phrases, including non-compositional phrases, multiword terms and their variants, light verb constructions, and others. We study several embedding-based features for phrases and their components and estimate their contribution to finding multiword expressions of different types comparing them with traditional association and context measures. We found that one of the distributional features has relatively high results of MWE extraction even when used alone. Different forms of its combination with other features (phrase frequency, association measures) improve both initial orderings.
</div>
<div style="position:relative">						
	<div id="bib_4fb44e9a8bc6f0914f2ca19214a89d14lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Recognition of multiword expressions using word embeddings</b>. <br/>
In: S. O. Kuznetsov, G. S. Osipov and V. L. Stefanuk, editors, <i>Artificial Intelligence</i>, series Communications in Computer and Information Science, pages 112-124.
Springer International Publishing, 2018.

<br/>
Natalia Loukachevitch and Ekaterina Parkhomenko.
<br/>


<a onclick="toggleAbstract('lepsky', 'a83e598ca033ac61425f9a21df75aa8b'); return false;" href="https://www.bibsonomy.org/bibtex/2a83e598ca033ac61425f9a21df75aa8b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a83e598ca033ac61425f9a21df75aa8b', 'https://www.bibsonomy.org/bibtex/2a83e598ca033ac61425f9a21df75aa8b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a83e598ca033ac61425f9a21df75aa8b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a83e598ca033ac61425f9a21df75aa8blepsky" style="display:none;border:1px dotted grey;">
In this paper we consider the task of extracting multiword expressions (MWE) for Russian thesaurus RuThes, which contains various types of phrases, including non-compositional phrases, multiword terms and their variants, light verb constructions, and others. We study several embedding-based features for phrases and their components and estimate their contribution to finding multiword expressions of different types comparing them with traditional association and context measures. We found that one of the distributional features has relatively high results of MWE extraction even when used alone. Different forms of its combination with other features (phrase frequency, association measures) improve both initial orderings. Besides, we demonstrate significant potential of an existing thesaurus for recognition of new multiword expressions for adding to the thesaurus.
</div>
<div style="position:relative">						
	<div id="bib_a83e598ca033ac61425f9a21df75aa8blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>MwTExt : automatic extraction of multi-word terms to generate compound concepts within ontology</b>. <br/>
<i>International Journal of Information Technology</i>:1-9, 2018.

<br/>
Pratik Thanawala and Jyoti Pareek.
<br/>
<a href="https://link.springer.com/article/10.1007/s41870-018-0111-6">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1ca534e612baafab7c9bd86c62329d88'); return false;" href="https://www.bibsonomy.org/bibtex/21ca534e612baafab7c9bd86c62329d88/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1ca534e612baafab7c9bd86c62329d88', 'https://www.bibsonomy.org/bibtex/21ca534e612baafab7c9bd86c62329d88/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21ca534e612baafab7c9bd86c62329d88/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1ca534e612baafab7c9bd86c62329d88lepsky" style="display:none;border:1px dotted grey;">
Multiword expressions are omnipresent element of natural language, whose construal as a linguistic resource has significant importance in various applications. This paper presents an architecture-MwTExt, for automatic extraction of multi-word terms-MWTs from such expressions within un-annotated English documents. Natural Language Processing techniques such as Shallow parsing and syntactic structure analysis are used to extract MWTs, with specific focus on lexical patterns as (Noun Preposition Noun), (Noun Preposition Noun + Noun) and (Noun Preposition Noun Preposition Noun). The MWTs extracted can be further used to form compound concepts within Ontology. The lexical descriptions of MWTs are encoded in Web Ontology Language OWL/XML. MwTExt has been tested on Computer Science domain texts, and the results obtained are compared with those obtained by Text2Onto, an Ontology learning tool and term extractors such as TermRaider and TerMine. The result signifies that MwTExt performs better for extraction of accurate lexicalized MWTs with average precision of 97
</div>
<div style="position:relative">						
	<div id="bib_1ca534e612baafab7c9bd86c62329d88lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2017" class="bibsonomy_quicknav_group"><a name="2017">2017</a></h3>
<div style="margin-bottom:1em">
<b>Combining linguistic features for the detection of croatian multiword expressions</b>. <br/>
<i>MWE 2017</i>:194, 2017.
bibtex: buljan2017combining
<br/>
Maja Buljan and Jan Šnajder.
<br/>
<a href="http://www.aclweb.org/anthology/W/W17/W17-17.pdf#page=206">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'a5f7db2b43cda9e19f60d16fdff6a299', 'https://www.bibsonomy.org/bibtex/2a5f7db2b43cda9e19f60d16fdff6a299/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a5f7db2b43cda9e19f60d16fdff6a299/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a5f7db2b43cda9e19f60d16fdff6a299lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_a5f7db2b43cda9e19f60d16fdff6a299lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic phrase indexing for document retrieval : an examination of syntactic and non-syntactic methods</b>. <br/>
<i>SIGIR Forum</i>, 51:51-61, 2017.

<br/>
Joel L. Fagan.
<br/>

<a onclick="toggleAbstract('lepsky', 'a6e2d75e0abdc0c4d298be76440e713c'); return false;" href="https://www.bibsonomy.org/bibtex/2a6e2d75e0abdc0c4d298be76440e713c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a6e2d75e0abdc0c4d298be76440e713c', 'https://www.bibsonomy.org/bibtex/2a6e2d75e0abdc0c4d298be76440e713c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a6e2d75e0abdc0c4d298be76440e713c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a6e2d75e0abdc0c4d298be76440e713clepsky" style="display:none;border:1px dotted grey;">
An automatic phrase indexing method based on the term discrimination model is described, and the results of retrieval experiments on five document collections are presented. Problems related to this non-syntactic phrase construction method are discussed, and some possible solutions are proposed that make use of information about the syntactic structure of document and query texts.
</div>
<div style="position:relative">						
	<div id="bib_a6e2d75e0abdc0c4d298be76440e713clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Detection of verbal multi-word expressions via conditional random fields with syntactic dependency features and semantic re-ranking</b>. <br/>
<i>MWE 2017</i>:114, 2017.
bibtex: maldonado2017detection
<br/>
Alfredo Maldonado, Lifeng Han, Erwan Moreau, Ashjan Alsulaimani, Koel Dutta Chowdhury, Carl Vogel and Qun Liu.
<br/>


<a onclick="toggleBibtex('lepsky', '1776b0ee8ee2e80bdacdb9656903e923', 'https://www.bibsonomy.org/bibtex/21776b0ee8ee2e80bdacdb9656903e923/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21776b0ee8ee2e80bdacdb9656903e923/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1776b0ee8ee2e80bdacdb9656903e923lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_1776b0ee8ee2e80bdacdb9656903e923lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A corpus-based analysis of genre-specific mult-word combinations : minutes in English and Spanish</b>.<br/>
In: 

<i>Cross-linguistic correspondences : from lexis to genre</i>, pages 221-252.
John Benjamins, Amsterdam, 2017.

<br/>
Isabel Pizarro Sánchez.
<br/>


<a onclick="toggleAbstract('lepsky', '9f45771fc126bd2532fa7ea732c20825'); return false;" href="https://www.bibsonomy.org/bibtex/29f45771fc126bd2532fa7ea732c20825/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '9f45771fc126bd2532fa7ea732c20825', 'https://www.bibsonomy.org/bibtex/29f45771fc126bd2532fa7ea732c20825/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/29f45771fc126bd2532fa7ea732c20825/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_9f45771fc126bd2532fa7ea732c20825lepsky" style="display:none;border:1px dotted grey;">
English and Spanish minutes both contain two vocabulary sets, one that codifies the ‘field’ and belongs in a given content area, and another that codifies the discursive practices of the genre ‘minutes’. This paper sets out to explore which multi-word combinations can be identified as genre- and step-specific, and what correspondences can be identified across languages. The study draws on an English–Spanish comparable corpus of meeting minutes, tagged on the rhetorical level. A comparable corpus browser with a basic statistic feature has been used to obtain step subcorpora and WordSmith Tools was used to obtain n-grams within rhetorical steps in each language. N-grams were classified as genre-specific, step-specific, field-related, function-word combination or noise. Empirical findings show that for each rhetorical move, irrespective of text ‘field’, a number of n-grams have become readily associated in each of the languages. Since word choice is determined by genre-bound expectations and by context, selections across languages are not obvious and correspondences show different grams and number of grams.
</div>
<div style="position:relative">						
	<div id="bib_9f45771fc126bd2532fa7ea732c20825lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>The“Raison d’Être”of word embeddings in identifying multiword expressions in bilingual corpora</b>.
<br/>
PhD thesis, Universität Zürich; Philosophische Fakultät; Institut für Computerlinguistik, Zürich, 2017.

<br/>
Philipp Ströbel.
<br/>
<a href="http://www.cl.uzh.ch/dam/jcr:e5acda09-2f04-477e-a405-2e57183519e8/Masterarbeit_PStroebel_HS2016.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '8ca2c9290a572c90abb6b9a11e54e3c2'); return false;" href="https://www.bibsonomy.org/bibtex/28ca2c9290a572c90abb6b9a11e54e3c2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '8ca2c9290a572c90abb6b9a11e54e3c2', 'https://www.bibsonomy.org/bibtex/28ca2c9290a572c90abb6b9a11e54e3c2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/28ca2c9290a572c90abb6b9a11e54e3c2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_8ca2c9290a572c90abb6b9a11e54e3c2lepsky" style="display:none;border:1px dotted grey;">
Multiword expressions do not only pose problems in linguistics, but also in natural lan- guage processing. Linguists, for instance, besides being concerned with finding a proper definition of this phenomenon, tackle multiword expressions from different angels, e.g., semantics, phraseology, or syntax. The computational linguist, at the same time, tries to get to grips with multiword expressions in parsing or statistical machine translation. It is especially the translation of multiword expressions to which we dedicate this thesis. Since statistical machine translation relies on word alignments and phrase tables, and are not always able to produce satisfactory results, we are on the lookout for other methods. A promising, rather novel technique are bilingual word embeddings. After having deter- mined the best bilingual word embedding model from a total of 48 candidates trained on different parameters for a German-French parallel corpus, we compare its performance against phrase tables and a multilingual concordance system which relies on word align- ments. The findings are sobering, since bilingual word embeddings cannot yet compete with traditional methods. Depending on the translation direction and the method it is compared to, the bilingual word embedding model provides correct translations in only 15.04%to 34 Nevertheless, we find that bilingual word embeddings are indeed able to capture similarities and relationships that go undetected by conventional methods, which is why we argue for further development of hybrid systems.
</div>
<div style="position:relative">						
	<div id="bib_8ca2c9290a572c90abb6b9a11e54e3c2lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2016" class="bibsonomy_quicknav_group"><a name="2016">2016</a></h3>
<div style="margin-bottom:1em"><b>Automatische Extraktion fachterminologischer Mehrwortbegriffe : ein Verfahrensvergleich</b>.
<br/>
PhD thesis, Universität Trier; Fachbereich II; Studiengang Computerlinguistik, Trier, 2016.

<br/>
Juliane Bredack.
<br/>

<a onclick="toggleAbstract('lepsky', 'e135a792340fa2022075feb1965c1f13'); return false;" href="https://www.bibsonomy.org/bibtex/2e135a792340fa2022075feb1965c1f13/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e135a792340fa2022075feb1965c1f13', 'https://www.bibsonomy.org/bibtex/2e135a792340fa2022075feb1965c1f13/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e135a792340fa2022075feb1965c1f13/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e135a792340fa2022075feb1965c1f13lepsky" style="display:none;border:1px dotted grey;">
Terminologieextraktion ist eine wichtige Aufgabenstellung innerhalb der Computerlinguistik. Die Literatur zum Thema ist zahlreich, die eingesetzten Verfahren stammen überwiegend aus den Bereichen POS-Tagging, Chunking, Parsing und Textstatistik. Einzeln oder in Kombination dürfen sie als das klassische Instrumentarium zur Terminologieextraktion gelten. Mit der wachsenden Bedeutung der Zielsetzungen in Richtung Semantik ist auch die Identifzierung und Extraktion von Mehrwortgruppen zunehmend interessanter und wichtiger geworden. Auch hier dominieren die „klassischen“ CL-Ansätze. Für das Deutsche und andere stark  flektierende Sprachen spielen schon immer auch wörterbuchbasierte Ansätze zur Sprachverarbeitung eine große Rolle. Geprägt durch das Einsatzgebiet einer automatischen Indexierung stehen Funktionen wie Lemmatisierung und Dekomposition im Fokus, zunehmend aber auch algorithmische und wörterbuchgestützte Verfahren der Mehrworterkennung. Vergleichende Untersuchungen zur Leistungsfähigkeit beider Ansätze sind nicht bekannt, wie überhaupt die Evaluierung von CL-Verfahren ein gerne vernachlässigter Zweig der Forschung ist. Hier setzt die vorliegende Arbeit an, die auf der Basis der Verarbeitung einer Referenzkollektion „klassische“ und wörterbuchgestützte Ansätze zur Mehrwortextraktion einem Verfahrensvergleich unterzieht. Sie befindet sich damit im Schnittbereich von Computerlinguistik und Informationswissenschaft.
</div>
<div style="position:relative">						
	<div id="bib_e135a792340fa2022075feb1965c1f13lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Dutch compound splitting for bilingual terminology extraction</b>.<br/>
In: 

<i>Multi-word Units in Machine Translation and Translation Technology</i>.
John Benjamins, 2016.

<br/>
Lieve Macken and Arda Tezcan.
<br/>

<a href="http://hdl.handle.net/1854/LU-7126122">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '429cca8b418832533cf52c3869ddd2ed'); return false;" href="https://www.bibsonomy.org/bibtex/2429cca8b418832533cf52c3869ddd2ed/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '429cca8b418832533cf52c3869ddd2ed', 'https://www.bibsonomy.org/bibtex/2429cca8b418832533cf52c3869ddd2ed/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2429cca8b418832533cf52c3869ddd2ed/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_429cca8b418832533cf52c3869ddd2edlepsky" style="display:none;border:1px dotted grey;">
Compounds pose a problem for applications that rely on precise word alignments such as bilingual terminology extraction. We therefore developed a state-of-the-art hybrid compound splitter for Dutch that makes use of corpus frequency information and linguistic knowledge. Domain-adaptation techniques are used to combine large out-of-domain and dynamically compiled in-domain frequency lists. We perform an extensive intrinsic evaluation on a Gold Standard set of 50,000 Dutch compounds and a set of 5,000 Dutch compounds belonging to the automotive domain. We also propose a novel methodology for word alignment that makes use of the compound splitter. As compounds are not always translated compositionally, we train the word alignment models twice: a first time on the original data set and a second time on the data set in which the compounds are split into their component parts. The obtained word alignment points are then combined.
</div>
<div style="position:relative">						
	<div id="bib_429cca8b418832533cf52c3869ddd2edlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Multi-word expressions in English-Latvian machine translation</b>. <br/>
<i>Baltic J. Modern Computing</i>, 4(4):811-825, 2016.

<br/>
Inguna Skadina.
<br/>
<a href="http://www.bjmc.lu.lv/fileadmin/user_upload/lu_portal/projekti/bjmc/Contents/4_4_14_Skadina.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '0e7715dd47944280ca329ba35a45f20e'); return false;" href="https://www.bibsonomy.org/bibtex/20e7715dd47944280ca329ba35a45f20e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0e7715dd47944280ca329ba35a45f20e', 'https://www.bibsonomy.org/bibtex/20e7715dd47944280ca329ba35a45f20e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20e7715dd47944280ca329ba35a45f20e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0e7715dd47944280ca329ba35a45f20elepsky" style="display:none;border:1px dotted grey;">
The paper presents series of experiments that aim to find best method how to treat multi-word expressions (MWE) in machine translation task. Methods have been investigated in a framework of statistical machine translation (SMT) for translation form English into Latvian. MWE candidates have been extracted using pattern-based and statistical approaches. Different techniques for MWE integration into SMT system are analysed.
</div>
<div style="position:relative">						
	<div id="bib_0e7715dd47944280ca329ba35a45f20elepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2015" class="bibsonomy_quicknav_group"><a name="2015">2015</a></h3>
<div style="margin-bottom:1em"><b>Bildung von Komposita-Indextermen auf der Basis einer algorithmischen Mehrwortgruppenanalyse mit Lingo</b>.
<br/>
PhD thesis, Fachhochschule Köln; Fakultät für Informations- und Kommunikationswissenschaften, Köln, 2015.

<br/>
Stefan Grün.
<br/>

<a onclick="toggleAbstract('lepsky', 'daf09abdf12a69f8919aafc69b60af25'); return false;" href="https://www.bibsonomy.org/bibtex/2daf09abdf12a69f8919aafc69b60af25/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'daf09abdf12a69f8919aafc69b60af25', 'https://www.bibsonomy.org/bibtex/2daf09abdf12a69f8919aafc69b60af25/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2daf09abdf12a69f8919aafc69b60af25/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_daf09abdf12a69f8919aafc69b60af25lepsky" style="display:none;border:1px dotted grey;">
In der deutschen Sprache lassen sich Begriffe durch Komposita und Mehrwortgruppen ausdrücken. Letztere können dabei aber auch als Kompositum selbst ausgedrückt werden und entsprechend auf den gleichen Begriff verweisen. In der nachfolgenden Studie werden Mehrwortgruppen analysiert, die auch Komposita sein können. Ziel der Untersuchung ist es, diese Wortfolgen über Muster zu identifizieren. Analysiert wurden Daten des Karrieremanagers Placement24 GmbH – in Form von Stellenanzeigen. Die Extraktion von Mehrwortgruppen erfolgte algorithmisch und wurde mit der Open-Source Software Lingo durchgeführt. Auf der Basis von Erweiterungen bzw. Anpassungen in Wörterbüchern und den darin getaggten Wörtern, wurden drei- bis fünfstellige Kandidaten analysiert. Aus positiv bewerteten Mehrwortgruppen wurden Komposita gebildet. Diese wurden mit den identifizierten Komposita aus den Stellenanzeigen verglichen. Der Vergleich zeigte, dass ein Großteil der neu generierten Komposita nicht durch eine Kompositaidentifizierung erzeugt wurde.
</div>
<div style="position:relative">						
	<div id="bib_daf09abdf12a69f8919aafc69b60af25lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Robust semantic analysis of multiword expressions with FrameNet</b>. <br/>
In: <i>EMNLP 2015: Conference on Empirical Methods in Natural Language Processing — September 17–21, 2015 — Lisbon, Portugal</i>.
Lisbon, 2015.

<br/>
Miriam R. L. Petruck and Valia Kordoni.
<br/>

<a href="http://www.emnlp2015.org/tutorials/20/MWEtutorialVK.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '068406e22926c380669c570765cd41ae'); return false;" href="https://www.bibsonomy.org/bibtex/2068406e22926c380669c570765cd41ae/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '068406e22926c380669c570765cd41ae', 'https://www.bibsonomy.org/bibtex/2068406e22926c380669c570765cd41ae/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2068406e22926c380669c570765cd41ae/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_068406e22926c380669c570765cd41aelepsky" style="display:none;border:1px dotted grey;">
This tutorial will give participants a solid understanding of the linguistic features of multiword expressions (MWEs), focusing on the semantics of such expressions and their importance for natural language processing and language technology, with particular attention to the way that FrameNet (framenet.icsi.berkeley.edu) handles this wide spread phenomenon. Our target audience includes researchers and practitioners of language technology, not necessarily experts in MWEs or knowledgeable about FrameNet, who are interested in NLP tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication. NLP research has been interested in automatic processing of multiword expressions, with reports on and tasks relating to such efforts presented at workshops and conferences for at least ten years (e.g. ACL 2003, LREC 2008, COLING 2010, EACL 2014). Overcoming the challenge of automatically processing MWEs remains elusive in part because of the difficulty in recognizing, acquiring, and interpreting such forms. Indeed the phenomenon manifests in a range of linguistic forms (as Sag et al. (2001), among many others, have documented), including: noun + noun compounds (e.g. fish knife, health hazard etc.); adjective + noun compounds (e.g. political agenda, national interest, etc.); particle verbs (shut up, take out, etc.); prepositional verbs (e.g. look into, talk into, etc.); VP idioms, such as kick the bucket, and pull someone's leg, along with less obviously idiomatic forms like answer the door, mention someone's name, etc.; expressions that have their own mini-grammars, such as names with honorifics and terms of address (e.g. Rabbi Lord Jonathan Sacks), kinship terms (e.g. second cousin once removed), and time expressions (e.g. January 9, 2015); support verb constructions (e.g. verbs: take a bath, make a promise, etc; and prepositions: in doubt, under review, etc.). Linguists address issues of polysemy, compositionality, idiomaticity, and continuity for each type included here. While native speakers use these forms with ease, the treatment and interpretation of MWEs in computational systems requires considerable effort due to the very issues that concern linguists
</div>
<div style="position:relative">						
	<div id="bib_068406e22926c380669c570765cd41aelepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Multiword expressions acquisition : a generic and open framework</b>.<br/>
2015. 
<br/>Carlos Ramisch.
<br/>
<a href="http://www.springer.com/new+%26+forthcoming+titles+%28default%29/book/978-3-319-09206-5">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd4ae7b74950ce6be7f515eff7a601747'); return false;" href="https://www.bibsonomy.org/bibtex/2d4ae7b74950ce6be7f515eff7a601747/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd4ae7b74950ce6be7f515eff7a601747', 'https://www.bibsonomy.org/bibtex/2d4ae7b74950ce6be7f515eff7a601747/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d4ae7b74950ce6be7f515eff7a601747/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d4ae7b74950ce6be7f515eff7a601747lepsky" style="display:none;border:1px dotted grey;">
This book is an excellent introduction to multiword expressions. It provides a unique, comprehensive and up-to-date overview of this exciting topic in computational linguistics. The first part describes the diversity and richness of multiword expressions, including many examples in several languages. These constructions are not only complex and arbitrary, but also much more frequent than one would guess, making them a real nightmare for natural language processing applications. The second part introduces a new generic framework for automatic acquisition of multiword expressions from texts. Furthermore, it describes the accompanying free software tool, the mwetoolkit, which comes in handy when looking for expressions in texts (regardless of the language). Evaluation is greatly emphasized, underlining the fact that results depend on parameters like corpus size, language, MWE type, etc. The last part contains solid experimental results and evaluates the mwetoolkit, demonstrating its usefulness for computer-assisted lexicography and machine translation. This is the first book to cover the whole pipeline of multiword expression acquisition in a single volume. It is addresses the needs of students and researchers in computational and theoretical linguistics, cognitive sciences, artificial intelligence and computer science. Its good balance between computational and linguistic views make it the perfect starting point for anyone interested in multiword expressions, language and text processing in general.
</div>
<div style="position:relative">						
	<div id="bib_d4ae7b74950ce6be7f515eff7a601747lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>A single word is not enough : ranking multiword expressions using distributional semantics</b>.<br/>
2015. 
<br/>Martin Riedl and Chris Biemann.
<br/>
<a href="https://www.lt.informatik.tu-darmstadt.de/fileadmin/user_upload/Group_LangTech/publications/2015_EMNLP_MWE_DRUID.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'cb2044127db903991f0aa1da3b5cb1ec'); return false;" href="https://www.bibsonomy.org/bibtex/2cb2044127db903991f0aa1da3b5cb1ec/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cb2044127db903991f0aa1da3b5cb1ec', 'https://www.bibsonomy.org/bibtex/2cb2044127db903991f0aa1da3b5cb1ec/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cb2044127db903991f0aa1da3b5cb1ec/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cb2044127db903991f0aa1da3b5cb1eclepsky" style="display:none;border:1px dotted grey;">
We present a new unsupervised mechanism, which ranks word n-grams according to their multiwordness. It heavily re- lies on a new uniqueness measure that computes, based on a distributional thesaurus, how often an n-gram could be re- placed in context by a single-worded term. In addition with a downweighting mechanism for incomplete terms this forms a new measure called DRUID. Results show large improvements on two small test sets over competitive baselines. We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic filtering.
</div>
<div style="position:relative">						
	<div id="bib_cb2044127db903991f0aa1da3b5cb1eclepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Evaluating noise reduction strategies for terminology extraction</b>. <br/>
In: T. Poibeau and P. Faber, editors, <i>Terminology and Artificial Intelligence   Proceedings of the 11th International Conference on Terminology and Artificial Intelligence</i>, pages 123-131.
Granada, 2015.

<br/>
Johannes Schäfer, Ina Rösiger, Ulrich Heid and Michael Dorna.
<br/>

<a href="http://ceur-ws.org/Vol-1495/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ffae172623db783ff8b7562925337d4c'); return false;" href="https://www.bibsonomy.org/bibtex/2ffae172623db783ff8b7562925337d4c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ffae172623db783ff8b7562925337d4c', 'https://www.bibsonomy.org/bibtex/2ffae172623db783ff8b7562925337d4c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ffae172623db783ff8b7562925337d4c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ffae172623db783ff8b7562925337d4clepsky" style="display:none;border:1px dotted grey;">
We present work on the task of reducing noise in nominal terminology extraction. Based on a comparative evaluation of statistical measures aimed at capturing domain specificity, we propose strategies to increase the typically quite low accuracy of classical hybrid nominal multi-word term extraction. Our experiments on a set of German do-it-yourself instruction texts show that using linguistic filters that determine the right span of the MWE before applying a suitable combination of statistical measures improves results.
</div>
<div style="position:relative">						
	<div id="bib_ffae172623db783ff8b7562925337d4clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Modeling semantic compositionality of croatian multiword expressions</b>. <br/>
<i>Informatica</i>, 39(3):301-309, 2015.
bibtex: snajder2015modeling
<br/>
Jan Šnajder and Petra Almić.
<br/>
<a href="http://takelab.fer.hr/data/cromwesc/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '63af2cf27ee1db37c093073d2cd6a527'); return false;" href="https://www.bibsonomy.org/bibtex/263af2cf27ee1db37c093073d2cd6a527/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '63af2cf27ee1db37c093073d2cd6a527', 'https://www.bibsonomy.org/bibtex/263af2cf27ee1db37c093073d2cd6a527/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/263af2cf27ee1db37c093073d2cd6a527/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_63af2cf27ee1db37c093073d2cd6a527lepsky" style="display:none;border:1px dotted grey;">
A distinguishing feature of many multiword expressions (MWEs) is their semantic non-compositionality. Determining the semantic compositionality of MWEs is important for many natural language process- ing tasks. We address the task of modeling semantic compositionality of Croatian MWEs. We adopt a composition-based approach within the distributional semantics framework. We build and evaluate models based on Latent Semantic Analysis and the recently proposed neural network-based Skip-gram model, and experiment with different composition functions. We show that the compositionality scores predicted by the Skip-gram additive models correlate well with human judgments (ρ=0.50). When framed as a classifi- cation task, the model achieves an accuracy of 0.64.
</div>
<div style="position:relative">						
	<div id="bib_63af2cf27ee1db37c093073d2cd6a527lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2014" class="bibsonomy_quicknav_group"><a name="2014">2014</a></h3>
<div style="margin-bottom:1em">
<b>The ACL RD-TEC : a dataset for benchmarking terminology extraction and classification in computational linguistics</b>.<br/>
In: 
P. Drouin, N. Grabar, T. Hamon and K. Kageura, editors, 
<i>COLING 2014: 4th International Workshop on Computational Terminology Dublin</i>.
Dublin, 2014.

<br/>
Siegfried Handschuh and Behrang QasemiZadeh.
<br/>

<a href="https://aran.library.nuigalway.ie/handle/10379/4489">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd9541aad8bbc6da611dd6dc7285d3a76'); return false;" href="https://www.bibsonomy.org/bibtex/2d9541aad8bbc6da611dd6dc7285d3a76/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd9541aad8bbc6da611dd6dc7285d3a76', 'https://www.bibsonomy.org/bibtex/2d9541aad8bbc6da611dd6dc7285d3a76/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d9541aad8bbc6da611dd6dc7285d3a76/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d9541aad8bbc6da611dd6dc7285d3a76lepsky" style="display:none;border:1px dotted grey;">
This paper introduces ACL RD-TEC: a dataset for evaluating the extraction and classification of terms from literature in the domain of computational linguistics. The dataset is derived from the Association for Computational Linguistics anthology reference corpus (ACL ARC). In its first release, the ACL RD-TEC consists of automatically segmented, part-of-speech-tagged ACL ARC documents, three lists of candidate terms, and more than 82,000 manually annotated terms. The annotated terms are marked as either valid or invalid, and valid terms are further classified as technology and non-technology terms. Technology terms signify methods, algorithms, and solutions in computational linguistics. The paper describes the dataset and reports the relevant statistics. We hope the step described in this paper encourages a collaborative effort towards building a full-fledged annotated corpus from the computational linguistics literature.
</div>
<div style="position:relative">						
	<div id="bib_d9541aad8bbc6da611dd6dc7285d3a76lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Yet another ranking function for automatic multiword term extraction</b>. <br/>
, 2014.

<br/>
Juan Lossio-Ventura, Clement Jonquet, Mathieu Roche and Maguelonne Teisseire.
<br/>
<a href="http://www.lirmm.fr/~jonquet/publications/documents/Article_PolTAL2014_Lossio.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '026799f12b8d5fe9ab2638afd5239ef8'); return false;" href="https://www.bibsonomy.org/bibtex/2026799f12b8d5fe9ab2638afd5239ef8/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '026799f12b8d5fe9ab2638afd5239ef8', 'https://www.bibsonomy.org/bibtex/2026799f12b8d5fe9ab2638afd5239ef8/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2026799f12b8d5fe9ab2638afd5239ef8/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_026799f12b8d5fe9ab2638afd5239ef8lepsky" style="display:none;border:1px dotted grey;">
Term extraction is an essential task in domain knowledge acquisition. We propose two new measures to extract multiword terms from a domain-specific text. The first measure is both linguistic and sta- tistical based. The second measure is graph-based, allowing assessment of the importance of a multiword term of a domain. Existing measures often solve some problems related (but not completely) to term extrac- tion, e.g., noise, silence, low frequency, large-corpora, complexity of the multiword term extraction process. Instead, we focus on managing the entire set of problems, e.g., detecting rare terms and overcoming the low frequency issue. We show that the two proposed measures outperform precision results previously reported for automatic multiword extraction by comparing them with the state-of-the-art reference measures.
</div>
<div style="position:relative">						
	<div id="bib_026799f12b8d5fe9ab2638afd5239ef8lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Detecting multiword expressions and named entities in natural language texts</b>. <br/>
, 2014.

<br/>
István Nagy.
<br/>
<a href="http://doktori.bibl.u-szeged.hu/2434/1/main.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '2d238993b4f330e5fa5c8a50512d976b'); return false;" href="https://www.bibsonomy.org/bibtex/22d238993b4f330e5fa5c8a50512d976b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '2d238993b4f330e5fa5c8a50512d976b', 'https://www.bibsonomy.org/bibtex/22d238993b4f330e5fa5c8a50512d976b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22d238993b4f330e5fa5c8a50512d976b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2d238993b4f330e5fa5c8a50512d976blepsky" style="display:none;border:1px dotted grey;">
Multiword expressions (MWEs) are lexical items that can be decomposed into single words and display lexical, syntactic, semantic, pragmatic and/or statistical idiosyncrasy (Sag et al., 2002; Kim, 2008; Calzolari et al., 2002). The proper treatment of multiword expressions such as rock ’n’ roll and make a decision is essential for many natural language process- ing (NLP) applications like information extraction and retrieval, terminology extraction and machine translation, and it is important to identify multiword expressions in context. For example, in machine translation we must know that MWEs form one semantic unit, hence their parts should not be translated separately. For this, multiword expressions should be identified first in the text to be translated. The chief aim of this thesis is to develop machine learning-based approaches for the auto- matic detection of different types of multiword expressions in English and Hungarian natural language texts. In our investigations, we pay attention to the characteristics of different types of multiword expressions such as nominal compounds, multiword named entities and light verb constructions, and we apply novel methods to identify MWEs in raw texts. In the thesis it will be demonstrated that nominal compounds and multiword named enti- ties may require a similar approach for their automatic detection as they behave in the same way from a linguistic point of view. Furthermore, it will be shown that the automatic detec- tion of light verb constructions can be carried out using two effective machine learning-based approaches.
</div>
<div style="position:relative">						
	<div id="bib_2d238993b4f330e5fa5c8a50512d976blepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2013" class="bibsonomy_quicknav_group"><a name="2013">2013</a></h3>
<div style="margin-bottom:1em"><b>Terminologieextraktion von Mehrwortgruppen in kunsthistorischen Fachtexten</b>.
<br/>
PhD thesis, Fachhochschule Köln; Fakultät für Informations- und Kommunikationswissenschaften, Köln, 2013.

<br/>
Juliane Bredack.
<br/>
<a href="http://ixtrieve.fh-koeln.de/lehre/bredack-2013.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3cd63468cb914d78411e7e44a8f1d6d2'); return false;" href="https://www.bibsonomy.org/bibtex/23cd63468cb914d78411e7e44a8f1d6d2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3cd63468cb914d78411e7e44a8f1d6d2', 'https://www.bibsonomy.org/bibtex/23cd63468cb914d78411e7e44a8f1d6d2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23cd63468cb914d78411e7e44a8f1d6d2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3cd63468cb914d78411e7e44a8f1d6d2lepsky" style="display:none;border:1px dotted grey;">
Mit Hilfe eines algorithmisch arbeitenden Verfahrens können Mehrwortgruppen aus elektronisch vorliegenden Texten identifiziert und extrahiert werden. Als Datengrundlage für diese Arbeit dienen kunsthistorische Lexikonartikel des Reallexikons zur Deutschen Kunstgeschichte. Die linguistisch, wörterbuchbasierte Open-Source-Software Lingo wurde in dieser Studie genutzt. Mit Lingo ist es möglich, auf Basis erstellter Wortmuster, bestimmte Wortfolgen aus elektronisch vorliegenden Daten algorithmisch zu identifizieren und zu extrahieren. Die erstellten Wortmuster basieren auf Wortklassen, mit denen die lexikalisierten Einträge in den Wörterbüchern getaggt sind und dadurch näher definiert werden. So wurden individuelle Wortklassen für Fachterminologie, Eigennamen, oder Adjektive vergeben. In der vorliegenden Arbeit werden zusätzlich Funktionswörter in die Musterbildung mit einbezogen. Dafür wurden neue Wortklassen definiert. Funktionswörter bestimmen Artikel, Konjunktionen und Präpositionen. Ziel war es fachterminologische Mehrwortgruppen mit kunsthistorischen Inhalten zu extrahieren unter der gezielten Einbindung von Funktionswörtern. Anhand selbst gebildeter Kriterien, wurden die extrahierten Mehrwortgruppen qualitativ analysiert. Es konnte festgestellt werden, dass die Verwendung von Funktionswörtern fachterminologische Mehrwortgruppen erzeugt, die als potentielle Indexterme weitere Verwendung im Information Retrieval finden können.
</div>
<div style="position:relative">						
	<div id="bib_3cd63468cb914d78411e7e44a8f1d6d2lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Multilingual compound splitting combining language dependent and independent features</b>. <br/>
, 2013.

<br/>
Elizaveta Loginova-Clouet and Béatrice Daille.
<br/>
<a href="http://www.dialog-21.ru/digests/dialog2013/materials/pdf/Loginova-ClouetEA.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7d0410ed01a121ab19a90ea2c11665cd'); return false;" href="https://www.bibsonomy.org/bibtex/27d0410ed01a121ab19a90ea2c11665cd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7d0410ed01a121ab19a90ea2c11665cd', 'https://www.bibsonomy.org/bibtex/27d0410ed01a121ab19a90ea2c11665cd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27d0410ed01a121ab19a90ea2c11665cd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7d0410ed01a121ab19a90ea2c11665cdlepsky" style="display:none;border:1px dotted grey;">
Compounding is a common phenomenon for many languages, especially those with rich morphology. Dealing with compounds is a challenge for NLP systems since compounds are not often included in the dictionaries and other lexical sources. We present a compound splitting method combining language independent features (similarity measure, corpus data) and language specific component transformation rules. Due to the usage of language independent features, the method can be applied to different languages. We report on our experiments in splitting of German and Russian compound words, giving positive results compared to matching of compound parts in a lexicon. To the best of our knowledge. elaborated compound splitting is a rare component of NLP systems for Russian, yet our experiments show that it could be beneficial to use a specialized vocabulary.
</div>
<div style="position:relative">						
	<div id="bib_7d0410ed01a121ab19a90ea2c11665cdlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An experimental study of term extraction for real information-retrieval thesauri</b>. <br/>
In: , pages 69-76.
2013.

<br/>
Natalia Loukachevitch and Michael Nokel.
<br/>

<a href="https://lipn.univ-paris13.fr/tia2013/Proceedings/actesTIA2013.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'fa5a6cc80a2e5f4b4b155bca6183f07a'); return false;" href="https://www.bibsonomy.org/bibtex/2fa5a6cc80a2e5f4b4b155bca6183f07a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'fa5a6cc80a2e5f4b4b155bca6183f07a', 'https://www.bibsonomy.org/bibtex/2fa5a6cc80a2e5f4b4b155bca6183f07a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2fa5a6cc80a2e5f4b4b155bca6183f07a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_fa5a6cc80a2e5f4b4b155bca6183f07alepsky" style="display:none;border:1px dotted grey;">
Models for effective term extraction can de- pend on the type of a terminological re- source under construction. In this paper we study term extraction models for real- working information-retrieval thesauri. The first thesaurus is the English version of Eu- roVoc thesaurus, the second one is the Rus- sian Banking thesaurus. We study single- word and two-word term extraction sepa- rately to reveal the best features and fea- ture combinations, compare best models for two thesauri. In particular, we found for this type of terminological resources that the use of association measures does not im- prove the quality of two-word term extrac- tion based on combining multiple features.
</div>
<div style="position:relative">						
	<div id="bib_fa5a6cc80a2e5f4b4b155bca6183f07alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Modeling the internal variability of multiword expressions through a pattern-based method</b>. <br/>
<i>ACM Trans. Speech Lang. Process.</i>, 10(2), 2013.

<br/>
Malvina Nissim and Andrea Zaninello.
<br/>
<a href="http://dx.doi.org/10.1145/2483691.2483696">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '73b816a0f2c5d7b8dbfffe98fdeaebff'); return false;" href="https://www.bibsonomy.org/bibtex/273b816a0f2c5d7b8dbfffe98fdeaebff/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '73b816a0f2c5d7b8dbfffe98fdeaebff', 'https://www.bibsonomy.org/bibtex/273b816a0f2c5d7b8dbfffe98fdeaebff/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/273b816a0f2c5d7b8dbfffe98fdeaebff/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_73b816a0f2c5d7b8dbfffe98fdeaebfflepsky" style="display:none;border:1px dotted grey;">
The issue of internal variability of multiword expressions (MWEs) is crucial towards their identification and extraction in running text. We present a corpus-supported and computational study on Italian MWEs, aimed at defining an automatic method for modeling internal variation, exploiting frequency and part-of-speech (POS) information. We do so by deriving an XML-encoded lexicon of MWEs based on a manually compiled dictionary, which is then projected onto a a large corpus. Since a search for fixed forms suffers from low recall, while an unconstrained flexible search for lemmas yields a loss in precision, we suggest a procedure aimed at maximizing precision in the identification of MWEs within a flexible search. Our method builds on the idea that internal variability can be modelled via the novel introduction of variation patterns, which work over POS patterns, and can be used as working tools for controlling precision. We also compare the performance of variation patterns to that of association measures, and explore the possibility of using variation patterns in MWE extraction in addition to identification. Finally, we suggest that corpus-derived, pattern-related information can be included in the original MWE lexicon by means of an enriched coding and the creation of an XML-based repository of patterns.
</div>
<div style="position:relative">						
	<div id="bib_73b816a0f2c5d7b8dbfffe98fdeaebfflepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Introduction to the special issue on multiword expressions : from theory to practice and use</b>. <br/>
<i>ACM Trans. Speech Lang. Process.</i>, 10(2), 2013.

<br/>
Carlos Ramisch, Aline Villavicencio and Valia Kordoni.
<br/>
<a href="http://dx.doi.org/10.1145/2483691.2483692">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a99606c725721df9f4fbe4de61333158'); return false;" href="https://www.bibsonomy.org/bibtex/2a99606c725721df9f4fbe4de61333158/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a99606c725721df9f4fbe4de61333158', 'https://www.bibsonomy.org/bibtex/2a99606c725721df9f4fbe4de61333158/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a99606c725721df9f4fbe4de61333158/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a99606c725721df9f4fbe4de61333158lepsky" style="display:none;border:1px dotted grey;">
We are in 2013, and multiword expressions have been around for a while in the computational linguistics research community. Since the first ACL workshop on MWEs 12 years ago in Sapporo, Japan, much has been discussed, proposed, experimented, evaluated and argued about MWEs. And yet, they deserve the publication of a whole special issue of the ACM Transactions on Speech and Language Processing. But what is it about multiword expressions that keeps them in fashion? Who are the people and the institutions who perform and publish groundbreaking fundamental and applied research in this field? What is the place and the relevance of our lively research community in the bigger picture of computational linguistics? Where do we come from as a community, and most importantly, where are we heading? In this introductory article, we share our point of view about the answers to these questions and introduce the articles that compose the current special issue.
</div>
<div style="position:relative">						
	<div id="bib_a99606c725721df9f4fbe4de61333158lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A lexicon of multiword expressions for linguistically precise, wide-coverage natural language processing</b>. <br/>
<i>Computer Speech &amp; Language</i>, 2013.

<br/>
Toshifumi Tanabe, Masahito Takahashi and Kosho Shudo.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/S0885230813000600#">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7352f243b5790b805aae9124ab5a1514'); return false;" href="https://www.bibsonomy.org/bibtex/27352f243b5790b805aae9124ab5a1514/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7352f243b5790b805aae9124ab5a1514', 'https://www.bibsonomy.org/bibtex/27352f243b5790b805aae9124ab5a1514/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27352f243b5790b805aae9124ab5a1514/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7352f243b5790b805aae9124ab5a1514lepsky" style="display:none;border:1px dotted grey;">
Since Sag et al. (2002) highlighted a key problem that had been underappreciated in the past in natural language processing (NLP), namely idiosyncratic multiword expressions (MWEs) such as idioms, quasi-idioms, clichés, quasi-clichés, institutionalized phrases, proverbs, old sayings, etc., and how to deal with them, many attempts have been made to extract these expressions from corpora and construct a lexicon of them. However, no extensive, reliable solution has yet been realized. This paper presents an overview of a comprehensive lexicon of Japanese multiword expressions (Japanese MWE Lexicon: JMWEL), which has been compiled in order to realize linguistically precise and wide-coverage natural Japanese processing systems. The JMWEL is characterized by significant notational, syntactic, and semantic diversity as well as a detailed description of the syntactic functions, structures, and flexibilities of MWEs. The lexicon contains about 111,000 header entries written in kana (phonetic characters) and their almost 820,000 variants written in kana and kanji (ideographic characters). The paper demonstrates the JMWEL's validity, supported mainly by comparing the lexicon with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08 generated by Google Inc. (Kudo and Kazawa, 2009). The present work is an attempt to provide a tentative answer for Japanese, from outside statistical empiricism, to the question posed by Church (2011): '' How many multiword expressions do people know?''
</div>
<div style="position:relative">						
	<div id="bib_7352f243b5790b805aae9124ab5a1514lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2012" class="bibsonomy_quicknav_group"><a name="2012">2012</a></h3>
<div style="margin-bottom:1em">
<b>Detecting multiword phrases in mathematical text corpora</b>. <br/>
<i>arXiv</i>, 2012.

<br/>
Winfried Gödert.
<br/>
<a href="http://arxiv.org/abs/1210.0852">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b07ea77151e5f279eaa6240ec524c932'); return false;" href="https://www.bibsonomy.org/bibtex/2b07ea77151e5f279eaa6240ec524c932/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b07ea77151e5f279eaa6240ec524c932', 'https://www.bibsonomy.org/bibtex/2b07ea77151e5f279eaa6240ec524c932/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b07ea77151e5f279eaa6240ec524c932/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b07ea77151e5f279eaa6240ec524c932lepsky" style="display:none;border:1px dotted grey;">
We present an approach for detecting multiword phrases in mathematical text corpora. The method used is based on characteristic features of mathematical terminology. It makes use of a software tool named Lingo which allows to identify words by means of previously defined dictionaries for specific word classes as adjectives, personal names or nouns. The detection of multiword groups is done algorithmically. Possible advantages of the method for indexing and information retrieval and conclusions for applying dictionary-based methods of automatic indexing instead of stemming procedures are discussed.
</div>
<div style="position:relative">						
	<div id="bib_b07ea77151e5f279eaa6240ec524c932lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2010" class="bibsonomy_quicknav_group"><a name="2010">2010</a></h3>
<div style="margin-bottom:1em">
<b>Multiword expressions : hard going or plain sailing?</b>. <br/>
<i>Language Resources and Evaluation</i>, 44(1):1-5, 2010.
bibtex: rayson2010multiword
<br/>
Paul Rayson, Scott Piao, Serge Sharoff, Stefan Evert and Begona Villada Moirón.
<br/>


<a onclick="toggleBibtex('lepsky', '1557f96f5a53a7e1cdcb3f3fd7676515', 'https://www.bibsonomy.org/bibtex/21557f96f5a53a7e1cdcb3f3fd7676515/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21557f96f5a53a7e1cdcb3f3fd7676515/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1557f96f5a53a7e1cdcb3f3fd7676515lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_1557f96f5a53a7e1cdcb3f3fd7676515lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2009" class="bibsonomy_quicknav_group"><a name="2009">2009</a></h3>
<div style="margin-bottom:1em">
<b>Multiword expressions : a pain in the neck of lexical semantics</b>. <br/>
:26, 2009.

<br/>
Gemma Boleda and Stefan Evert.
<br/>
<a href="http://clseslli09.files.wordpress.com/2009/07/05_multiword_expressions.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', '53df26d32b0bdc732a1278e32b2a0ce1', 'https://www.bibsonomy.org/bibtex/253df26d32b0bdc732a1278e32b2a0ce1/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/253df26d32b0bdc732a1278e32b2a0ce1/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_53df26d32b0bdc732a1278e32b2a0ce1lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_53df26d32b0bdc732a1278e32b2a0ce1lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2008" class="bibsonomy_quicknav_group"><a name="2008">2008</a></h3>
<div style="margin-bottom:1em">
<b>Efficient multi-word expressions extractor using suffix arrays and related structures</b>. <br/>
In: <i>Proceedings of the 2Nd ACM Workshop on Improving Non English Web Searching</i>, series iNEWS '08, pages 1-8.
ACM, New York, NY, 2008.

<br/>
José Aires, Gabriel Lopes and Joaquim Ferreira Silva.
<br/>

<a href="http://doi.acm.org/10.1145/1460027.1460029">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '08a5c264d298849306bf9117eecd0c43'); return false;" href="https://www.bibsonomy.org/bibtex/208a5c264d298849306bf9117eecd0c43/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '08a5c264d298849306bf9117eecd0c43', 'https://www.bibsonomy.org/bibtex/208a5c264d298849306bf9117eecd0c43/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/208a5c264d298849306bf9117eecd0c43/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_08a5c264d298849306bf9117eecd0c43lepsky" style="display:none;border:1px dotted grey;">
For Information Retrieval purposes, there is a need for regularly processing predictably dynamic and potentially huge corpora for extraction of contiguous Multi Word Expressions (MWEs), in a way that should be computationally tractable. In this paper we'll be mainly exploring the use of Suffix Arrays, together with the SCP association measure and the Smoothed LocalMaxs algorithm. The choice of Suffix Arrays and the construction of auxiliary structures enabled a clear minimization of the time for extracting multi-word expressions, with linear complexity by the introduction of a limitation on the number of words. Despite the methodology being essentially of a statistical nature, we show how to handle hybrid extraction mechanisms.
</div>
<div style="position:relative">						
	<div id="bib_08a5c264d298849306bf9117eecd0c43lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An evaluation of methods for the extraction of multiword expressions</b>. <br/>
In: , pages 50-53.
2008.

<br/>
Carlos Ramisch, Paulo Schreiner, Marco Idiart and Aline Villavicencio.
<br/>

<a href="http://lrec.elra.info/proceedings/lrec2008/workshops/W20_Proceedings.pdf#page=54">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '53d2a5cd517b872fc251110e211b559a'); return false;" href="https://www.bibsonomy.org/bibtex/253d2a5cd517b872fc251110e211b559a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '53d2a5cd517b872fc251110e211b559a', 'https://www.bibsonomy.org/bibtex/253d2a5cd517b872fc251110e211b559a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/253d2a5cd517b872fc251110e211b559a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_53d2a5cd517b872fc251110e211b559alepsky" style="display:none;border:1px dotted grey;">
This paper focuses on the evaluation of some methods for the automatic acquisition of Multiword Expressions (MWEs). First we investigate the hypothesis that MWEs can be detected solely by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: Mutual Information, χ2 and Permutation Entropy. Moreover, we also look at the impact that the addition of type-specific linguistic information has on the performance of these methods.
</div>
<div style="position:relative">						
	<div id="bib_53d2a5cd517b872fc251110e211b559alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2007" class="bibsonomy_quicknav_group"><a name="2007">2007</a></h3>
<div style="margin-bottom:1em">
<b>Semantics-based multiword expression extraction</b>.<br/>
In: 

<i>Proceedings of the Workshop on A Broader Perspective on Multiword Expressions</i>, pages 25-32.
Association for Computational Linguistics, Prag, 2007.

<br/>
Tim Van de Cruys and Begona Villada Moirón.
<br/>


<a onclick="toggleAbstract('lepsky', '25ac3a9e11c988c43aacbc67ef8865d2'); return false;" href="https://www.bibsonomy.org/bibtex/225ac3a9e11c988c43aacbc67ef8865d2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '25ac3a9e11c988c43aacbc67ef8865d2', 'https://www.bibsonomy.org/bibtex/225ac3a9e11c988c43aacbc67ef8865d2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/225ac3a9e11c988c43aacbc67ef8865d2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_25ac3a9e11c988c43aacbc67ef8865d2lepsky" style="display:none;border:1px dotted grey;">
This paper describes a fully unsupervised and automated method for large-scale ex- traction of multiword expressions (MWEs) from large corpora. The method aims at cap- turing the non-compositionality of MWEs; the intuition is that a noun within a MWE cannot easily be replaced by a semanti- cally similar noun. To implement this intu- ition, a noun clustering is automatically ex- tracted (using distributional similarity mea- sures), which gives us clusters of semanti- cally related nouns. Next, a number of statis- tical measures – based on selectional prefer- ences – is developed that formalize the intu- ition of non-compositionality. Our approach has been tested on Dutch, and automatically evaluated using Dutch lexical resources.
</div>
<div style="position:relative">						
	<div id="bib_25ac3a9e11c988c43aacbc67ef8865d2lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2000" class="bibsonomy_quicknav_group"><a name="2000">2000</a></h3>
<div style="margin-bottom:1em">
<b>Combining linguistics with statistics for multiword term extraction : a fruitful association?</b>. <br/>
, 2000.

<br/>
G Dias, S Guilloré, JC Bassano and Lopes.
<br/>
<a href="http://133.23.229.11/~ysuzuki/Proceedingsall/RIAO2000/Friday/122DP2.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f0e77b615346e8b5edfb40dfe45db33a'); return false;" href="https://www.bibsonomy.org/bibtex/2f0e77b615346e8b5edfb40dfe45db33a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f0e77b615346e8b5edfb40dfe45db33a', 'https://www.bibsonomy.org/bibtex/2f0e77b615346e8b5edfb40dfe45db33a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f0e77b615346e8b5edfb40dfe45db33a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f0e77b615346e8b5edfb40dfe45db33alepsky" style="display:none;border:1px dotted grey;">
The acquisition of multiword terms from large text collections is a fundamental issue in the context of Information Retrieval. Indeed, their identification leads to improvements in the indexing process and allows guiding the user in his search for information. In this paper, we present an original methodology that allows extracting multiword terms by either (1) exclusively considering statistical word regularities or by (2) combining word statistics with endogenously acquired linguistic information. For that purpose, we conjugate a new association measure called the Mutual Expectation with a new acquisition process called the LocalMaxs. On one hand, the Mutual Expectation, based on the concept of Normalised Expectation, evaluates the degree of cohesiveness that links together all the textual units contained in an n-gram (i.e. "n, n 2). On the other hand, the LocalMaxs retrieves the candidate terms from the set of all the valued n-grams by evidencing local maxima of association measure...
</div>
<div style="position:relative">						
	<div id="bib_f0e77b615346e8b5edfb40dfe45db33alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<!-- 
	This software is distributed under a Creative Commons Attribution 3.0 License
	http://creativecommons.org/licenses/by/3.0/

	*Attribution*
	JavaScript by Mark Schenk, Dominik Benz and Michael Domhardt
	JabRef export filter and css by Michael Domhardt http://mensch-maschine-systemtechnik.de/
	BibSonomy and Typo3 integration by Dominik Benz
	Content by BibSonomy - Lesezeichen und Referenzen teilen - in blau! http://bibsonomy.org/
-->

<script type="text/javascript">
<!--
function toggleAbstract(user,hash) {
	var abs = document.getElementById('abs_'+hash+user);	
	if (abs) {
		if(abs.id.indexOf('abs_') != -1) {
			abs.style.display = ( abs.style.display == 'none' ? '' : 'none' );
		}
	} 
	return;
}

function toggleBibtex(user,hash,biburl) {
    var f = document.getElementById('bib_' + hash + user + '_src');
	if (undefined != f) {
		f.parentNode.removeChild(f);
		return;
	}
	var el = document.getElementById('bib_' + hash + user);
    iframe = document.createElement("iframe");
    iframe.setAttribute("src", biburl);
	iframe.setAttribute("id", 'bib_' + hash + user + '_src');
    iframe.style.width = 500+"px";
    iframe.style.height = 200+"px";
	iframe.style.background = "#eee";
    el.appendChild(iframe);
	return;
}
-->
</script>
