<h3 id="bib:year-9998" class="bibsonomy_quicknav_group"><a name="9998">9998</a></h3>
<div style="margin-bottom:1em">
<b>Biomedical term extraction : NLP techniques in computational medicine</b>. <br/>
<i>International Journal of Interactive Multimedia and Artificial Intelligence</i>, In Press(In Press):1-9, 9998.

<br/>
Antonio Moreno Sandoval.
<br/>


<a onclick="toggleBibtex('lepsky', '59fb300f49cde0257135e0fddfcb225f', 'https://www.bibsonomy.org/bibtex/259fb300f49cde0257135e0fddfcb225f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/259fb300f49cde0257135e0fddfcb225f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_59fb300f49cde0257135e0fddfcb225flepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_59fb300f49cde0257135e0fddfcb225flepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2021" class="bibsonomy_quicknav_group"><a name="2021">2021</a></h3>
<div style="margin-bottom:1em">
<b>Term extraction from medical documents using word embeddings</b>. <br/>
:6, 2021.

<br/>
Matthias Bay, Daniel Bruneß and Miriam Herold.
<br/>
<a href="http://www.wi.cs.uni-frankfurt.de/webdav/publications/TLDIA_Paper_IEEE_CRC.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'cdcde5a93b464e1d1e2aa13eefbdb957'); return false;" href="https://www.bibsonomy.org/bibtex/2cdcde5a93b464e1d1e2aa13eefbdb957/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cdcde5a93b464e1d1e2aa13eefbdb957', 'https://www.bibsonomy.org/bibtex/2cdcde5a93b464e1d1e2aa13eefbdb957/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cdcde5a93b464e1d1e2aa13eefbdb957/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cdcde5a93b464e1d1e2aa13eefbdb957lepsky" style="display:none;border:1px dotted grey;">
In this paper we present a new method for the extraction of discipline-speciﬁc terms from medical documents. Due to the small text corpora and the speciﬁc nature of medical documents, there are limitations for approaches that are solely based on term frequencies. A combination of such methods with procedures that are sensitive to semantic aspects is therefore promising. We use word embeddings in a neighborhood context based method which we call Snowball because of its layerwise way of working. Snowball is integrated together with established methods into an end to end pipeline with which we can process documents to extract relevant terms. Proof of concept is given on a gold standard created recently together with experts in medical coding. The preliminary results highlight the feasibility of our approach and its potential for automated, machine learning based text processing in the medical context.
</div>
<div style="position:relative">						
	<div id="bib_cdcde5a93b464e1d1e2aa13eefbdb957lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2020" class="bibsonomy_quicknav_group"><a name="2020">2020</a></h3>
<div style="margin-bottom:1em">
<b>Detecting multiword expression type helps lexical complexity assessment</b>. <br/>
<i>arXiv:2005.05692 [cs]</i>, 2020.
arXiv: 2005.05692
<br/>
Ekaterina Kochmar, Sian Gooding and Matthew Shardlow.
<br/>
<a href="http://arxiv.org/abs/2005.05692">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '9091605c7c01cedb86328d71e57809bb'); return false;" href="https://www.bibsonomy.org/bibtex/29091605c7c01cedb86328d71e57809bb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '9091605c7c01cedb86328d71e57809bb', 'https://www.bibsonomy.org/bibtex/29091605c7c01cedb86328d71e57809bb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/29091605c7c01cedb86328d71e57809bb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_9091605c7c01cedb86328d71e57809bblepsky" style="display:none;border:1px dotted grey;">
Multiword expressions (MWEs) represent lexemes that should be treated as single lexical units due to their idiosyncratic nature. Multiple NLP applications have been shown to benefit from MWE identification, however the research on lexical complexity of MWEs is still an under-explored area. In this work, we re-annotate the Complex Word Identification Shared Task 2018 dataset of Yimam et al. (2017), which provides complexity scores for a range of lexemes, with the types of MWEs. We release the MWE-annotated dataset with this paper, and we believe this dataset represents a valuable resource for the text simplification community. In addition, we investigate which types of expressions are most problematic for native and non-native readers. Finally, we show that a lexical complexity assessment system benefits from the information about MWE types.
</div>
<div style="position:relative">						
	<div id="bib_9091605c7c01cedb86328d71e57809bblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A scientific information extraction dataset for nature inspired engineering</b>. <br/>
<i>arXiv:2005.07753 [cs]</i>, 2020.
arXiv: 2005.07753
<br/>
Ruben Kruiper, Julian F. V. Vincent, Jessica Chen-Burger, Marc P. Y. Desmulliez and Ioannis Konstas.
<br/>
<a href="http://arxiv.org/abs/2005.07753">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c5ae9b42900f9eaa1d747bb331f11f6f'); return false;" href="https://www.bibsonomy.org/bibtex/2c5ae9b42900f9eaa1d747bb331f11f6f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c5ae9b42900f9eaa1d747bb331f11f6f', 'https://www.bibsonomy.org/bibtex/2c5ae9b42900f9eaa1d747bb331f11f6f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c5ae9b42900f9eaa1d747bb331f11f6f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c5ae9b42900f9eaa1d747bb331f11f6flepsky" style="display:none;border:1px dotted grey;">
Nature has inspired various ground-breaking technological developments in applications ranging from robotics to aerospace engineering and the manufacturing of medical devices. However, accessing the information captured in scientific biology texts is a time-consuming and hard task that requires domain-specific knowledge. Improving access for outsiders can help interdisciplinary research like Nature Inspired Engineering. This paper describes a dataset of 1,500 manually-annotated sentences that express domain-independent relations between central concepts in a scientific biology text, such as trade-offs and correlations. The arguments of these relations can be Multi Word Expressions and have been annotated with modifying phrases to form non-projective graphs. The dataset allows for training and evaluating Relation Extraction algorithms that aim for coarse-grained typing of scientific biological documents, enabling a high-level filter for engineers.
</div>
<div style="position:relative">						
	<div id="bib_c5ae9b42900f9eaa1d747bb331f11f6flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Unsupervised neural aspect search with related terms extraction</b>. <br/>
<i>arXiv:2005.02771 [cs]</i>, 2020.
arXiv: 2005.02771
<br/>
Timur Sokhin, Maria Khodorchenko and Nikolay Butakov.
<br/>
<a href="http://arxiv.org/abs/2005.02771">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '6cf0bf6d58ae770a25657524c5cc0e99'); return false;" href="https://www.bibsonomy.org/bibtex/26cf0bf6d58ae770a25657524c5cc0e99/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6cf0bf6d58ae770a25657524c5cc0e99', 'https://www.bibsonomy.org/bibtex/26cf0bf6d58ae770a25657524c5cc0e99/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26cf0bf6d58ae770a25657524c5cc0e99/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6cf0bf6d58ae770a25657524c5cc0e99lepsky" style="display:none;border:1px dotted grey;">
The tasks of aspect identification and term extraction remain challenging in natural language processing. While supervised methods achieve high scores, it is hard to use them in real-world applications due to the lack of labelled datasets. Unsupervised approaches outperform these methods on several tasks, but it is still a challenge to extract both an aspect and a corresponding term, particularly in the multi-aspect setting. In this work, we present a novel unsupervised neural network with convolutional multi-attention mechanism, that allows extracting pairs (aspect, term) simultaneously, and demonstrate the effectiveness on the real-world dataset. We apply a special loss aimed to improve the quality of multi-aspect extraction. The experimental study demonstrates, what with this loss we increase the precision not only on this joint setting but also on aspect prediction only.
</div>
<div style="position:relative">						
	<div id="bib_6cf0bf6d58ae770a25657524c5cc0e99lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Embarrassingly simple unsupervised aspect extraction</b>. <br/>
<i>arXiv:2004.13580 [cs]</i>, 2020.
arXiv: 2004.13580
<br/>
Stéphan Tulkens and Andreas van Cranenburgh.
<br/>
<a href="http://arxiv.org/abs/2004.13580">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '8dd7f428d2dda5b036fcb300a550d830'); return false;" href="https://www.bibsonomy.org/bibtex/28dd7f428d2dda5b036fcb300a550d830/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '8dd7f428d2dda5b036fcb300a550d830', 'https://www.bibsonomy.org/bibtex/28dd7f428d2dda5b036fcb300a550d830/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/28dd7f428d2dda5b036fcb300a550d830/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_8dd7f428d2dda5b036fcb300a550d830lepsky" style="display:none;border:1px dotted grey;">
We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat
</div>
<div style="position:relative">						
	<div id="bib_8dd7f428d2dda5b036fcb300a550d830lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Keywords extraction of medical information in the era of big data</b>. <br/>
In: J. H. Abawajy, K.-K. R. Choo, R. Islam, Z. Xu and M. Atiquzzaman, editors, <i>International Conference on Applications and Techniques in Cyber Intelligence ATCI 2019</i>, series Advances in Intelligent Systems and Computing, pages 1670-1674.
Springer International Publishing, 2020.

<br/>
Na Wang and Jinguo Wang.
<br/>


<a onclick="toggleAbstract('lepsky', '6d677906ec4b00b7d13784e2284110b2'); return false;" href="https://www.bibsonomy.org/bibtex/26d677906ec4b00b7d13784e2284110b2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6d677906ec4b00b7d13784e2284110b2', 'https://www.bibsonomy.org/bibtex/26d677906ec4b00b7d13784e2284110b2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26d677906ec4b00b7d13784e2284110b2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6d677906ec4b00b7d13784e2284110b2lepsky" style="display:none;border:1px dotted grey;">
This paper introduces a kind of network intelligence collection system and the system structure, combined with the architecture analysis of key techniques and implementation methods in the process of system implementation. It generated in the improvement of cyber intelligence automatically. The generation process of key words and the sentence extraction algorithm and analysis of the quality evaluation method provide the measures to guarantee the safe operation of the system. In order to overcome the shortcoming of low precision of the traditional medical keyword extraction, the article puts forward a kind of multistage medical keyword extraction algorithm on the base of the statistical characteristics. The algorithm USES which is the word index of discrete coefficient formula eliminates noise, builds word frequency - reverse text information on weights of evaluation function to measure the importance of keywords, on the base of frequency skewness, words, and position of words. The experimental results show that the algorithm is superior to the traditional method, and in the era of big data it has extensive application value in the network information monitoring.
</div>
<div style="position:relative">						
	<div id="bib_6d677906ec4b00b7d13784e2284110b2lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2019" class="bibsonomy_quicknav_group"><a name="2019">2019</a></h3>
<div style="margin-bottom:1em"><b>Knowledge extraction from simpliﬁed natural language text</b>.
<br/>
PhD thesis, College of Engineering and Informatics, National University of Ireland,Galway, Galway, 2019.

<br/>
Hazem Safwat Abdelaal.
<br/>
<a href="http://hdl.handle.net/10379/15492">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7d85cd7cfcd1823d7b36e3556f45a33b'); return false;" href="https://www.bibsonomy.org/bibtex/27d85cd7cfcd1823d7b36e3556f45a33b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7d85cd7cfcd1823d7b36e3556f45a33b', 'https://www.bibsonomy.org/bibtex/27d85cd7cfcd1823d7b36e3556f45a33b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27d85cd7cfcd1823d7b36e3556f45a33b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7d85cd7cfcd1823d7b36e3556f45a33blepsky" style="display:none;border:1px dotted grey;">
Knowledge base creation and population are an essential formal backbone for a variety of intelligent applications, decision support and expert systems and intelligent search. Although knowledge extraction from unstructured text offers a means of easing the knowledge acquisition process, the ambiguous nature of language tends to impact on accuracy when engaging in more complex semantic analysis. Controlled Natural Languages (CNLs) are subsets of natural language which are restricted grammatically in order to reduce or eliminate ambiguity for the purposes of machine understanding, or unambiguous human communication within a domain or industry context, such as Simplified English. Moreover, CNLs help engaging non-expert users with no background in knowledge engineering, as these languages offer user-friendly interfaces that are easier to understand and accepted by users. The latter type of human-oriented CNL is under-researched despite having found favor in industry over many years. Rewriting such human-oriented CNL content into a machine-oriented CNL could potentially unlock significant silos of implicit valuable general purpose domain knowledge. In this thesis, we have a developed an approach for a series of corpus based rewriting rules for subsequent knowledge capture. Our work confirms that a substantial amount of human-oriented CNL content can be easily translated into a machine processable CNL for formal knowledge capture with little semantic loss. In addition, we describe a novel dataset which aligns a representative sample of Simplified English Wikipedia sentences with a well known machine-oriented CNL. This linguistic resource is both human-readable and semantically machine interpretable, where it can be used by the community as a gold-standard dataset which can benefit a variety of language processing and knowledge based applications.
</div>
<div style="position:relative">						
	<div id="bib_7d85cd7cfcd1823d7b36e3556f45a33blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic keyphrase extraction : a survey and trends</b>. <br/>
<i>Journal of Intelligent Information Systems</i>, 2019.

<br/>
Zakariae Alami Merrouni, Bouchra Frikh and Brahim Ouhbi.
<br/>
<a href="https://doi.org/10.1007/s10844-019-00558-9">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4e6e6b0c02f31dfe37ad8ba0bd92574e'); return false;" href="https://www.bibsonomy.org/bibtex/24e6e6b0c02f31dfe37ad8ba0bd92574e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4e6e6b0c02f31dfe37ad8ba0bd92574e', 'https://www.bibsonomy.org/bibtex/24e6e6b0c02f31dfe37ad8ba0bd92574e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24e6e6b0c02f31dfe37ad8ba0bd92574e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4e6e6b0c02f31dfe37ad8ba0bd92574elepsky" style="display:none;border:1px dotted grey;">
Due to the exponential growth of textual data and web sources, an automatic mechanism is required to identify relevant information embedded within them. The utility of Automatic Keyphrase Extraction (AKPE) cannot be overstated, given its widespread adoption in many Information Retrieval (IR), Natural Language Processing (NLP) and Text Mining (TM) applications, and its potential ability to solve difficulties related to extracting valuable information. In recent years, a wide range of AKPE techniques have been proposed. However, they are still impaired by low accuracy rates and moderate performance. This paper provides a comprehensive review of recent research efforts on the AKPE task and its related techniques. More concretely, we highlight the common process of this task, while also illustrating the various approaches used (supervised, unsupervised, and Deep Learning) and released techniques. We investigate the major challenges that such techniques face and depict the specific complexities they address. Besides, we provide a comparison study of the best performing techniques, discuss why some perform better than others and propose recommendations to improve each stage of the AKPE process.
</div>
<div style="position:relative">						
	<div id="bib_4e6e6b0c02f31dfe37ad8ba0bd92574elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An efficient approach for super and nested term indexing and retrieval</b>. <br/>
<i>arXiv:1905.09761 [cs]</i>, 2019.
arXiv: 1905.09761
<br/>
Md Faisal Mahbub Chowdhury and Robert Farrell.
<br/>
<a href="http://arxiv.org/abs/1905.09761">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '0a3bace7e922df3424fc07ddb5b1101e'); return false;" href="https://www.bibsonomy.org/bibtex/20a3bace7e922df3424fc07ddb5b1101e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0a3bace7e922df3424fc07ddb5b1101e', 'https://www.bibsonomy.org/bibtex/20a3bace7e922df3424fc07ddb5b1101e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20a3bace7e922df3424fc07ddb5b1101e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0a3bace7e922df3424fc07ddb5b1101elepsky" style="display:none;border:1px dotted grey;">
This paper describes a new approach, called Terminological Bucket Indexing (TBI), for efficient indexing and retrieval of both nested and super terms using a single method. We propose a hybrid data structure for facilitating faster indexing building. An evaluation of our approach with respect to widely used existing approaches on several publicly available dataset is provided. Compared to Trie based approaches, TBI provides comparable performance on nested term retrieval and far superior performance on super term retrieval. Compared to traditional hash table, TBI needs 80textbackslash%less time for indexing.
</div>
<div style="position:relative">						
	<div id="bib_0a3bace7e922df3424fc07ddb5b1101elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Building a feature taxonomy of the terms extracted from a text collection</b>.<br/>
In: 

<i>Proceedings of the Masters Symposium on Advances in Data Mining, Machine Learning, and Computer Vision : Lviv, Ukraine, November 15-16, 2019</i>, pages 59-70.
2019.

<br/>
Svitlana Moiseyenko, Alexander Vasileyko and Vadim Ermolayev.
<br/>

<a href="http://ceur-ws.org/Vol-2566/MS-AMLV-2019-paper13-p059.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1752608e4ef4c8d3a49023b60957f07f'); return false;" href="https://www.bibsonomy.org/bibtex/21752608e4ef4c8d3a49023b60957f07f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1752608e4ef4c8d3a49023b60957f07f', 'https://www.bibsonomy.org/bibtex/21752608e4ef4c8d3a49023b60957f07f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21752608e4ef4c8d3a49023b60957f07f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1752608e4ef4c8d3a49023b60957f07flepsky" style="display:none;border:1px dotted grey;">
This position paper presents an approach for feature grouping and taxonomic relationship extraction with the further objective to build a feature taxonomy of a learned ontology. The approach needs to be developed as a part of the OntoElect methodology for domain ontologies refinement. The paper contributes a review of the related work in taxonomic relationships extraction from natural language texts. Within this review, the research gaps and remaining challenges are analyzed. The paper proceeds with outlining the envisioned solution. It presents the approach to this solution starting with the research questions, followed by the initial research hypotheses to be tested. Consequently, the plan of research is presented, including the potential research problems, the rationale to use and re-use existing components, and evaluation plan. Finally, the proposed solution, and the project, are placed in the broader context of the overall OntoElect workflow.
</div>
<div style="position:relative">						
	<div id="bib_1752608e4ef4c8d3a49023b60957f07flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Evaluating ontology development from the extraction of noun phrases</b>. <br/>
In: , pages 12.
Salvador, BA, 2019.

<br/>
Alexandra Moreira, Alcione P Oliveira and Jugurta Lisboa-Filho.
<br/>

<a href="http://www.bracis2019.ufba.br/Camera_Ready/197851_1.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '16a2c1058a6e88d435172bc0093d4ec7'); return false;" href="https://www.bibsonomy.org/bibtex/216a2c1058a6e88d435172bc0093d4ec7/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '16a2c1058a6e88d435172bc0093d4ec7', 'https://www.bibsonomy.org/bibtex/216a2c1058a6e88d435172bc0093d4ec7/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/216a2c1058a6e88d435172bc0093d4ec7/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_16a2c1058a6e88d435172bc0093d4ec7lepsky" style="display:none;border:1px dotted grey;">
There are several methods for constructing an ontology. Among the automatic methods, one approach is the extraction of terms from domain documents and their subsequent extraction. In this case, the ﬁrst step of the process is the extraction of noun phrases that are potential candidates to be components of the terminology of the area of interest. This article describes an automatic tool for the Brazilian Portuguese language that extracts noun phrases that can be adopted as terms for a certain domain. In addition, the system couples the extracted terms into a top-level ontology, which results in an initial ontology that can be further reﬁned. To couple with the ontology an anchor term was used, and a statistic analysis showed that the use of the term anchor leads to an improvement in the performance of the system. The tool described in this article was used to select terms to be used in an ontology for the power sector domain. Also, the precision in the creation of the ontology was evaluated. The technique was able to generate the correct hierarchy for 70%of the terms.
</div>
<div style="position:relative">						
	<div id="bib_16a2c1058a6e88d435172bc0093d4ec7lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Textual keyword extraction and summarization : state-of-the-art</b>. <br/>
<i>Information Processing &amp; Management</i>, 56(6):102088, 2019.

<br/>
Zara Nasar, Syed Waqar Jaffry and Muhammad Kamran Malik.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/S0306457319300044">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '528f2c3aedd247a06b5f75d77081e439'); return false;" href="https://www.bibsonomy.org/bibtex/2528f2c3aedd247a06b5f75d77081e439/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '528f2c3aedd247a06b5f75d77081e439', 'https://www.bibsonomy.org/bibtex/2528f2c3aedd247a06b5f75d77081e439/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2528f2c3aedd247a06b5f75d77081e439/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_528f2c3aedd247a06b5f75d77081e439lepsky" style="display:none;border:1px dotted grey;">
With the advent of Web 2.0, there exist many online platforms that results in massive textual data production such as social networks, online blogs, magazines etc. This textual data carries information that can be used for betterment of humanity. Hence, there is a dire need to extract potential information out of it. This study aims to present an overview of approaches that can be applied to extract and later present these valuable information nuggets residing within text in brief, clear and concise way. In this regard, two major tasks of automatic keyword extraction and text summarization are being reviewed. To compile the literature, scientific articles were collected using major digital computing research repositories. In the light of acquired literature, survey study covers early approaches up to all the way till recent advancements using machine learning solutions. Survey findings conclude that annotated benchmark datasets for various textual data-generators such as twitter and social forms are not available. This scarcity of dataset has resulted into relatively less progress in many domains. Also, applications of deep learning techniques for the task of automatic keyword extraction are relatively unaddressed. Hence, impact of various deep architectures stands as an open research direction. For text summarization task, deep learning techniques are applied after advent of word vectors, and are currently governing state-of-the-art for abstractive summarization. Currently, one of the major challenges in these tasks is semantic aware evaluation of generated results.
</div>
<div style="position:relative">						
	<div id="bib_528f2c3aedd247a06b5f75d77081e439lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A combined approach to automatic taxonomy extraction</b>. <br/>
In: <i>2019 14th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)</i>, pages 1-6.
2019.

<br/>
Samuel Pecar and Marian Simko.
<br/>


<a onclick="toggleAbstract('lepsky', '5c075da8ecba3565dfed54523b99b2c2'); return false;" href="https://www.bibsonomy.org/bibtex/25c075da8ecba3565dfed54523b99b2c2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5c075da8ecba3565dfed54523b99b2c2', 'https://www.bibsonomy.org/bibtex/25c075da8ecba3565dfed54523b99b2c2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25c075da8ecba3565dfed54523b99b2c2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5c075da8ecba3565dfed54523b99b2c2lepsky" style="display:none;border:1px dotted grey;">
We propose a method for taxonomic relationships extraction from text based on morpho-syntactic and pattern-based approach combined with utilization of distributional vectors and application of graph algorithms. We evaluated our method on the datasets from popular SemEval Workshop series. In addition to the standard evaluation measures, we explored also taxonomic measures, which can show other aspects of hierarchy quality. We also conducted an experiment for manual evaluation, which employed participants to evaluate outputs of our method. In most observed measures we obtained better results in comparison with methods competing at the workshops. We show that utilization of vector space and additional external semantic knowledge can significantly improve overall quality of extracted taxonomic hierarchy.
</div>
<div style="position:relative">						
	<div id="bib_5c075da8ecba3565dfed54523b99b2c2lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Taxonomy extraction for customer service knowledge base construction</b>. <br/>
In: M. Acosta, P. Cudré-Mauroux, M. Maleshkova, T. Pellegrini, H. Sack and Y. Sure-Vetter, editors, <i>Semantic Systems. The Power of AI and Knowledge Graphs</i>, series Lecture Notes in Computer Science, pages 175-190.
Springer International Publishing, Cham, 2019.

<br/>
Bianca Pereira, Cecile Robin, Tobias Daudert, John P. McCrae, Pranab Mohanty and Paul Buitelaar.
<br/>


<a onclick="toggleAbstract('lepsky', 'e663e6740c8209600b61aed68cd04147'); return false;" href="https://www.bibsonomy.org/bibtex/2e663e6740c8209600b61aed68cd04147/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e663e6740c8209600b61aed68cd04147', 'https://www.bibsonomy.org/bibtex/2e663e6740c8209600b61aed68cd04147/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e663e6740c8209600b61aed68cd04147/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e663e6740c8209600b61aed68cd04147lepsky" style="display:none;border:1px dotted grey;">
Customer service agents play an important role in bridging the gap between customers’ vocabulary and business terms. In a scenario where organisations are moving into semi-automatic customer service, semantic technologies with capacity to bridge this gap become a necessity. In this paper we explore the use of automatic taxonomy extraction from text as a means to reconstruct a customer-agent taxonomic vocabulary. We evaluate our proposed solution in an industry use case scenario in the financial domain and show that our approaches for automated term extraction and using in-domain training for taxonomy construction can improve the quality of automatically constructed taxonomic knowledge bases.
</div>
<div style="position:relative">						
	<div id="bib_e663e6740c8209600b61aed68cd04147lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Multilingual patent text retrieval evaluation : CLEF–IP</b>.<br/>
In: 
N. Ferro and C. Peters, editors, 
<i>Information Retrieval Evaluation in a Changing World: Lessons Learned from 20 Years of CLEF</i>, pages 365-387.
Springer International Publishing, Cham, 2019.

<br/>
Florina Piroi and Allan Hanbury.
<br/>

<a href="https://doi.org/10.1007/978-3-030-22948-1_15">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd33fd07d556bb2794877687cea4e9137'); return false;" href="https://www.bibsonomy.org/bibtex/2d33fd07d556bb2794877687cea4e9137/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd33fd07d556bb2794877687cea4e9137', 'https://www.bibsonomy.org/bibtex/2d33fd07d556bb2794877687cea4e9137/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d33fd07d556bb2794877687cea4e9137/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d33fd07d556bb2794877687cea4e9137lepsky" style="display:none;border:1px dotted grey;">
The CLEF–IP evaluation lab ran between 2009 and 2013 with a two-fold expressed purpose: (a) to encourage research in the area of patent retrieval with a focus on cross language retrieval, and (b) to provide a large and clean data set of patent related data, in the three main European languages, for experimentation. In its first year, CLEF–IP organized one task only, a text retrieval task that modelled the “Search for Prior Art” done by experts at patent offices. In the following years the types of CLEF–IP tasks broadened to include patent text classification, patent image retrieval and classification, and (formal) structure recognition. With each task, the test collection was extended to accommodate for the additional tasks. In this chapter we overview the evaluation tasks dealing with the textual content of the patents. The Intellectual Property (IP) domain is one where specific expertise is critical, implementing Information Retrieval (IR) approaches to support some of its tasks cannot be done without the use of this domain know-how. Even when such know-how is at hand, retrieval results, in general, do not come close to the expectations of patent experts.
</div>
<div style="position:relative">						
	<div id="bib_d33fd07d556bb2794877687cea4e9137lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Term extraction from domain specific texts</b>.
<br/>
PhD thesis, Faculté des Sciences appliquées; Liège université, Lüttich, 2019.

<br/>
Judicaël Poumay.
<br/>

<a onclick="toggleAbstract('lepsky', '2ec3720537be71b8d4290837548555d4'); return false;" href="https://www.bibsonomy.org/bibtex/22ec3720537be71b8d4290837548555d4/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '2ec3720537be71b8d4290837548555d4', 'https://www.bibsonomy.org/bibtex/22ec3720537be71b8d4290837548555d4/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22ec3720537be71b8d4290837548555d4/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2ec3720537be71b8d4290837548555d4lepsky" style="display:none;border:1px dotted grey;">
Tous les documents placés en accès ouvert sur le site le site MatheO sont protégés par le droit d'auteur. Conformément aux principes énoncés par la "Budapest Open Access Initiative"(BOAI, 2002), l'utilisateur du site peut lire, télécharger, copier, transmettre, imprimer, chercher ou faire un lien vers le texte intégral de ces documents, les disséquer pour les indexer, s'en servir de données pour un logiciel, ou s'en servir à toute autre fin légale (ou prévue par la réglementation relative au droit d'auteur). Toute utilisation du document à des fins commerciales est strictement interdite.
</div>
<div style="position:relative">						
	<div id="bib_2ec3720537be71b8d4290837548555d4lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Insights into relevant knowledge extraction techniques : a comprehensive review</b>. <br/>
<i>The Journal of Supercomputing</i>, 2019.

<br/>
Abdul Shahid, Muhammad Tanvir Afzal, Moloud Abdar, Mohammad Ehsan Basiri, Xujuan Zhou, Neil Y. Yen and Jia-Wei Chang.
<br/>
<a href="https://doi.org/10.1007/s11227-019-03009-y">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e08756821ed4a130e872bdd11df99474'); return false;" href="https://www.bibsonomy.org/bibtex/2e08756821ed4a130e872bdd11df99474/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e08756821ed4a130e872bdd11df99474', 'https://www.bibsonomy.org/bibtex/2e08756821ed4a130e872bdd11df99474/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e08756821ed4a130e872bdd11df99474/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e08756821ed4a130e872bdd11df99474lepsky" style="display:none;border:1px dotted grey;">
More than 50 million journal papers will have been published by the end of 2019 with 2 million more journal papers published every year. The number of conference papers is even higher, and millions of other types of scientific research are added to the knowledge base every year. Scientific databases such as Web of Science, Scopus, and PubMed index millions of scientific papers and Google Scholar indexes a huge amount of scientific knowledge across diverse domains. However, current systems provide long lists of results when users attempt to find relevant papers, leaving them with little choice other than manually skimming through the lists. This article surveys different techniques used to identify relevant research papers by knowledge-based organizations. We categorized current literature content as content, metadata, collaborative filtering, and citation based techniques and identified the strengths and limitation for each approach. Further, we evaluated the published techniques and research-based products used to identify relevant documents and identified the strengths and limitations of each approach. This research will greatly help to understand current state-of-the-art techniques internal workings for finding relevant papers, understand the relevant strengths and limitations, and explore previously proposed techniques targeting this area.
</div>
<div style="position:relative">						
	<div id="bib_e08756821ed4a130e872bdd11df99474lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Variance-based features for keyword extraction in Persian and English text documents</b>. <br/>
<i>Scientia Iranica</i>, 0(0), 2019.

<br/>
Hadi Veisi, Niloofar Aflaki and Pouyan Parsafard.
<br/>
<a href="http://scientiairanica.sharif.edu/article_21440.html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4584c077bfd17afa9e2fee951ba3b26d'); return false;" href="https://www.bibsonomy.org/bibtex/24584c077bfd17afa9e2fee951ba3b26d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4584c077bfd17afa9e2fee951ba3b26d', 'https://www.bibsonomy.org/bibtex/24584c077bfd17afa9e2fee951ba3b26d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24584c077bfd17afa9e2fee951ba3b26d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4584c077bfd17afa9e2fee951ba3b26dlepsky" style="display:none;border:1px dotted grey;">
This paper address automatic keyword extraction in Persian and English text documents. Generally, for keyword extraction in a text, a weight is assigned to each token and words having higher weights are selected as the keywords. We have proposed four methods for weighting the words and have compared these methods with five previous weighting techniques. The previous methods used in this paper are term frequency (TF), term frequency inverse document frequency (TF-IDF), variance, discriminative feature selection (DFS), and document length normalization based on unit words (LNU). The proposed weighting methods are based on using variance features and include variance to TF-IDF ratio, variance to TF ratio, the intersection of TF and variance, and the intersection of variance and IDF. For evaluation, the documents are clustered using the extracted keywords as feature vectors, and K-means, expectation maximization (EM), and Ward hierarchical clustering methods. The entropy of the clusters and pre-defined classes of the documents are used as the evaluation metric. For the evaluations, we have collected and labelled Persian documents. Results show that our proposed weighting method, variance to TF ratio, has the best performance for Persian. Also, the best entropy is resulted by variance to TD-IDF ratio for English.
</div>
<div style="position:relative">						
	<div id="bib_4584c077bfd17afa9e2fee951ba3b26dlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic keyphrase extraction using word embeddings</b>. <br/>
<i>Soft Computing</i>, 2019.

<br/>
Yuxiang Zhang, Huan Liu, Suge Wang, W. H. Ip., Wei Fan and Chunjing Xiao.
<br/>
<a href="https://doi.org/10.1007/s00500-019-03963-y">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '875f8310d1a76cfd071e53f2e008c501'); return false;" href="https://www.bibsonomy.org/bibtex/2875f8310d1a76cfd071e53f2e008c501/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '875f8310d1a76cfd071e53f2e008c501', 'https://www.bibsonomy.org/bibtex/2875f8310d1a76cfd071e53f2e008c501/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2875f8310d1a76cfd071e53f2e008c501/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_875f8310d1a76cfd071e53f2e008c501lepsky" style="display:none;border:1px dotted grey;">
Unsupervised random-walk keyphrase extraction models mainly rely on global structural information of the word graph, with nodes representing candidate words and edges capturing the co-occurrence information between candidate words. However, using word embedding method to integrate multiple kinds of useful information into the random-walk model to help better extract keyphrases is relatively unexplored. In this paper, we propose a random-walk-based ranking method to extract keyphrases from text documents using word embeddings. Specifically, we first design a heterogeneous text graph embedding model to integrate local context information of the word graph (i.e., the local word collocation patterns) with some crucial features of candidate words and edges of the word graph. Then, a novel random-walk-based ranking model is designed to score candidate words by leveraging such learned word embeddings. Finally, a new and generic similarity-based phrase scoring model using word embeddings is proposed to score phrases for selecting top-scoring phrases as keyphrases. Experimental results show that the proposed method consistently outperforms eight state-of-the-art unsupervised methods on three real datasets for keyphrase extraction.
</div>
<div style="position:relative">						
	<div id="bib_875f8310d1a76cfd071e53f2e008c501lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Evaluating automatic term extraction methods on individual documents</b>. <br/>
In: , pages 149-154.
2019.

<br/>
Antonio Šajatović, Maja Buljan, Jan Šnajder and Bojana Dalbelo Bašić.
<br/>

<a href="https://www.aclweb.org/anthology/W19-5118/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3fa9fe0ec1c9fb79fb86729244cb9313'); return false;" href="https://www.bibsonomy.org/bibtex/23fa9fe0ec1c9fb79fb86729244cb9313/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3fa9fe0ec1c9fb79fb86729244cb9313', 'https://www.bibsonomy.org/bibtex/23fa9fe0ec1c9fb79fb86729244cb9313/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23fa9fe0ec1c9fb79fb86729244cb9313/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3fa9fe0ec1c9fb79fb86729244cb9313lepsky" style="display:none;border:1px dotted grey;">
Automatic Term Extraction (ATE) extracts terminology from domain-specific corpora. ATE is used in many NLP tasks, including Computer Assisted Translation, where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated, it is not obvious how the results transfer to document-level ATE. To fill this gap, we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains, on both corpus and document levels. Unlike existing studies, our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE, but C-Value and KeyConceptRelatendess surpass others in document-level ATE.  Anthology ID:     W19-5118
</div>
<div style="position:relative">						
	<div id="bib_3fa9fe0ec1c9fb79fb86729244cb9313lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2018" class="bibsonomy_quicknav_group"><a name="2018">2018</a></h3>
<div style="margin-bottom:1em">
<b>Distributed specificity for automatic terminology extraction</b>. <br/>
<i>Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication</i>, 24(1):23-40, 2018.

<br/>
Ehsan Amjadian, Diana Inkpen, T. Sima Paribakht and Farahnaz Faez.
<br/>
<a href="http://www.jbe-platform.com/content/journals/10.1075/term.00012.amj">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c6907ead15ced8af032593b7cd34432b'); return false;" href="https://www.bibsonomy.org/bibtex/2c6907ead15ced8af032593b7cd34432b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c6907ead15ced8af032593b7cd34432b', 'https://www.bibsonomy.org/bibtex/2c6907ead15ced8af032593b7cd34432b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c6907ead15ced8af032593b7cd34432b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c6907ead15ced8af032593b7cd34432blepsky" style="display:none;border:1px dotted grey;">
The present article explores two novel methods that integrate distributed representations with terminology extraction. Both methods assess the specificity of a word (unigram) to the target corpus by leveraging its distributed representation in the target domain as well as in the general domain. The first approach adopts this distributed specificity as a filter, and the second directly applies it to the corpus. The filter can be mounted on any other Automatic Terminology Extraction (ATE) method, allows merging any number of other ATE methods, and achieves remarkable results with minimal training. The direct approach does not perform as high as the filtering approach, but it reemphasizes that using distributed specificity as the words’ representation, very little data is required to train an ATE classifier. This encourages more minimally supervised ATE algorithms in the future.
</div>
<div style="position:relative">						
	<div id="bib_c6907ead15ced8af032593b7cd34432blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic extraction of specialized verbal units</b>. <br/>
<i>Terminology</i>, 23(2):207-237, 2018.

<br/>
Nizar Ghazzawi, Benoît Robichaud, Patrick Drouin and Fatiha Sadat.
<br/>

<a onclick="toggleAbstract('lepsky', '407eed45093d4729d62d3db10cb84462'); return false;" href="https://www.bibsonomy.org/bibtex/2407eed45093d4729d62d3db10cb84462/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '407eed45093d4729d62d3db10cb84462', 'https://www.bibsonomy.org/bibtex/2407eed45093d4729d62d3db10cb84462/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2407eed45093d4729d62d3db10cb84462/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_407eed45093d4729d62d3db10cb84462lepsky" style="display:none;border:1px dotted grey;">
Abstract This paper presents a methodology for the automatic extraction of specialized Arabic, English and French verbs of the field of computing. Since nominal terms are predominant in terminology, our interest is to explore to what extent verbs can also be part of a terminological analysis. Hence, our objective is to verify how an existing extraction tool will perform when it comes to specialized verbs in a given specialized domain. Furthermore, we want to investigate any particularities that a language can represent regarding verbal terms from the automatic extraction perspective. Our choice to operate on three different languages reflects our desire to see whether the chosen tool can perform better on one language compared to the others. Moreover, given that Arabic is a morphologically rich and complex language, we consider investigating the results yielded by the extraction tool. The extractor used for our experiment is TermoStat (Drouin 2003). So far, our results show that the extraction of verbs of computing represents certain differences in terms of quality and particularities of these units in this specialized domain between the languages under question.
</div>
<div style="position:relative">						
	<div id="bib_407eed45093d4729d62d3db10cb84462lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>TEST : a terminology extraction system for technology related terms</b>. <br/>
<i>arXiv:1812.09541 [cs]</i>, 2018.
arXiv: 1812.09541
<br/>
Murhaf Hossari, Soumyabrata Dev and John D. Kelleher.
<br/>
<a href="http://arxiv.org/abs/1812.09541">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '617582348c3822712a95e23f07941b91'); return false;" href="https://www.bibsonomy.org/bibtex/2617582348c3822712a95e23f07941b91/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '617582348c3822712a95e23f07941b91', 'https://www.bibsonomy.org/bibtex/2617582348c3822712a95e23f07941b91/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2617582348c3822712a95e23f07941b91/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_617582348c3822712a95e23f07941b91lepsky" style="display:none;border:1px dotted grey;">
Tracking developments in the highly dynamic data-technology landscape are vital to keeping up with novel technologies and tools, in the various areas of Artificial Intelligence (AI). However, It is difficult to keep track of all the relevant technology keywords. In this paper, we propose a novel system that addresses this problem. This tool is used to automatically detect the existence of new technologies and tools in text, and extract terms used to describe these new technologies. The extracted new terms can be logged as new AI technologies as they are found on-the-fly in the web. It can be subsequently classified into the relevant semantic labels and AI domains. Our proposed tool is based on a two-stage cascading model -- the first stage classifies if the sentence contains a technology term or not; and the second stage identifies the technology keyword in the sentence. We obtain a competitive accuracy for both tasks of sentence classification and text identification.
</div>
<div style="position:relative">						
	<div id="bib_617582348c3822712a95e23f07941b91lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Similar term aggregation method and apparatus</b>.<br/>
2018. 
<br/>Guangyuan Huang, Jinhe Lan, Ganglin Mai and Xiaojing Shi.
<br/>
<a href="https://patents.google.com/patent/US20180293294A1/en">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f1fd919ebf70ae3a2883771623d266de'); return false;" href="https://www.bibsonomy.org/bibtex/2f1fd919ebf70ae3a2883771623d266de/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f1fd919ebf70ae3a2883771623d266de', 'https://www.bibsonomy.org/bibtex/2f1fd919ebf70ae3a2883771623d266de/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f1fd919ebf70ae3a2883771623d266de/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f1fd919ebf70ae3a2883771623d266delepsky" style="display:none;border:1px dotted grey;">
A method and an apparatus for aggregating similar terms are provided by the embodiments of the present disclosure. The method includes extracting a plurality of candidate terms having a same term property from historical labeled data of network items; separately extracting associated terms that are adjacent to the candidate terms and have term properties associated therewith from the historical labeled data; and aggregating the plurality of candidate terms based on similarity degrees of the associated terms, and labeling thereof as synonyms. Based on the embodiments of the present disclosure, similar relationships among candidate terms can be mined, and classification of synonyms can be effectively performed for unstructured and non-standardized review terms related to electronic commerce.
</div>
<div style="position:relative">						
	<div id="bib_f1fd919ebf70ae3a2883771623d266delepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Information extraction from scientific literature for method recommendation</b>. <br/>
<i>arXiv:1901.00401 [cs]</i>, 2018.
arXiv: 1901.00401
<br/>
Yi Luan.
<br/>
<a href="http://arxiv.org/abs/1901.00401">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '05a4a8982cfaf2ab336b8d1b98048b8e'); return false;" href="https://www.bibsonomy.org/bibtex/205a4a8982cfaf2ab336b8d1b98048b8e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '05a4a8982cfaf2ab336b8d1b98048b8e', 'https://www.bibsonomy.org/bibtex/205a4a8982cfaf2ab336b8d1b98048b8e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/205a4a8982cfaf2ab336b8d1b98048b8e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_05a4a8982cfaf2ab336b8d1b98048b8elepsky" style="display:none;border:1px dotted grey;">
As a research community grows, more and more papers are published each year. As a result there is increasing demand for improved methods for finding relevant papers, automatically understanding the key ideas and recommending potential methods for a target problem. Despite advances in search engines, it is still hard to identify new technologies according to a researcher's need. Due to the large variety of domains and extremely limited annotated resources, there has been relatively little work on leveraging natural language processing in scientific recommendation. In this proposal, we aim at making scientific recommendations by extracting scientific terms from a large collection of scientific papers and organizing the terms into a knowledge graph. In preliminary work, we trained a scientific term extractor using a small amount of annotated data and obtained state-of-the-art performance by leveraging large amount of unannotated papers through applying multiple semi-supervised approaches. We propose to construct a knowledge graph in a way that can make minimal use of hand annotated data, using only the extracted terms, unsupervised relational signals such as co-occurrence, and structural external resources such as Wikipedia. Latent relations between scientific terms can be learned from the graph. Recommendations will be made through graph inference for both observed and unobserved relational pairs.
</div>
<div style="position:relative">						
	<div id="bib_05a4a8982cfaf2ab336b8d1b98048b8elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>SmartPub : a platform for long-tail entity extraction from scientific publications</b>. <br/>
In: <i>Companion Proceedings of the The Web Conference 2018</i>, series WWW '18, pages 191-194.
International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 2018.

<br/>
Sepideh Mesbah, Alessandro Bozzon, Christoph Lofi and Geert-Jan Houben.
<br/>

<a href="https://doi.org/10.1145/3184558.3186976">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '56830df54c304ca051668fb2f95055fd'); return false;" href="https://www.bibsonomy.org/bibtex/256830df54c304ca051668fb2f95055fd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '56830df54c304ca051668fb2f95055fd', 'https://www.bibsonomy.org/bibtex/256830df54c304ca051668fb2f95055fd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/256830df54c304ca051668fb2f95055fd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_56830df54c304ca051668fb2f95055fdlepsky" style="display:none;border:1px dotted grey;">
This demo presents SmartPub, a novel web-based platform that supports the exploration and visualization of shallow meta-data (e.g., author list, keywords) and deep meta-data--long tail named entities which are rare, and often relevant only in specific knowledge domain--from scientific publications. The platform collects documents from different sources (e.g. DBLP and Arxiv), and extracts the domain-specific named entities from the text of the publications using Named Entity Recognizers (NERs) which we can train with minimal human supervision even for rare entity types. The platform further enables the interaction with the Crowd for filtering purposes or training data generation, and provides extended visualization and exploration capabilities. SmartPub will be demonstrated using sample collection of scientific publications focusing on the computer science domain and will address the entity types Dataset (i.e. dataset presented or used in a publication), and Methods (i.e. algorithms used to create/enrich/analyse a data set)
</div>
<div style="position:relative">						
	<div id="bib_56830df54c304ca051668fb2f95055fdlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Recognition of irrelevant phrases in automatically extracted lists of domain terms</b>. <br/>
<i>Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication</i>, 24(1):66-90, 2018.

<br/>
Agnieszka Mykowiecka, Małgorzata Marciniak and Piotr Rychlik.
<br/>
<a href="http://www.jbe-platform.com/content/journals/10.1075/term.00014.myk">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '409399236a213fc560774187da0bac1c'); return false;" href="https://www.bibsonomy.org/bibtex/2409399236a213fc560774187da0bac1c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '409399236a213fc560774187da0bac1c', 'https://www.bibsonomy.org/bibtex/2409399236a213fc560774187da0bac1c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2409399236a213fc560774187da0bac1c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_409399236a213fc560774187da0bac1clepsky" style="display:none;border:1px dotted grey;">
In our paper, we address the problem of recognition of irrelevant phrases in terminology lists obtained with an automatic term extraction tool. We focus on identification of multi-word phrases that are general terms or discourse expressions. We defined several methods based on comparison of domain corpora and a method based on contexts of phrases identified in a large corpus of general language. The methods were tested on Polish data. We used six domain corpora and one general corpus. Two test sets were prepared to evaluate the methods. The first one consisted of many presumably irrelevant phrases, as we selected phrases which occurred in at least three domain corpora. The second set mainly consisted of domain terms, as it was composed of the top-ranked phrases automatically extracted from the analyzed domain corpora.textless/ptextgreater textlessptextgreaterThe results show that the task is quite hard as the inter-annotator agreement is low. Several tested methods achieved similar overall results, although the phrase ordering varied between methods. The most successful method, with a precision of about 0.75 on half of the tested list, was the context based method using a modified contextual diversity coefficient. Although the methods were tested on Polish, they seems to be language independent.
</div>
<div style="position:relative">						
	<div id="bib_409399236a213fc560774187da0bac1clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Extracting abstract and keywords from context for academic articles</b>. <br/>
<i>Social Network Analysis and Mining</i>, 8(1):45, 2018.

<br/>
Ahmet Anıl Müngen and Mehmet Kaya.
<br/>
<a href="https://link.springer.com/article/10.1007/s13278-018-0524-z">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'cea8a04d6a8098d3eb8c21ffff73eb9b'); return false;" href="https://www.bibsonomy.org/bibtex/2cea8a04d6a8098d3eb8c21ffff73eb9b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cea8a04d6a8098d3eb8c21ffff73eb9b', 'https://www.bibsonomy.org/bibtex/2cea8a04d6a8098d3eb8c21ffff73eb9b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cea8a04d6a8098d3eb8c21ffff73eb9b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cea8a04d6a8098d3eb8c21ffff73eb9blepsky" style="display:none;border:1px dotted grey;">
Every year thousands of academic studies are published all over the world. When researchers search for a topic, they quickly look at abstracts and keywords. In many academic disciplines, the authors write keywords and abstracts in their publications. On the other hand, there are publications of some disciplines, such as social sciences which do not contain keywords and abstracted information. In addition, there may be no abstract or keyword in some of old publications in all disciplines. Search engines for academic publications usually conduct this search by checking keywords, abstracts and titles. The lack of an abstract and a keyword in the publication makes this situation difficult to provide accurate search results and it prevents the researcher to review the publication quickly. This study proposes a method to generate keywords and an abstract from the text that can be used in academic studies. In the previous studies, k-NN and fuzzy CCG methods have been generally used to solve this problem. Nonetheless, the structures of words have not been examined and semantic analysis has not been used for solving this problem. In this study, the sections of the publication are also divided into parts such as the references, the introduction and the methodology. Each section is graded differently so that the word in each section has a different score. Furthermore, NLP methods were used to analyze texts and phrases, removing prepositions and conjunctions. After these processes, the data was used to generate the keyword using TF–IDF. Text generation for abstract is also performed using the TextRank method with this data. Thus, much more successful, truthful and contextually relevant keywords and abstracts are produced. The proposed method was tested on Sobiad Academic Database, which is employed by 72 universities in Turkey, covering more than 250,000 academic publications. Experimental results were measured with precision and F measure, and the results were found to be promising compared to the previous studies, which focused on keyword derivation and abstract generation.
</div>
<div style="position:relative">						
	<div id="bib_cea8a04d6a8098d3eb8c21ffff73eb9blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Arabic nested noun compound extraction based on linguistic features and statistical measures</b>. <br/>
<i>GEMA Online® Journal of Language Studies</i>, 18(2), 2018.

<br/>
Nazlia Omar and Qasem Al-Tashi.
<br/>
<a href="http://ejournal.ukm.my/gema/article/view/25313">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4fa98f2ae4cb4576c43f4602fe42beb8'); return false;" href="https://www.bibsonomy.org/bibtex/24fa98f2ae4cb4576c43f4602fe42beb8/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4fa98f2ae4cb4576c43f4602fe42beb8', 'https://www.bibsonomy.org/bibtex/24fa98f2ae4cb4576c43f4602fe42beb8/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24fa98f2ae4cb4576c43f4602fe42beb8/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4fa98f2ae4cb4576c43f4602fe42beb8lepsky" style="display:none;border:1px dotted grey;">
The extraction of Arabic nested noun compound is significant for several research areas such as sentiment analysis, text summarization, word categorization, grammar checker, and machine translation. Much research has studied the extraction of Arabic noun compound using linguistic approaches, statistical methods, or a hybrid of both. A wide range of the existing approaches concentrate on the extraction of the bi-gram or tri-gram noun compound. Nonetheless, extracting a 4-gram or 5-gram nested noun compound is a challenging task due to the morphological, orthographic, syntactic and semantic variations. Many features have an important effect on the efficiency of extracting a noun compound such as unit-hood, contextual information, and term-hood. Hence, there is a need to improve the effectiveness of the Arabic nested noun compound extraction. Thus, this paper proposes a hybrid linguistic approach and a statistical method with a view to enhance the extraction of the Arabic nested noun compound. A number of pre-processing phases are presented, including transformation, tokenization, and normalisation. The linguistic approaches that have been used in this study consist of a part-of-speech tagging and the named entities pattern, whereas the proposed statistical methods that have been used in this study consist of the NC-value, NTC-value, NLC-value, and the combination of these association measures. The proposed methods have demonstrated that the combined association measures have outperformed the NLC-value, NTC-value, and NC-value in terms of nested noun compound extraction by achieving 90 88 87 and 81%for bigram, trigram, 4-gram, and 5-gram, respectively.
</div>
<div style="position:relative">						
	<div id="bib_4fa98f2ae4cb4576c43f4602fe42beb8lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Improving term candidates selection using terminological tokens</b>. <br/>
<i>Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication</i>, 24(1):122-147, 2018.

<br/>
Mercè Vàzquez and Antoni Oliver.
<br/>
<a href="http://www.jbe-platform.com/content/journals/10.1075/term.00016.vaz">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'eabfd9228fd32a20d7e6d5a5f8464d77'); return false;" href="https://www.bibsonomy.org/bibtex/2eabfd9228fd32a20d7e6d5a5f8464d77/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'eabfd9228fd32a20d7e6d5a5f8464d77', 'https://www.bibsonomy.org/bibtex/2eabfd9228fd32a20d7e6d5a5f8464d77/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2eabfd9228fd32a20d7e6d5a5f8464d77/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_eabfd9228fd32a20d7e6d5a5f8464d77lepsky" style="display:none;border:1px dotted grey;">
The identification of reliable terms from domain-specific corpora using computational methods is a task that has to be validated manually by specialists, which is a highly time-consuming activity. To reduce this effort and improve term candidate selection, we implemented the Token Slot Recognition method, a filtering method based on terminological tokens which is used to rank extracted term candidates from domain-specific corpora. This paper presents the implementation of the term candidates filtering method we developed in linguistic and statistical approaches applied for automatic term extraction using several domain-specific corpora in different languages. We observed that the filtering method outperforms term candidate selection by ranking a higher number of terms at the top of the term candidate list than raw frequency, and for statistical term extraction the improvement is between 15%and 25%both in precision and recall. Our analyses further revealed a reduction in the number of term candidates to be validated manually by specialists. In conclusion, the number of term candidates extracted automatically from domain-specific corpora has been reduced significantly using the Token Slot Recognition filtering method, so term candidates can be easily and quickly validated by specialists.
</div>
<div style="position:relative">						
	<div id="bib_eabfd9228fd32a20d7e6d5a5f8464d77lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2017" class="bibsonomy_quicknav_group"><a name="2017">2017</a></h3>
<div style="margin-bottom:1em">
<b>Extraction of terms and semantic relationships from Arabic texts for automatic construction of an ontology</b>. <br/>
<i>International Journal of Speech Technology</i>:1-8, 2017.

<br/>
Ali Benabdallah, Mohammed AlaEddine Abderrahim and Mohammed El-Amine Abderrahim.
<br/>
<a href="https://link.springer.com/article/10.1007/s10772-017-9405-5">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '9b06a8e843dc1f9da72fd64e45c3e659'); return false;" href="https://www.bibsonomy.org/bibtex/29b06a8e843dc1f9da72fd64e45c3e659/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '9b06a8e843dc1f9da72fd64e45c3e659', 'https://www.bibsonomy.org/bibtex/29b06a8e843dc1f9da72fd64e45c3e659/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/29b06a8e843dc1f9da72fd64e45c3e659/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_9b06a8e843dc1f9da72fd64e45c3e659lepsky" style="display:none;border:1px dotted grey;">
The task of building an ontology from a textual corpus starts with the conceptualization phase, which extracts ontology concepts. These concepts are linked by semantic relationships. In this paper, we describe an approach to the construction of an ontology from an Arabic textual corpus, starting first with the collection and preparation of the corpus through normalization, removing stop words and stemming; then, to extract terms of our ontology, a statistical method for extracting simple and complex terms, called “the repeated segments method” are applied. To select segments with sufficient weight we apply the weighting method term frequency–inverse document frequency (TF–IDF), and to link these terms by semantic relationships we apply an automatic method of learning linguistic markers from text. This method requires a dataset of relationship pairs, which are extracted from two external resources: an Arabic dictionary of synonyms and antonyms and the lexical database Arabic WordNet. Finally, we present the results of our experimentation using our textual corpus. The evaluation of our approach shows encouraging results in terms of recall and precision.
</div>
<div style="position:relative">						
	<div id="bib_9b06a8e843dc1f9da72fd64e45c3e659lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>RENT : regular expression and NLP-based term extraction scheme for agricultural domain</b>.<br/>
In: 
S. C. Satapathy, V. Bhateja and A. Joshi, editors, 
<i>Proceedings of the International Conference on Data Engineering and Communication Technology</i>, pages 511-522.
Springer, Singapore, 2017.

<br/>
Niladri Chatterjee and Neha Kaushik.
<br/>

<a href="http://link.springer.com/chapter/10.1007/978-981-10-1675-2_51">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f5ba04d3c2fa684ea2cd61e88c843a05'); return false;" href="https://www.bibsonomy.org/bibtex/2f5ba04d3c2fa684ea2cd61e88c843a05/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f5ba04d3c2fa684ea2cd61e88c843a05', 'https://www.bibsonomy.org/bibtex/2f5ba04d3c2fa684ea2cd61e88c843a05/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f5ba04d3c2fa684ea2cd61e88c843a05/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f5ba04d3c2fa684ea2cd61e88c843a05lepsky" style="display:none;border:1px dotted grey;">
This paper addresses the task of automatic term extraction in agricultural domain. There is a paramount call for applying effective data processing on a huge amount of agricultural data lying unprocessed. The method is based on basic techniques in Named-entity recognition, and involves a resequencing of the conventional procedure of automatic term extraction. Several domain-specific patterns identified by the domain experts have been used for this purpose in the baseline algorithm. After evaluating the performance of baseline, several improvements have been proposed by observing the obtained results on a given agricultural text. These improvements have been incorporated into the RENT algorithm. Both the algorithms have been applied on more than 1400 pages of agricultural text. It is concluded that the RENT algorithm significantly outperforms the baseline algorithm with a precision of more than 80  recall more than 60 %and f-measure more than 68 %on random samples. A comparison with the Termine, a well-known software for term extraction, is also presented which shows that RENT has a better precision.
</div>
<div style="position:relative">						
	<div id="bib_f5ba04d3c2fa684ea2cd61e88c843a05lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Domain-independent term extraction and term network for scientific publications</b>. <br/>
, 2017.

<br/>
Zheng Chen and Erjia Yan.
<br/>
<a href="https://www.ideals.illinois.edu/handle/2142/96671">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '241dd2210f731ffb937081c19665cccf'); return false;" href="https://www.bibsonomy.org/bibtex/2241dd2210f731ffb937081c19665cccf/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '241dd2210f731ffb937081c19665cccf', 'https://www.bibsonomy.org/bibtex/2241dd2210f731ffb937081c19665cccf/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2241dd2210f731ffb937081c19665cccf/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_241dd2210f731ffb937081c19665cccflepsky" style="display:none;border:1px dotted grey;">
Term extraction is an essential tool for content-based publication analysis, and has a long history dating back to 1970s. However, previous methods are either domain-specific, or need complex model training, or relies on external resources like Wikipedia. Recent rise of cross-domain publication content analyses has put forward the demand for simple and efficient domain-independent extraction method. This paper proposes a new rule-based method that adapts C-value method to publication analysis, extends it with two types of frequency lists and sigmoid functions, and develops a prototype term extraction system. Our experiment shows a remarkable reduction of “error” with better or competitive “keyword recall” against C-Value method and a complex term extraction method provided by Translated.net. We then construct a term network by connecting adjacent terms in a paragraph and demonstrate that rich and meaningful analysis can be done on such network through a case study on an HCI abstract corpus.
</div>
<div style="position:relative">						
	<div id="bib_241dd2210f731ffb937081c19665cccflepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An efficient approach for keyphrase extraction from english document</b>. <br/>
<i>International Journal of Intelligent Systems and Applications(IJISA)</i>, 9(12):59-66, 2017.

<br/>
Imtiaz Hossain Emu, Asraf Uddin Ahmed, Manowarul Islam, Selim Al Mamun and Ashraf Uddin.
<br/>
<a href="http://www.mecs-press.org/ijisa/ijisa-v9-n12/v9n12-6.html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4ac282b2ef3bfd5c51f227ea8bb4ed3c'); return false;" href="https://www.bibsonomy.org/bibtex/24ac282b2ef3bfd5c51f227ea8bb4ed3c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4ac282b2ef3bfd5c51f227ea8bb4ed3c', 'https://www.bibsonomy.org/bibtex/24ac282b2ef3bfd5c51f227ea8bb4ed3c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24ac282b2ef3bfd5c51f227ea8bb4ed3c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4ac282b2ef3bfd5c51f227ea8bb4ed3clepsky" style="display:none;border:1px dotted grey;">
Keyphrases are set of words that reflect the main topic of interest of a document. It plays vital roles in document summarization, text mining, and retrieval of web contents. As it is closely related to a document, it reflects the contents of the document and acts as indices for a given document. Extracting the ideal keyphrases is important to understand the main contents of the document. In this work, we present a keyphrase extraction method that efficiently finds the keywords from English documents. The methods use some important features of the document such as TF, TF*IDF, GF, GF*IDF, TF*GF*IDF for the purpose. Finally, the performance of the proposal is evaluated using well-known document corpus.
</div>
<div style="position:relative">						
	<div id="bib_4ac282b2ef3bfd5c51f227ea8bb4ed3clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Cross-evaluation of automated term extraction tools</b>.<br/>
2017. 
<br/>Victoria Kosa, David Chaves-Fraga, Dmitriy Naumenko, Eugene Yuschenko, Carlos Badenes-Olmedo, Vadim Ermolayev and Aliaksandr Birukou.
<br/>

<a onclick="toggleAbstract('lepsky', '0b44779db415cf5c0525de81db6bc875'); return false;" href="https://www.bibsonomy.org/bibtex/20b44779db415cf5c0525de81db6bc875/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0b44779db415cf5c0525de81db6bc875', 'https://www.bibsonomy.org/bibtex/20b44779db415cf5c0525de81db6bc875/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20b44779db415cf5c0525de81db6bc875/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0b44779db415cf5c0525de81db6bc875lepsky" style="display:none;border:1px dotted grey;">
This document reports on our activity in cross-evaluating the two freely available software tools for automated term extraction (ATE) from English texts: NaCTeM TerMine and UPM Term Extractor. The objective to do this cross evaluation was to find the most fitting software for extracting the bags of terms to be the part of our instrumental pipeline for exploring terminological saturation in professional text document collections in a domain of interest. The choice of these particular tools from the bunch of the other available is explained in our review of the related work in ATE. The approach to measure terminological saturation is based on the use of the THD algorithm developed in frame of our OntoElect methodology for ontology refinement. The report presents the suite of instrumental software modules, experimental workflow, 2 synthetic and 3 real document collections, generated datasets, and the setup of our experiments. The results of the cross-evaluation experiments are further presented, analyzed, and discussed. Finally the report offers some conclusions and recommendations on the use of ATE software for measuring terminological saturation in retrospective text document collections.
</div>
<div style="position:relative">						
	<div id="bib_0b44779db415cf5c0525de81db6bc875lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Reference string extraction using line-based conditional random fields</b>. <br/>
<i>arXiv:1705.08154 [cs]</i>, 2017.
arXiv: 1705.08154
<br/>
Martin Körner.
<br/>
<a href="http://arxiv.org/abs/1705.08154">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '36429325c6a45a2bf4648473671029a6'); return false;" href="https://www.bibsonomy.org/bibtex/236429325c6a45a2bf4648473671029a6/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '36429325c6a45a2bf4648473671029a6', 'https://www.bibsonomy.org/bibtex/236429325c6a45a2bf4648473671029a6/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/236429325c6a45a2bf4648473671029a6/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_36429325c6a45a2bf4648473671029a6lepsky" style="display:none;border:1px dotted grey;">
The extraction of individual reference strings from the reference section of scientific publications is an important step in the citation extraction pipeline. Current approaches divide this task into two steps by first detecting the reference section areas and then grouping the text lines in such areas into reference strings. We propose a classification model that considers every line in a publication as a potential part of a reference string. By applying line-based conditional random fields rather than constructing the graphical model based on the individual words, dependencies and patterns that are typical in reference sections provide strong features while the overall complexity of the model is reduced.
</div>
<div style="position:relative">						
	<div id="bib_36429325c6a45a2bf4648473671029a6lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A syntactic method of extracting terms from special texts for replenishing domain ontologies</b>. <br/>
In: <i>2017 Second Russia and Pacific Conference on Computer Technology and Applications (RPC)</i>, pages 127-131.
2017.

<br/>
O. Nevzorova, V. Nevzorov and A. Kirillovich.
<br/>


<a onclick="toggleAbstract('lepsky', '1ea48d9884197643b264072f08e1b79a'); return false;" href="https://www.bibsonomy.org/bibtex/21ea48d9884197643b264072f08e1b79a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1ea48d9884197643b264072f08e1b79a', 'https://www.bibsonomy.org/bibtex/21ea48d9884197643b264072f08e1b79a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21ea48d9884197643b264072f08e1b79a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1ea48d9884197643b264072f08e1b79alepsky" style="display:none;border:1px dotted grey;">
Natural Language Processing (NLP) is one of the principal areas of artificial intelligence. It can be argued that the use of ontologies increases the efficiency of natural language processing. However, most ontologies are built manually and require a lot of work. Thus, the problem of automated ontology replenishment is very relevant. One approach is to develop methods for replenishing ontologies using NLP for specific texts of a certain area. We applied the developed method of replenishing the OntoMathPro mathematical ontology, by extracting new terminology from mathematical documents. We developed a method for processing complex syntactic structures (structures with coordination reduction). The method includes certain rule schemata, conditions under which they are to be applied, and conditions determining the sequence of subtrees for which they are to be performed. In our studies, we investigated typical coordination models for mathematical works and performed experiments with a big mathematical collection.
</div>
<div style="position:relative">						
	<div id="bib_1ea48d9884197643b264072f08e1b79alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Term annotation guidelines 1.0</b>. <br/>
, 2017.

<br/>
Ayla Rigouts Terryn.
<br/>
<a href="http://hdl.handle.net/1854/LU-8503113">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'db52b71840a9b58013fc206c7f608ada'); return false;" href="https://www.bibsonomy.org/bibtex/2db52b71840a9b58013fc206c7f608ada/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'db52b71840a9b58013fc206c7f608ada', 'https://www.bibsonomy.org/bibtex/2db52b71840a9b58013fc206c7f608ada/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2db52b71840a9b58013fc206c7f608ada/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_db52b71840a9b58013fc206c7f608adalepsky" style="display:none;border:1px dotted grey;">
This document contains elaborate term annotation guidelines, with an annotation scheme that has 4 different categories. Annotating specialised texts according to these guidelines will allow for an in depth evaluation of automatic term extraction.
</div>
<div style="position:relative">						
	<div id="bib_db52b71840a9b58013fc206c7f608adalepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>The exploration of information extraction and analysis about science and technology policy in China</b>. <br/>
<i>The Electronic Library</i>:00-00, 2017.

<br/>
Wen Zeng, Changqing Yao and Hui Li.
<br/>
<a href="http://www.emeraldinsight.com/doi/abs/10.1108/EL-10-2016-0235">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', '5bc40383685ab8c5033c540dd35617c4', 'https://www.bibsonomy.org/bibtex/25bc40383685ab8c5033c540dd35617c4/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25bc40383685ab8c5033c540dd35617c4/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5bc40383685ab8c5033c540dd35617c4lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_5bc40383685ab8c5033c540dd35617c4lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2016" class="bibsonomy_quicknav_group"><a name="2016">2016</a></h3>
<div style="margin-bottom:1em">
<b>An approach of automatic extraction of domain keywords from the Kazakh text</b>.<br/>
In: 
N. T. Nguyen, L. Iliadis, Y. Manolopoulos and B. Trawiński, editors, 
<i>Computational Collective Intelligence</i>, pages 555-562.
Springer International Publishing, 2016.

<br/>
Yermek Alimzhanov and Madina Mansurova.
<br/>

<a href="http://link.springer.com/chapter/10.1007/978-3-319-45246-3_53">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '2aa1cf3668e5c572e500c10db257e7d3'); return false;" href="https://www.bibsonomy.org/bibtex/22aa1cf3668e5c572e500c10db257e7d3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '2aa1cf3668e5c572e500c10db257e7d3', 'https://www.bibsonomy.org/bibtex/22aa1cf3668e5c572e500c10db257e7d3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22aa1cf3668e5c572e500c10db257e7d3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2aa1cf3668e5c572e500c10db257e7d3lepsky" style="display:none;border:1px dotted grey;">
In this paper we consider the approach of automatic extraction of domain keywords from the Kazakh Text based on statistical methods of natural language processing. The proposed approach can be used to build domain dictionaries and thesauri without manual work of domain experts. Results of experiments on a corpus of texts from a Kazakh book and online websites demonstrate that applying latent semantic analysis to keywords extraction significantly decreases information noise and strengthens the words relations.
</div>
<div style="position:relative">						
	<div id="bib_2aa1cf3668e5c572e500c10db257e7d3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Keyphrase extraction methodology from short abstracts of medical documents</b>.<br/>
In: 

<i>2016 8th Cairo International Biomedical Engineering Conference (CIBEC)</i>, pages 23-26.
2016.

<br/>
E. Amer and K. M. Fouad.
<br/>

<a href="http://ieeexplore.ieee.org/document/7836091/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '8c3a4c016d039a55695b0fec51d3edfc'); return false;" href="https://www.bibsonomy.org/bibtex/28c3a4c016d039a55695b0fec51d3edfc/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '8c3a4c016d039a55695b0fec51d3edfc', 'https://www.bibsonomy.org/bibtex/28c3a4c016d039a55695b0fec51d3edfc/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/28c3a4c016d039a55695b0fec51d3edfc/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_8c3a4c016d039a55695b0fec51d3edfclepsky" style="display:none;border:1px dotted grey;">
Keyphrases are the keywords that provide the semantic metadata about document that represents the top concepts inside an article. Number of natural language processing (NLP) methodologies; such as text summarization, text mining and information retrieval applications, consider Keyphrase Extraction as a critical step to accomplish its tasks. This paper presents a method to extract keyphrases from abstracts of medical research papers. The proposed method utilizes DBpedia to gain access to terms that may be of interest to the candidate keyphrases extracted from the original document. Experimental results showed that the proposed method outperforms other related methods in terms of higher precision and F-measure values.
</div>
<div style="position:relative">						
	<div id="bib_8c3a4c016d039a55695b0fec51d3edfclepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Temporal properties of recurring in-text references</b>. <br/>
<i>D-Lib Magazine</i>, 22(9/10):10-10, 2016.

<br/>
Iana Atanassova and Marc Bertin.
<br/>

<a onclick="toggleAbstract('lepsky', 'a442bcd3c6a3724e416e0bb8fdfbe86e'); return false;" href="https://www.bibsonomy.org/bibtex/2a442bcd3c6a3724e416e0bb8fdfbe86e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a442bcd3c6a3724e416e0bb8fdfbe86e', 'https://www.bibsonomy.org/bibtex/2a442bcd3c6a3724e416e0bb8fdfbe86e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a442bcd3c6a3724e416e0bb8fdfbe86e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a442bcd3c6a3724e416e0bb8fdfbe86elepsky" style="display:none;border:1px dotted grey;">
In this paper we study the properties of recurring in-text references in research articles and more specifically their positions in the rhetorical structure of articles and their age with respect to the citing article. We have processed a large scale corpus of approximately 80,000 papers published by PLOS (Public Library of Science). We examine the number and types of recurring in-text references, as well as their age according to positions in the IMRaD structure. (Introduction, Methods, Results, and Discussion). The results show that the age of recurring references varies considerably in all sections and journals. While they are especially dense in the Introduction section, most of them reappear in the beginning of the Results and the Discussion sections. We also observe that the beginning of the Methods and the Results sections share a significant number of recurring references.
</div>
<div style="position:relative">						
	<div id="bib_a442bcd3c6a3724e416e0bb8fdfbe86elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>How document pre-processing affects keyphrase extraction performance</b>. <br/>
<i>arXiv:1610.07809 [cs]</i>, 2016.
arXiv: 1610.07809
<br/>
Florian Boudin, Hugo Mougard and Damien Cram.
<br/>
<a href="http://arxiv.org/abs/1610.07809">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e4756a4ef94d58cd8a702dbaf0cf9a0e'); return false;" href="https://www.bibsonomy.org/bibtex/2e4756a4ef94d58cd8a702dbaf0cf9a0e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e4756a4ef94d58cd8a702dbaf0cf9a0e', 'https://www.bibsonomy.org/bibtex/2e4756a4ef94d58cd8a702dbaf0cf9a0e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e4756a4ef94d58cd8a702dbaf0cf9a0e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e4756a4ef94d58cd8a702dbaf0cf9a0elepsky" style="display:none;border:1px dotted grey;">
The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.
</div>
<div style="position:relative">						
	<div id="bib_e4756a4ef94d58cd8a702dbaf0cf9a0elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Automatische Extraktion fachterminologischer Mehrwortbegriffe : ein Verfahrensvergleich</b>.
<br/>
PhD thesis, Universität Trier; Fachbereich II; Studiengang Computerlinguistik, Trier, 2016.

<br/>
Juliane Bredack.
<br/>

<a onclick="toggleAbstract('lepsky', 'e135a792340fa2022075feb1965c1f13'); return false;" href="https://www.bibsonomy.org/bibtex/2e135a792340fa2022075feb1965c1f13/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e135a792340fa2022075feb1965c1f13', 'https://www.bibsonomy.org/bibtex/2e135a792340fa2022075feb1965c1f13/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e135a792340fa2022075feb1965c1f13/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e135a792340fa2022075feb1965c1f13lepsky" style="display:none;border:1px dotted grey;">
Terminologieextraktion ist eine wichtige Aufgabenstellung innerhalb der Computerlinguistik. Die Literatur zum Thema ist zahlreich, die eingesetzten Verfahren stammen überwiegend aus den Bereichen POS-Tagging, Chunking, Parsing und Textstatistik. Einzeln oder in Kombination dürfen sie als das klassische Instrumentarium zur Terminologieextraktion gelten. Mit der wachsenden Bedeutung der Zielsetzungen in Richtung Semantik ist auch die Identifzierung und Extraktion von Mehrwortgruppen zunehmend interessanter und wichtiger geworden. Auch hier dominieren die „klassischen“ CL-Ansätze. Für das Deutsche und andere stark  flektierende Sprachen spielen schon immer auch wörterbuchbasierte Ansätze zur Sprachverarbeitung eine große Rolle. Geprägt durch das Einsatzgebiet einer automatischen Indexierung stehen Funktionen wie Lemmatisierung und Dekomposition im Fokus, zunehmend aber auch algorithmische und wörterbuchgestützte Verfahren der Mehrworterkennung. Vergleichende Untersuchungen zur Leistungsfähigkeit beider Ansätze sind nicht bekannt, wie überhaupt die Evaluierung von CL-Verfahren ein gerne vernachlässigter Zweig der Forschung ist. Hier setzt die vorliegende Arbeit an, die auf der Basis der Verarbeitung einer Referenzkollektion „klassische“ und wörterbuchgestützte Ansätze zur Mehrwortextraktion einem Verfahrensvergleich unterzieht. Sie befindet sich damit im Schnittbereich von Computerlinguistik und Informationswissenschaft.
</div>
<div style="position:relative">						
	<div id="bib_e135a792340fa2022075feb1965c1f13lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Comparison of automatic keyphrase extraction systems in scientific papers</b>. <br/>
<i>Research in Computing Science</i>, 115:181-191, 2016.

<br/>
Jesús Ernesto Padilla Camacho, Yulia Ledeneva and René Arnulfo García Hernández.
<br/>
<a href="http://www.rcs.cic.ipn.mx/rcs/2016_115/Comparison%20of%20Automatic%20Keyphrase%20Extraction%20Systems%20in%20Scientific%20Papers.html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1762be8e9ebe35a70b6e3d18c5f27e52'); return false;" href="https://www.bibsonomy.org/bibtex/21762be8e9ebe35a70b6e3d18c5f27e52/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1762be8e9ebe35a70b6e3d18c5f27e52', 'https://www.bibsonomy.org/bibtex/21762be8e9ebe35a70b6e3d18c5f27e52/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21762be8e9ebe35a70b6e3d18c5f27e52/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1762be8e9ebe35a70b6e3d18c5f27e52lepsky" style="display:none;border:1px dotted grey;">
Nowadays the amount of digital information that found in internet has considerably increased that is why online search is needed to automatically find the  corresponding documents. These  documents must be verified  in  order  to know whether they contain the required information. A way to simplify the online search is using keywords or keyphrases since they act as filters within a search field. The paper presents  the  comparison  of  automatic  keyphrases extraction scientific papers used in task 5 of SemEval-2010 which  calls “Automatic keyphrase extraction from scientific articles”.  In  the experimental section, the results are presented for installable and online systems.  We found systems that can  match  better the  author-,  reader-,  and  combined- assigned  keyphrases  with  the  keyphrases proposed  by  an  expert.  Finally,  the obtained results are compared to the results obtained in task 5 of SemEval-2010.
</div>
<div style="position:relative">						
	<div id="bib_1762be8e9ebe35a70b6e3d18c5f27e52lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Event extraction from documents</b>.<br/>
2016. 
<br/>Jeffrey D. Carpenter.
<br/>
<a href="https://patents.google.com/patent/US20170357625A1/en">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '479fcd81ce42bbb8947bc95fd12db599'); return false;" href="https://www.bibsonomy.org/bibtex/2479fcd81ce42bbb8947bc95fd12db599/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '479fcd81ce42bbb8947bc95fd12db599', 'https://www.bibsonomy.org/bibtex/2479fcd81ce42bbb8947bc95fd12db599/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2479fcd81ce42bbb8947bc95fd12db599/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_479fcd81ce42bbb8947bc95fd12db599lepsky" style="display:none;border:1px dotted grey;">
Systems and methods are provided for indexing a document according to identified events. An event-based indexing system includes a source interface configured to receive the document from an associated data source and format the document for processing and an indexer configured to extract event mentions from the document, with a given event mention comprising a verb and at least one of a subject and an object of the verb. A document index is configured to store the extracted event mentions such that a given document from an associated document corpus can be retrieved according to its associated event mentions
</div>
<div style="position:relative">						
	<div id="bib_479fcd81ce42bbb8947bc95fd12db599lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Knowledge Extraction for Information Retrieval</b>.<br/>
2016. 
<br/>Francesco Corcoglioniti, Mauro Dragoni, Marco Rospocher and Alessio Palmero Aprosio.
<br/>
<a href="https://dkm-static.fbk.eu/people/rospocher/files/pubs/2016eswc.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7cceeccf69bf12ea2e802b561f87f514'); return false;" href="https://www.bibsonomy.org/bibtex/27cceeccf69bf12ea2e802b561f87f514/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7cceeccf69bf12ea2e802b561f87f514', 'https://www.bibsonomy.org/bibtex/27cceeccf69bf12ea2e802b561f87f514/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27cceeccf69bf12ea2e802b561f87f514/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7cceeccf69bf12ea2e802b561f87f514lepsky" style="display:none;border:1px dotted grey;">
Document retrieval is the task of returning relevant textual resources for a given user query. In this paper, we investigate whether the semantic analysis of the query and the documents, obtained exploit- ing state-of-the-art Natural Language Processing techniques (e.g., Entity Linking, Frame Detection) and Semantic Web resources (e.g., YAGO, DBpedia), can improve the performances of the traditional term-based similarity approach. Our experiments, conducted on a recently released document collection, show that Mean Average Precision (MAP) increases of 3.5 percentage points when combining textual and semantic analysis, thus suggesting that semantic content can effectively improve the per- formances of Information Retrieval systems.
</div>
<div style="position:relative">						
	<div id="bib_7cceeccf69bf12ea2e802b561f87f514lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A framework for keyphrase extraction from scientific journals</b>.<br/>
In: 

.
Montreal, 2016.

<br/>
Vidas Daudaravicius.
<br/>

<a href="http://cs.unibo.it/save-sd/2016/papers/pdf/daudaravicius-savesd2016.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e786050501369a5765a6fb5d8418dd0e'); return false;" href="https://www.bibsonomy.org/bibtex/2e786050501369a5765a6fb5d8418dd0e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e786050501369a5765a6fb5d8418dd0e', 'https://www.bibsonomy.org/bibtex/2e786050501369a5765a6fb5d8418dd0e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e786050501369a5765a6fb5d8418dd0e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e786050501369a5765a6fb5d8418dd0elepsky" style="display:none;border:1px dotted grey;">
We present a framework for keyphrase extraction from scientific journals in diverse research fields. While journal articles are often provided with manually assigned keywords, it is not clear how to automatically extract keywords and measure their significance for a set of journal articles. We compare extracted keyphrases from journals in the fields of astrophysics, mathematics, physics, and computer science. We show that the presented statistics-based framework is able to demonstrate differences among journals, and that the extracted keyphrases can be used to represent journal or conference research topics, dynamics, and specificity.
</div>
<div style="position:relative">						
	<div id="bib_e786050501369a5765a6fb5d8418dd0elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Unsupervised relation extraction in specialized corpora using sequence mining</b>.<br/>
In: 
H. Boström, A. Knobbe, C. Soares and P. Papapetrou, editors, 
<i>Advances in Intelligent Data Analysis XV</i>, pages 237-248.
Springer International Publishing, 2016.

<br/>
Kata Gábor, Haïfa Zargayouna, Isabelle Tellier, Davide Buscaldi and Thierry Charnois.
<br/>

<a href="http://link.springer.com/chapter/10.1007/978-3-319-46349-0_21">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '31dfffecd1905c9bcb72b3b6131f2f4b'); return false;" href="https://www.bibsonomy.org/bibtex/231dfffecd1905c9bcb72b3b6131f2f4b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '31dfffecd1905c9bcb72b3b6131f2f4b', 'https://www.bibsonomy.org/bibtex/231dfffecd1905c9bcb72b3b6131f2f4b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/231dfffecd1905c9bcb72b3b6131f2f4b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_31dfffecd1905c9bcb72b3b6131f2f4blepsky" style="display:none;border:1px dotted grey;">
This paper deals with the extraction of semantic relations from scientific texts. Pattern-based representations are compared to word embeddings in unsupervised clustering experiments, according to their potential to discover new types of semantic relations and recognize their instances. The results indicate that sequential pattern mining can significantly improve pattern-based representations, even in a completely unsupervised setting.
</div>
<div style="position:relative">						
	<div id="bib_31dfffecd1905c9bcb72b3b6131f2f4blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Extracting topical phrases from clinical documents</b>. <br/>
In: <i>Thirtieth AAAI Conference on Artificial Intelligence</i>.
2016.

<br/>
Yulan He.
<br/>

<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11771">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'cb1c597ceff651602bf41ad13cdb46bd'); return false;" href="https://www.bibsonomy.org/bibtex/2cb1c597ceff651602bf41ad13cdb46bd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cb1c597ceff651602bf41ad13cdb46bd', 'https://www.bibsonomy.org/bibtex/2cb1c597ceff651602bf41ad13cdb46bd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cb1c597ceff651602bf41ad13cdb46bd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cb1c597ceff651602bf41ad13cdb46bdlepsky" style="display:none;border:1px dotted grey;">
In clinical documents, medical terms are often expressed in multi-word phrases. Traditional topic modelling approaches relying on the "bag-of-words" assumption are not effective in extracting topic themes from clinical documents. This paper proposes to first extract medical phrases using an off-the-shelf tool for medical concept mention extraction, and then train a topic model which takes a hierarchy of Pitman-Yor processes as prior for modelling the generation of phrases of arbitrary length. Experimental results on patients' discharge summaries show that the proposed approach outperforms the state-of-the-art topical phrase extraction model on both perplexity and topic coherence measure and finds more interpretable topics.
</div>
<div style="position:relative">						
	<div id="bib_cb1c597ceff651602bf41ad13cdb46bdlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Software keyphrase extraction with domain-specific features</b>. <br/>
In: <i>2016 International Conference on Advanced Computing and Applications (ACOMP)</i>, pages 43-50.
2016.

<br/>
O. Karnalim.
<br/>


<a onclick="toggleAbstract('lepsky', 'e1ea068bfe8b7b47a045e310d4693d97'); return false;" href="https://www.bibsonomy.org/bibtex/2e1ea068bfe8b7b47a045e310d4693d97/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e1ea068bfe8b7b47a045e310d4693d97', 'https://www.bibsonomy.org/bibtex/2e1ea068bfe8b7b47a045e310d4693d97/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e1ea068bfe8b7b47a045e310d4693d97/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e1ea068bfe8b7b47a045e310d4693d97lepsky" style="display:none;border:1px dotted grey;">
Despite the fact that keyphrase is widely used as a brief summary to represent documents, most keyphrase extraction is only focused on arbitrary text. However, many document types have specific behavior which require particular pre-processing in order to extract keyphrases. In software domain, keyphrases can only be extracted by utilizing reverse-engineering approach and applying several conversion rules. This paper proposes a mechanism to extract software keyphrases with domain-specific features. For our case study, our proposed method is applied to Java Archive, a distributional form of Java binaries. Besides pre-processing and conversion rules, our method also utilizes the combination of supervised and unsupervised keyphrase extraction approach to exploit the benefits of both approaches. Furthermore, in order to extract keyphrase pattern more accurately, software-related features are also incorporated besides standard keyphrase extraction features. These features are software structure, software-related natural language text, and software term association. Based on overall evaluation, our proposed method yields moderate R-precision. Thus, our approach is quite considerable to be applied for extracting software keyphrase.
</div>
<div style="position:relative">						
	<div id="bib_e1ea068bfe8b7b47a045e310d4693d97lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Meshable : searching PubMed abstracts by utilizing MeSH and MeSH-derived topical terms</b>. <br/>
<i>Bioinformatics</i>:btw331, 2016.

<br/>
Sun Kim, Lana Yeganova and W. John Wilbur.
<br/>
<a href="http://bioinformatics.oxfordjournals.org/content/early/2016/06/09/bioinformatics.btw331">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f41eabb03cb805ed7b54b620c935727c'); return false;" href="https://www.bibsonomy.org/bibtex/2f41eabb03cb805ed7b54b620c935727c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f41eabb03cb805ed7b54b620c935727c', 'https://www.bibsonomy.org/bibtex/2f41eabb03cb805ed7b54b620c935727c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f41eabb03cb805ed7b54b620c935727c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f41eabb03cb805ed7b54b620c935727clepsky" style="display:none;border:1px dotted grey;">
Summary: MeSH® is a controlled vocabulary for indexing and searching biomedical literature. MeSH terms and subheadings are organized in a hierarchical structure and are used to indicate the topics of an article. Biologists can use either MeSH terms as queries or the MeSH interface provided in PubMed® for searching PubMed abstracts. However, these are rarely used, and there is no convenient way to link standardized MeSH terms to user queries. Here, we introduce a web interface which allows users to enter queries to find MeSH terms closely related to the queries. Our method relies on co-occurrence of text words and MeSH terms to find keywords that are related to each MeSH term. A query is then matched with the keywords for MeSH terms, and candidate MeSH terms are ranked based on their relatedness to the query. The experimental results show that our method achieves the best performance among several term extraction approaches in terms of topic coherence. Moreover, the interface can be effectively used to find full names of abbreviations and to disambiguate user queries. Availability: https://www.ncbi.nlm.nih.gov/IRET/MESHABLE/ Contact: sun.kimatnih.gov Supplementary information: Supplementary data are available at Bioinformatics online.
</div>
<div style="position:relative">						
	<div id="bib_f41eabb03cb805ed7b54b620c935727clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Rule-based multi-word term extraction, lemmatization and description</b>. <br/>
, 2016.
bibtex: krstevrule
<br/>
Cvetana Krstev, Ranka Stanković and Dǔsko Vitas.
<br/>
<a href="http://typo.uni-konstanz.de/parseme/images/Meeting/2016-09-26-Dubrovnik-meeting/PARSEME_7_paper_26.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3e7bdf5f4a77c2eaf5543548f5a250a3'); return false;" href="https://www.bibsonomy.org/bibtex/23e7bdf5f4a77c2eaf5543548f5a250a3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3e7bdf5f4a77c2eaf5543548f5a250a3', 'https://www.bibsonomy.org/bibtex/23e7bdf5f4a77c2eaf5543548f5a250a3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23e7bdf5f4a77c2eaf5543548f5a250a3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3e7bdf5f4a77c2eaf5543548f5a250a3lepsky" style="display:none;border:1px dotted grey;">
In this paper we present a rule-based method for multi-word term (MWT) extraction and lemmatization of extracted multi-word terms. Extracted and lemmatized MWT candidates are post-processed using data-driven and heuristic approach in order to reject falsely offered lemmas (“parasite lemmas”) and then ranked by calculating various measures before passing them to human evaluators. For accepted terms dictionary entries are automatically produced that enable generation of all terms’ inflected forms. All subtasks of this process are integrated into a tool for development and management of lexical resources LeXimir (Stanković et al., 2016).
</div>
<div style="position:relative">						
	<div id="bib_3e7bdf5f4a77c2eaf5543548f5a250a3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>SemGraph : extracting keyphrases following a novel semantic graph-based approach</b>. <br/>
<i>Journal of the Association for Information Science and Technology</i>, 67(1):71-82, 2016.

<br/>
Juan Martinez-Romo, Lourdes Araujo and Andres Duque Fernandez.
<br/>
<a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.23365/abstract">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '149f298924852d392e33fc187ba08095'); return false;" href="https://www.bibsonomy.org/bibtex/2149f298924852d392e33fc187ba08095/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '149f298924852d392e33fc187ba08095', 'https://www.bibsonomy.org/bibtex/2149f298924852d392e33fc187ba08095/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2149f298924852d392e33fc187ba08095/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_149f298924852d392e33fc187ba08095lepsky" style="display:none;border:1px dotted grey;">
Keyphrases represent the main topics a text is about. In this article, we introduce SemGraph, an unsupervised algorithm for extracting keyphrases from a collection of texts based on a semantic relationship graph. The main novelty of this algorithm is its ability to identify semantic relationships between words whose presence is statistically significant. Our method constructs a co-occurrence graph in which words appearing in the same document are linked, provided their presence in the collection is statistically significant with respect to a null model. Furthermore, the graph obtained is enriched with information from WordNet. We have used the most recent and standardized benchmark to evaluate the system ability to detect the keyphrases that are part of the text. The result is a method that achieves an improvement of 5.3%and 7.28%in F measure over the two labeled sets of keyphrases used in the evaluation of SemEval-2010.
</div>
<div style="position:relative">						
	<div id="bib_149f298924852d392e33fc187ba08095lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Utilising a statistical inequality for efficiently finding term sets</b>. <br/>
<i>Information Processing &amp; Management</i>, 52(6):1086-1121, 2016.

<br/>
Massimo Melucci.
<br/>

<a onclick="toggleAbstract('lepsky', 'd054f22e080c773937d38b376cd03261'); return false;" href="https://www.bibsonomy.org/bibtex/2d054f22e080c773937d38b376cd03261/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd054f22e080c773937d38b376cd03261', 'https://www.bibsonomy.org/bibtex/2d054f22e080c773937d38b376cd03261/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d054f22e080c773937d38b376cd03261/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d054f22e080c773937d38b376cd03261lepsky" style="display:none;border:1px dotted grey;">
Information Retrieval (IR) systems aim to find sets of terms that discriminate documents and often exploit frequency as an evidence that signals a non-random set of terms. Frequent Itemset (FI) mining refers to a class of algorithms that can be applied to IR to find non-random set of terms. Finding FIs is a very expensive computational task because of the exponential number of itemsets. To reduce this cost, many approaches to mining FIs are based on the monotonicity property that an itemset is frequent only if all its subsets are frequent. However, it is still uncertain whether an itemset is frequent if all its subsets are frequent, thus requiring additional scans and eventually computational cost. We introduce a statistical inequality called Bell-Wigner Inequality (BWI) as a conceptual enhancement of monotonicity to predict with certainty when an itemset is frequent and when it is infrequent. Using both data mining datasets and a large IR test collection, an empirical validation shows that the BWI can significantly reduce computational cost.
</div>
<div style="position:relative">						
	<div id="bib_d054f22e080c773937d38b376cd03261lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Term frequencies for 235k language and literature texts</b>. <br/>
, 2016.

<br/>
Peter Organisciak.
<br/>
<a href="https://www.ideals.illinois.edu/handle/2142/89515">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7f4ccdf8798c7009d023e82b4063ad51'); return false;" href="https://www.bibsonomy.org/bibtex/27f4ccdf8798c7009d023e82b4063ad51/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7f4ccdf8798c7009d023e82b4063ad51', 'https://www.bibsonomy.org/bibtex/27f4ccdf8798c7009d023e82b4063ad51/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27f4ccdf8798c7009d023e82b4063ad51/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7f4ccdf8798c7009d023e82b4063ad51lepsky" style="display:none;border:1px dotted grey;">
Corpus-level term statistics are valuable for numerous text analysis activities, such as term weighting or probability distribution smoothing. In instances where there is an insufficient corpus to calculate such statistics, falling back on a general corpus of similar texts is useful.    This dataset provides statistics for a collection of 235k books from the HathiTrust that are classified as Language and Literature (i.e. class P in LCC). For each term seen in these books, book frequency, page frequency, and term frequency are provided. Book frequency is the count of books that the term is seen in, page frequency is the number of pages that have the term, and term frequency is the overall count of the term. This data is derived from the holding of the HathiTrust, using the Extracted Features dataset from the HathiTrust Research Center.
</div>
<div style="position:relative">						
	<div id="bib_7f4ccdf8798c7009d023e82b4063ad51lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Extraction of entities and concepts from finnish texts</b>.
<br/>
PhD thesis, Aalto University, School of Science, Espoo, 2016.

<br/>
Minna Tamper.
<br/>
<a href="http://seco.cs.aalto.fi/publications/2016/tamper-msc-thesis-2016.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '5498fd6db5013012af0fcd9d0767bf4c'); return false;" href="https://www.bibsonomy.org/bibtex/25498fd6db5013012af0fcd9d0767bf4c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5498fd6db5013012af0fcd9d0767bf4c', 'https://www.bibsonomy.org/bibtex/25498fd6db5013012af0fcd9d0767bf4c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25498fd6db5013012af0fcd9d0767bf4c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5498fd6db5013012af0fcd9d0767bf4clepsky" style="display:none;border:1px dotted grey;">
Keywords are used in many document databases to improve search. The process of assigning keywords from controlled vocabularies to a document is called subject indexing. If the controlled vocabulary used for indexing is an ontology, with semantic relations and descriptions of concepts, the process is also called semantic annotation. In this thesis an automatic annotation tool was created to provide the documents with semantic annotations. The application links entities found from the texts to ontologies defined by the user. The application is highly configurable and can be used with different Finnish texts. The application was developed as a part of WarSampo and Semantic Finlex projects and tested using Kansa Taisteli magazine articles and consolidated legislation of Finnish legislation. The quality of the automatic annotation was evaluated by measuring precision and recall against existing manual annotations. The results showed that the quality of the input text, as well as the selection and configuration of the ontologies impacted the results.
</div>
<div style="position:relative">						
	<div id="bib_5498fd6db5013012af0fcd9d0767bf4clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Identification of interdisciplinary ideas</b>. <br/>
<i>Information Processing &amp; Management</i>, 52(6):1074-1085, 2016.

<br/>
D. Thorleuchter and D. Van den Poel.
<br/>

<a onclick="toggleAbstract('lepsky', '096e2447cccddfd4c79f44ebfd48d858'); return false;" href="https://www.bibsonomy.org/bibtex/2096e2447cccddfd4c79f44ebfd48d858/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '096e2447cccddfd4c79f44ebfd48d858', 'https://www.bibsonomy.org/bibtex/2096e2447cccddfd4c79f44ebfd48d858/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2096e2447cccddfd4c79f44ebfd48d858/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_096e2447cccddfd4c79f44ebfd48d858lepsky" style="display:none;border:1px dotted grey;">
Literature shows interdisciplinary research as an essential driver for innovation. Ideas that are used as a starting point for this research are of an interdisciplinary nature because they combine aspects from different disciplines. The identification of interdisciplinary ideas at an early stage enables the start of interdisciplinary research and thus, it enables advances to be made in the innovation process. We propose a new methodology that combines semantic clustering and classification to estimate the interdisciplinary nature of ideas from a set of given ideas. The set is created automatically by use of an existing idea mining approach. Ideas from this set are semantically clustered to obtain concepts that are latent in the data. The relationship between each concept and each discipline pair from a set of given disciplines is calculated. Based on the degree of relationship, concepts are used to represent the interdisciplinary field spanned by the two disciplines. The ideas standing behind these concepts are identified as interdisciplinary ideas. As a result, the proposed methodology enables an estimation of the interdisciplinary nature of given ideas. The results might be helpful for researchers as well as for decision makers in the field of innovation management.
</div>
<div style="position:relative">						
	<div id="bib_096e2447cccddfd4c79f44ebfd48d858lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Evaluation and analysis of term scoring methods for term extraction</b>. <br/>
<i>Information Retrieval Journal</i>, 19(5):510-545, 2016.

<br/>
Suzan Verberne, Maya Sappelli, Djoerd Hiemstra and Wessel Kraaij.
<br/>

<a onclick="toggleAbstract('lepsky', 'f33177b644a3de9ca14dc4edec81ead2'); return false;" href="https://www.bibsonomy.org/bibtex/2f33177b644a3de9ca14dc4edec81ead2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f33177b644a3de9ca14dc4edec81ead2', 'https://www.bibsonomy.org/bibtex/2f33177b644a3de9ca14dc4edec81ead2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f33177b644a3de9ca14dc4edec81ead2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f33177b644a3de9ca14dc4edec81ead2lepsky" style="display:none;border:1px dotted grey;">
We evaluate five term scoring methods for automatic term extraction on four different types of text collections: personal document collections, news articles, scientific articles and medical discharge summaries. Each collection has its own use case: author profiling, boolean query term suggestion, personalized query suggestion and patient query expansion. The methods for term scoring that have been proposed in the literature were designed with a specific goal in mind. However, it is as yet unclear how these methods perform on collections with characteristics different than what they were designed for, and which method is the most suitable for a given (new) collection. In a series of experiments, we evaluate, compare and analyse the output of six term scoring methods for the collections at hand. We found that the most important factors in the success of a term scoring method are the size of the collection and the importance of multi-word terms in the domain. Larger collections lead to better terms; all methods are hindered by small collection sizes (below 1000 words). The most flexible method for the extraction of single-word and multi-word terms is pointwise Kullback-Leibler divergence for informativeness and phraseness. Overall, we have shown that extracting relevant terms using unsupervised term scoring methods is possible in diverse use cases, and that the methods are applicable in more contexts than their original design purpose.
</div>
<div style="position:relative">						
	<div id="bib_f33177b644a3de9ca14dc4edec81ead2lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A knowledge-based approach to information extraction for semantic interoperability in the archaeology domain</b>. <br/>
<i>Journal of the Association for Information Science and Technology</i>, 67(5):1138-1152, 2016.

<br/>
Andreas Vlachidis and Douglas Tudhope.
<br/>
<a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.23485/abstract">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '507715370462e7a09d1adc89314f8e96'); return false;" href="https://www.bibsonomy.org/bibtex/2507715370462e7a09d1adc89314f8e96/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '507715370462e7a09d1adc89314f8e96', 'https://www.bibsonomy.org/bibtex/2507715370462e7a09d1adc89314f8e96/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2507715370462e7a09d1adc89314f8e96/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_507715370462e7a09d1adc89314f8e96lepsky" style="display:none;border:1px dotted grey;">
The article presents a method for automatic semantic indexing of archaeological grey-literature reports using empirical (rule-based) Information Extraction techniques in combination with domain-specific knowledge organization systems. The semantic annotation system (OPTIMA) performs the tasks of Named Entity Recognition, Relation Extraction, Negation Detection, and Word-Sense Disambiguation using hand-crafted rules and terminological resources for associating contextual abstractions with classes of the standard ontology CIDOC Conceptual Reference Model (CRM) for cultural heritage and its archaeological extension, CRM-EH. Relation Extraction (RE) performance benefits from a syntactic-based definition of RE patterns derived from domain oriented corpus analysis. The evaluation also shows clear benefit in the use of assistive natural language processing (NLP) modules relating to Word-Sense Disambiguation, Negation Detection, and Noun Phrase Validation, together with controlled thesaurus expansion. The semantic indexing results demonstrate the capacity of rule-based Information Extraction techniques to deliver interoperable semantic abstractions (semantic annotations) with respect to the CIDOC CRM and archaeological thesauri. Major contributions include recognition of relevant entities using shallow parsing NLP techniques driven by a complimentary use of ontological and terminological domain resources and empirical derivation of context-driven RE rules for the recognition of semantic relationships from phrases of unstructured text.
</div>
<div style="position:relative">						
	<div id="bib_507715370462e7a09d1adc89314f8e96lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2015" class="bibsonomy_quicknav_group"><a name="2015">2015</a></h3>
<div style="margin-bottom:1em">
<b>Extracting concrete entities through spatial relations</b>. <br/>
In: A. Lieto, C. Battaglino, D. P. Radicioni and M. Sanguinetti, editors, <i>Artificial Intelligence and Cognition 2015 : Proceedings of the 3rd International Workshop on Artificial Intelligence and Cognition</i>.
Turin, 2015.

<br/>
Olga Acosta and César Aguilar.
<br/>

<a href="http://ceur-ws.org/Vol-1510/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '2e064ecf2b76c774cdef6d5f73521e4f'); return false;" href="https://www.bibsonomy.org/bibtex/22e064ecf2b76c774cdef6d5f73521e4f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '2e064ecf2b76c774cdef6d5f73521e4f', 'https://www.bibsonomy.org/bibtex/22e064ecf2b76c774cdef6d5f73521e4f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22e064ecf2b76c774cdef6d5f73521e4f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2e064ecf2b76c774cdef6d5f73521e4flepsky" style="display:none;border:1px dotted grey;">
This paper focuses on the automated extraction of concrete entities from a specialized-domain corpus. Then, in a bootstrapping phase, the candidates are used to extract new candidates. Concrete entities are automatically identified by a set of spatial features. In a spatial scene something is located by virtue of the spatial properties associated with a reference object. The axial properties are represented by place adverbs. Additionally, for identifying referent objects in a sentence we consider syntactical patterns extracted by chunking. In order to reduce noise in results, we take into account a corpus comparison approach and linguist heuristics. Results show high precision in candidates with high weights.
</div>
<div style="position:relative">						
	<div id="bib_2e064ecf2b76c774cdef6d5f73521e4flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Semantic enrichment of terminology</b>. <br/>
, 2015.

<br/>
Michael Esser.
<br/>
<a href="https://opus4.kobv.de/opus4-fhpotsdam/frontdoor/index/index/docId/1031">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '5f32e85924fe9c8ca074d36ba3f8c847'); return false;" href="https://www.bibsonomy.org/bibtex/25f32e85924fe9c8ca074d36ba3f8c847/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5f32e85924fe9c8ca074d36ba3f8c847', 'https://www.bibsonomy.org/bibtex/25f32e85924fe9c8ca074d36ba3f8c847/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25f32e85924fe9c8ca074d36ba3f8c847/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5f32e85924fe9c8ca074d36ba3f8c847lepsky" style="display:none;border:1px dotted grey;">
Die praktische Bedeutung hochwertiger Terminologien kann in einer auf Wissen- und Informationen beruhenden Gesellschaft nicht hoch genug bewertet werden. Gerade im Rahmen aktueller Bestrebungen, bisher isolierte Datensilos miteinander zu vernetzen, stellen eindeutige terminologische Daten ein Fundament der Entwicklung semantischer Lösungen dar. Die Öffnung des eigenen Vokabulars innerhalb des Semantic Webs stellt dabei eine große Chance dar bisher rein interne Datenbestände semantisch fundiert anreichern zu können. Dieser Schritt erfordert weniger eine komplizierte technische Infrastruktur, sondern vielmehr ein breites theoretisches Grundlagenwissen. Diese Grundlagen werden in der Arbeit anhand von ausführlichen Erläuterungen zu Thesaurusstrukturen, semantischen Technologien und zum SKOS Format vermittelt. Anschließend wird versucht dieses Wissen anhand eines ersten SKOS Entwurfes für den Bestand der DINTerminologie des Deutschen Instituts für Normung anzuwenden.
</div>
<div style="position:relative">						
	<div id="bib_5f32e85924fe9c8ca074d36ba3f8c847lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A method for extracting technical terms using the modified weirdness measure</b>. <br/>
<i>Automatic Documentation and Mathematical Linguistics</i>, 49(3):89-95, 2015.

<br/>
NA Kochetkova.
<br/>
<a href="http://dx.doi.org/10.3103/S0005105515030036">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '93e8e488110991c0e8ed1ccd27b9b9c4'); return false;" href="https://www.bibsonomy.org/bibtex/293e8e488110991c0e8ed1ccd27b9b9c4/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '93e8e488110991c0e8ed1ccd27b9b9c4', 'https://www.bibsonomy.org/bibtex/293e8e488110991c0e8ed1ccd27b9b9c4/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/293e8e488110991c0e8ed1ccd27b9b9c4/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_93e8e488110991c0e8ed1ccd27b9b9c4lepsky" style="display:none;border:1px dotted grey;">
A method for extracting terms from technical texts based on a new terminology measure is described. Morphological constraints are used to select term candidates. The results of experiments based on a corpus of texts in the area of computer-aided design systems and computer graphics are described.
</div>
<div style="position:relative">						
	<div id="bib_93e8e488110991c0e8ed1ccd27b9b9c4lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Terminology acquisition and description using lexical resources and local grammars</b>. <br/>
In: T. Poibeau and P. Faber, editors, <i>Terminology and Artificial Intelligence   Proceedings of the 11th International Conference on Terminology and Artificial Intelligence</i>, pages 81-89.
Granada, 2015.
bibtex: krstevterminology2015
<br/>
Cvetana Krstev, Ranka Stanković, Ivan Obradović and Biljana Lazić.
<br/>

<a href="http://ceur-ws.org/Vol-1495/paper_13.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'd41fe904eb077288570bbddf6f434591', 'https://www.bibsonomy.org/bibtex/2d41fe904eb077288570bbddf6f434591/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d41fe904eb077288570bbddf6f434591/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d41fe904eb077288570bbddf6f434591lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_d41fe904eb077288570bbddf6f434591lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Estimating term domain relevance through term frequency, disjoint corpora frequency - tf-dcf</b>. <br/>
<i>Knowledge-Based Systems</i>, 2015.

<br/>
Lucelene Lopes, Paulo Fernandes and Renata Vieira.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/S0950705115004979">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '5cac8d07b0962d8ee25789c9fe8b279d'); return false;" href="https://www.bibsonomy.org/bibtex/25cac8d07b0962d8ee25789c9fe8b279d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5cac8d07b0962d8ee25789c9fe8b279d', 'https://www.bibsonomy.org/bibtex/25cac8d07b0962d8ee25789c9fe8b279d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25cac8d07b0962d8ee25789c9fe8b279d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5cac8d07b0962d8ee25789c9fe8b279dlepsky" style="display:none;border:1px dotted grey;">
This paper proposes a new relevance index for terms extracted from domain corpora. We call it term frequency, disjoint corpora frequency (tf-dcf), and it is based on the absolute frequency of each term tempered by its frequency in other (contrasting) corpora. Conceptual differences and mathematical computation of the proposed index are discussed in respect with other similar approaches that also take contrasting corpora into account. To illustrate the efficiency of our index, this paper evaluates tf-dcf against other similar approaches. Finally, other experiments are made in order to analyze the tf-dcf behavior according to the characteristics of contrasting corpora.
</div>
<div style="position:relative">						
	<div id="bib_5cac8d07b0962d8ee25789c9fe8b279dlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Biomedical term extraction : overview and a new methodology</b>. <br/>
<i>Information Retrieval Journal</i>:1-41, 2015.

<br/>
Juan Antonio Lossio-Ventura, Clement Jonquet, Mathieu Roche and Maguelonne Teisseire.
<br/>
<a href="http://link.springer.com/article/10.1007/s10791-015-9262-2">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '99e2fb02c323e81ed8ef52e7a04570b3'); return false;" href="https://www.bibsonomy.org/bibtex/299e2fb02c323e81ed8ef52e7a04570b3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '99e2fb02c323e81ed8ef52e7a04570b3', 'https://www.bibsonomy.org/bibtex/299e2fb02c323e81ed8ef52e7a04570b3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/299e2fb02c323e81ed8ef52e7a04570b3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_99e2fb02c323e81ed8ef52e7a04570b3lepsky" style="display:none;border:1px dotted grey;">
Terminology extraction is an essential task in domain knowledge acquisition, as well as for information retrieval. It is also a mandatory first step aimed at building/enriching terminologies and ontologies. As often proposed in the literature, existing terminology extraction methods feature linguistic and statistical aspects and solve some problems related (but not completely) to term extraction, e.g. noise, silence, low frequency, large-corpora, complexity of the multi-word term extraction process. In contrast, we propose a cutting edge methodology to extract and to rank biomedical terms, covering all the mentioned problems. This methodology offers several measures based on linguistic, statistical, graphic and web aspects. These measures extract and rank candidate terms with excellent precision: we demonstrate that they outperform previously reported precision results for automatic term extraction, and work with different languages (English, French, and Spanish). We also demonstrate how the use of graphs and the web to assess the significance of a term candidate, enables us to outperform precision results. We evaluated our methodology on the biomedical GENIA and LabTestsOnline corpora and compared it with previously reported measures.
</div>
<div style="position:relative">						
	<div id="bib_99e2fb02c323e81ed8ef52e7a04570b3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Identifying ISI-indexed articles by their lexical usage : a text analysis approach</b>. <br/>
<i>Journal of the Association for Information Science &amp; Technology</i>, 66(3):501-511, 2015.

<br/>
Mohammadreza Moohebat, Ram Gopal Raj, Sameem Binti Abdul Kareem and Dirk Thorleuchter.
<br/>

<a onclick="toggleAbstract('lepsky', '6181aee33f9a85217c815243d094ab5d'); return false;" href="https://www.bibsonomy.org/bibtex/26181aee33f9a85217c815243d094ab5d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6181aee33f9a85217c815243d094ab5d', 'https://www.bibsonomy.org/bibtex/26181aee33f9a85217c815243d094ab5d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26181aee33f9a85217c815243d094ab5d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6181aee33f9a85217c815243d094ab5dlepsky" style="display:none;border:1px dotted grey;">
This research creates an architecture for investigating the existence of probable lexical divergences between articles, categorized as Institute for Scientific Information ( ISI) and non- ISI, and consequently, if such a difference is discovered, to propose the best available classification method. Based on a collection of ISI- and non- ISI-indexed articles in the areas of business and computer science, three classification models are trained. A sensitivity analysis is applied to demonstrate the impact of words in different syntactical forms on the classification decision. The results demonstrate that the lexical domains of ISI and non- ISI articles are distinguishable by machine learning techniques. Our findings indicate that the support vector machine identifies ISI-indexed articles in both disciplines with higher precision than do the Naïve Bayesian and K- Nearest Neighbors techniques.
</div>
<div style="position:relative">						
	<div id="bib_6181aee33f9a85217c815243d094ab5dlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>TBXTools : a free, fast and flexible tool for automatic terminology extraction</b>. <br/>
In: <i>Proceedings of Recent Advances in Natural Language Processing</i>, pages 473-479.
Conference: International Conference Recent Advances in Natural Language Processing (RANLP-2015), At Hissar (Bulgaria), Hissar, 2015.

<br/>
Antoni Oliver and Mercè Vázquez.
<br/>

<a href="http://www.researchgate.net/publication/281859088_TBXTools_A_Free_Fast_and_Flexible_Tool_for_Automatic_Terminology_Extraction">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '85ae1aa87e825d4097888c4e4e61ae12'); return false;" href="https://www.bibsonomy.org/bibtex/285ae1aa87e825d4097888c4e4e61ae12/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '85ae1aa87e825d4097888c4e4e61ae12', 'https://www.bibsonomy.org/bibtex/285ae1aa87e825d4097888c4e4e61ae12/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/285ae1aa87e825d4097888c4e4e61ae12/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_85ae1aa87e825d4097888c4e4e61ae12lepsky" style="display:none;border:1px dotted grey;">
The manual identification of terminology from specialized corpora is a complex task that needs to be addressed by flexible tools, in order to facilitate the construction of multilingual terminologies which are the main resources for computer-assisted translation tools, machine translation or ontologies. The automatic terminology extraction tools developed so far either use a proprietary code or an open source code, that is limited to certain software functionalities. To automatically extract terms from specialized corpora for different pur- poses such as constructing dictionaries, thesauruses or translation memories, we need open source tools to easily integrate new functionalities to improve term selection. This paper presents TBXTools, a free automatic terminology extraction tool that implements linguistic and statistical methods for multiword term extraction. The tool allows the users to easily iden- tify multiword terms from specialized cor- pora and also, if needed, translation candidates from parallel corpora. In this paper we present the main features of TBXTools along with evaluation results for term extraction, both using statistical and linguistic methodology, for several corpora.
</div>
<div style="position:relative">						
	<div id="bib_85ae1aa87e825d4097888c4e4e61ae12lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Semantic relations extraction from unstructured information for domain ontologies enrichment</b>.
<br/>
PhD thesis, Faculdade Ciências e Tecnologia , Universidade Nova de Lisboa, Lissabon, 2015.

<br/>
Luis Miguel Sintra Salvo Paiva.
<br/>
<a href="http://run.unl.pt/handle/10362/16550">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '92faf1df9ef60fc4214ce2802ece47de'); return false;" href="https://www.bibsonomy.org/bibtex/292faf1df9ef60fc4214ce2802ece47de/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '92faf1df9ef60fc4214ce2802ece47de', 'https://www.bibsonomy.org/bibtex/292faf1df9ef60fc4214ce2802ece47de/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/292faf1df9ef60fc4214ce2802ece47de/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_92faf1df9ef60fc4214ce2802ece47delepsky" style="display:none;border:1px dotted grey;">
Based in internet growth, through semantic web, together with communication speed improvement and fast development of storage device sizes, data and information volume rises considerably every day. Because of this, in the last few years there has been a growing interest in structures for formal representation with suitable characteristics, such as the possibility to organize data and information, as well as the reuse of its contents aimed for the generation of new knowledge. Controlled Vocabulary, specifically Ontologies, present themselves in the lead as one of such structures of representation with high potential. Not only allow for data representation, as well as the reuse of such data for knowledge extraction, coupled with its subsequent storage through not so complex formalisms. However, for the purpose of assuring that ontology knowledge is always up to date, they need maintenance. Ontology Learning is an area which studies the details of update and maintenance of ontologies. It is worth noting that relevant literature already presents first results on automatic maintenance of ontologies, but still in a very early stage. Human-based processes are still the current way to update and maintain an ontology, which turns this into a cumbersome task. The generation of new knowledge aimed for ontology growth can be done based in Data Mining techniques, which is an area that studies techniques for data processing, pattern discovery and knowledge extraction in IT systems. This work aims at proposing a novel semi-automatic method for knowledge extraction from unstructured data sources, using Data Mining techniques, namely through pattern discovery, focused in improving the precision of concept and its semantic relations present in an ontology. In order to verify the applicability of the proposed method, a proof of concept was developed, presenting its results, which were applied in building and construction sector.
</div>
<div style="position:relative">						
	<div id="bib_92faf1df9ef60fc4214ce2802ece47delepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>DEXTER : automatic extraction of domain-specific glossaries for language teaching</b>. <br/>
<i>Procedia - Social and Behavioral Sciences</i>, 198:377-385, 2015.

<br/>
Carlos Periñán-Pascual and Eva Mestre-Mestre.
<br/>
<a href="http://dx.doi.org/10.1016/j.sbspro.2015.07.457">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '11f1485063792505c0a6a05915060bb0'); return false;" href="https://www.bibsonomy.org/bibtex/211f1485063792505c0a6a05915060bb0/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '11f1485063792505c0a6a05915060bb0', 'https://www.bibsonomy.org/bibtex/211f1485063792505c0a6a05915060bb0/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/211f1485063792505c0a6a05915060bb0/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_11f1485063792505c0a6a05915060bb0lepsky" style="display:none;border:1px dotted grey;">
Many researchers emphasize the importance of corpora in the design of Language-for-Specific-Purposes courses in higher education. However, identifying those lexical units which belong to a given specific domain is often a complex task for language teachers, where simple introspection or concordance analysis does not really become effective. The goal of this paper is to describe DEXTER, an open-access platform for data mining and terminology management, whose aim is not only the search, retrieval, exploration and analysis of texts in domain-specific corpora but also the automatic extraction of specialized words from the domain.
</div>
<div style="position:relative">						
	<div id="bib_11f1485063792505c0a6a05915060bb0lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Extraction of candidate terms from a corpus of non-specialized, general language</b>. <br/>
<i>Investigación Bibliotecológica</i>, 29:19-45, 2015.

<br/>
Gilberto Anguiano Peña and Catalina Naumis Peña.
<br/>

<a onclick="toggleAbstract('lepsky', '121a79b2a49ca31696144fd78269dc3f'); return false;" href="https://www.bibsonomy.org/bibtex/2121a79b2a49ca31696144fd78269dc3f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '121a79b2a49ca31696144fd78269dc3f', 'https://www.bibsonomy.org/bibtex/2121a79b2a49ca31696144fd78269dc3f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2121a79b2a49ca31696144fd78269dc3f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_121a79b2a49ca31696144fd78269dc3flepsky" style="display:none;border:1px dotted grey;">
Linguistic phenomena associated with the analysis of document content and employed for the purpose of organization and retrieval are well-visited objects of study in the field of library and information science. Language often acts as a gatekeeper, admitting or excluding people from gaining access to knowledge. As such, the terms used in the scientific and technical language of research need to be kept up and their behavior within the domain examined. Documental content analysis of scientific texts provides knowledge of specialized lexicons and their specific applications, while differentiating them from common use in order to establish indexing languages. Thus, as proposed herein, the application of lexicographic techniques to documental content analysis of non-specialized language yields the components needed to describe and extract lexical units of the specialized language. (English)
</div>
<div style="position:relative">						
	<div id="bib_121a79b2a49ca31696144fd78269dc3flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Content analysis and application of Zipf's Law in computer science literature</b>. <br/>
In: , pages 223-227.
2015.

<br/>
Rajneesh and MS Rana.
<br/>

<a href="http://dx.doi.org/10.1109/ETTLIS.2015.7048202">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '155da6687e712975d9d6337447293046'); return false;" href="https://www.bibsonomy.org/bibtex/2155da6687e712975d9d6337447293046/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '155da6687e712975d9d6337447293046', 'https://www.bibsonomy.org/bibtex/2155da6687e712975d9d6337447293046/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2155da6687e712975d9d6337447293046/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_155da6687e712975d9d6337447293046lepsky" style="display:none;border:1px dotted grey;">
The technique of content analysis is used in construction of thesaurus, subject headings and designing classification schemes for efficient organisation of knowledge in the libraries. Moreover, content analysis is also useful in analyzing the user queries and in designing search formulation. The purpose of paper is to analyze, evaluate and apply Zipf's Law in Computer Science through the content analysis of literature published in ACM journals. The study is focused on the analysis of 13, 053 unique keywords out of 107,467 total keywords retrieved from 1954 to 2008 from Journals. Further, a total of 748 keywords have been chosen for this study which occurred textgreater 20 times. The distribution of keywords shows that the studies have been conducted in a large number of areas of study in computer science. Further, computer science research has been covering all the aspects very well as occurrences of almost all the keywords have increasing trend. However, Zipf's Law only approximates in patches, not in the entire data set.
</div>
<div style="position:relative">						
	<div id="bib_155da6687e712975d9d6337447293046lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Multiword expressions acquisition : a generic and open framework</b>.<br/>
2015. 
<br/>Carlos Ramisch.
<br/>
<a href="http://www.springer.com/new+%26+forthcoming+titles+%28default%29/book/978-3-319-09206-5">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd4ae7b74950ce6be7f515eff7a601747'); return false;" href="https://www.bibsonomy.org/bibtex/2d4ae7b74950ce6be7f515eff7a601747/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd4ae7b74950ce6be7f515eff7a601747', 'https://www.bibsonomy.org/bibtex/2d4ae7b74950ce6be7f515eff7a601747/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d4ae7b74950ce6be7f515eff7a601747/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d4ae7b74950ce6be7f515eff7a601747lepsky" style="display:none;border:1px dotted grey;">
This book is an excellent introduction to multiword expressions. It provides a unique, comprehensive and up-to-date overview of this exciting topic in computational linguistics. The first part describes the diversity and richness of multiword expressions, including many examples in several languages. These constructions are not only complex and arbitrary, but also much more frequent than one would guess, making them a real nightmare for natural language processing applications. The second part introduces a new generic framework for automatic acquisition of multiword expressions from texts. Furthermore, it describes the accompanying free software tool, the mwetoolkit, which comes in handy when looking for expressions in texts (regardless of the language). Evaluation is greatly emphasized, underlining the fact that results depend on parameters like corpus size, language, MWE type, etc. The last part contains solid experimental results and evaluates the mwetoolkit, demonstrating its usefulness for computer-assisted lexicography and machine translation. This is the first book to cover the whole pipeline of multiword expression acquisition in a single volume. It is addresses the needs of students and researchers in computational and theoretical linguistics, cognitive sciences, artificial intelligence and computer science. Its good balance between computational and linguistic views make it the perfect starting point for anyone interested in multiword expressions, language and text processing in general.
</div>
<div style="position:relative">						
	<div id="bib_d4ae7b74950ce6be7f515eff7a601747lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Extracting terms and their relations from German texts : NLP tools for the preparation of raw material for specialized e-dictionaries</b>.<br/>
In: 

<i>Electronic lexicography in the 21st century: linking lexical data in the digital age. Proceedings of the eLex 2015 conference, 11-13 August 2015, Herstmonceux Castle, United Kingdom</i>, pages 486-503.
Trojina, Institute for Applied Slovene Studies ; Lexical Computing Ltd., Ljubljana ; Brighton, 2015.

<br/>
Ina Rösiger, Johannes Schäfer, Tanja George, Simon Tannert, Ulrich Heid and Michael Dorna.
<br/>

<a href="https://elex.link/elex2015/proceedings/eLex_2015_33_Rosiger+etal.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'cb78fc65bb8cf2d89386d3b620ee36d5'); return false;" href="https://www.bibsonomy.org/bibtex/2cb78fc65bb8cf2d89386d3b620ee36d5/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cb78fc65bb8cf2d89386d3b620ee36d5', 'https://www.bibsonomy.org/bibtex/2cb78fc65bb8cf2d89386d3b620ee36d5/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cb78fc65bb8cf2d89386d3b620ee36d5/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cb78fc65bb8cf2d89386d3b620ee36d5lepsky" style="display:none;border:1px dotted grey;">
We report on ongoing experiments in data extraction from German texts in the domain of do-it-yourself (DIY) instructions, where the objective is (i) to extract nominal term can- didates with high quality; (ii) to extract predicate-argument structures involving the term candidates, and (iii) to relate German word formation products with syntactic paraphrases: we focus on the analysis of compounds and on relating them with their syntactic paraphrases, in order to provide evidence for the (semantic) relationship between compound heads and non-heads (Holzbohrer (wood drill) ↔ HolzObject bohren ([to] drill wood)). The extracted material is collected in order to provide structured data input for the creation of special- ized dictionaries that are richer than standard terminological glossaries. For the creation of taxonomic knowledge (Bandsäge -is-a → Säge (bandsaw → saw)), we analyze subtypes of compounds.
</div>
<div style="position:relative">						
	<div id="bib_cb78fc65bb8cf2d89386d3b620ee36d5lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2014" class="bibsonomy_quicknav_group"><a name="2014">2014</a></h3>
<div style="margin-bottom:1em">
<b>Visual saliency and terminology extraction for document classification</b>.<br/>
In: 
B. Lamiroy and J.-M. Ogier, editors, 
<i>Graphics Recognition. Current Trends and Challenges</i>, pages 96-108.
Springer, Berlin ; Heidelberg, 2014.

<br/>
Duthil Benjamin, Coustaty Mickael, Courboulay Vincent and Jean-Marc Ogier.
<br/>

<a href="http://dx.doi.org/10.1007/978-3-662-44854-0_8">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e301dbe7304e89f2ab3213db65203f13'); return false;" href="https://www.bibsonomy.org/bibtex/2e301dbe7304e89f2ab3213db65203f13/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e301dbe7304e89f2ab3213db65203f13', 'https://www.bibsonomy.org/bibtex/2e301dbe7304e89f2ab3213db65203f13/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e301dbe7304e89f2ab3213db65203f13/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e301dbe7304e89f2ab3213db65203f13lepsky" style="display:none;border:1px dotted grey;">
The document digitization process becomes a crucial economical issue in our society. Then, it becomes necessary to be able to organize this huge amount of documents. The work proposed in this paper tends to propose a new method to automatically classify documents using a saliency-based segmentation process on one hand, and a terminology extraction and annotation on the other hand. The saliency-based segmentation is used to extract salient regions and by the way logo, while the terminology approach is used to annotate them and to automatically classify the document. The approach does not require human expertise, and use Google Images as a knowledge database. The results obtained on a real database of 1766 documents show the relevance of the approach.
</div>
<div style="position:relative">						
	<div id="bib_e301dbe7304e89f2ab3213db65203f13lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatische Extraktion von Fachterminologie aus Volltexten</b>. <br/>
<i>ABI Technik</i>, 34(1):2-8, 2014.

<br/>
Juliane Bredack and Klaus Lepsky.
<br/>
<a href="http://www.degruyter.com/view/j/abitech.2014.34.issue-1/abitech-2014-0002/abitech-2014-0002.xml?format=INT">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7e0c577a386eff495dc7fdeeccfda3aa'); return false;" href="https://www.bibsonomy.org/bibtex/27e0c577a386eff495dc7fdeeccfda3aa/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7e0c577a386eff495dc7fdeeccfda3aa', 'https://www.bibsonomy.org/bibtex/27e0c577a386eff495dc7fdeeccfda3aa/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27e0c577a386eff495dc7fdeeccfda3aa/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7e0c577a386eff495dc7fdeeccfda3aalepsky" style="display:none;border:1px dotted grey;">
Fachterminologie in wissenschaftlichen Texten liegt häufig in Form von Phrasen oder Mehrwortgruppen vor. Vorgestellt wird ein algorithmisches Verfahren zur Identifikation und Extraktion fachtermi-nologischer Mehrwortgruppen. Besonderer Schwerpunkt ist die Einbindung von Funktionswörtern der deutschen Sprache, um die Extraktion komplexer Mehrwortkonstruktionen zu ermöglichen. Eingesetzt wurde das automatische Indexierungssystem Lingo. Die Ergebnisse für eine Extraktion kunsthistorischer Fachterminologie aus dem Reallexikon zur Deutschen Kunstgeschichte belegen die Tauglichkeit des Verfahrens.
</div>
<div style="position:relative">						
	<div id="bib_7e0c577a386eff495dc7fdeeccfda3aalepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>The main challenge of semi-automatic term extraction methods</b>. <br/>
In: .
2014.

<br/>
Merley Conrado, Thiago Pardo and Solange Rezende.
<br/>

<a href="http://www.icmc.usp.br/pessoas/taspardo/NLPCS2014-ConradoEtAl.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e28d185c47badf75f8d3ae60fccfc3f3'); return false;" href="https://www.bibsonomy.org/bibtex/2e28d185c47badf75f8d3ae60fccfc3f3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e28d185c47badf75f8d3ae60fccfc3f3', 'https://www.bibsonomy.org/bibtex/2e28d185c47badf75f8d3ae60fccfc3f3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e28d185c47badf75f8d3ae60fccfc3f3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e28d185c47badf75f8d3ae60fccfc3f3lepsky" style="display:none;border:1px dotted grey;">
Term extraction is the basis for many tasks such as building of taxonomies, ontologies and dictionaries, for translation, organization and retrieval of textual data. This paper studies the main challenge of semi-automatic term extraction methods, which is the difficulty to analyze the rank of candidates created by these methods. With the experimental evaluation performed in this work, it is possible to fairly compare a wide set of semi-automatic term extraction methods, which allows other future investigations. Additionally, we discovered which level of knowledge and threshold should be adopted for these methods in order to obtain good precision or F-measure. The results show there is not a unique method that is the best one for the three used corpora.
</div>
<div style="position:relative">						
	<div id="bib_e28d185c47badf75f8d3ae60fccfc3f3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Tools for terminology processing</b>. <br/>
<i>CoRR</i>, abs/1412.4401, 2014.

<br/>
Chantal Enguehard, Béatrice Daille and Emmanuel Morin.
<br/>
<a href="http://arxiv.org/pdf/1412.4401.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3a04d95031bd0bd109a5d1a0ad9dddfe'); return false;" href="https://www.bibsonomy.org/bibtex/23a04d95031bd0bd109a5d1a0ad9dddfe/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3a04d95031bd0bd109a5d1a0ad9dddfe', 'https://www.bibsonomy.org/bibtex/23a04d95031bd0bd109a5d1a0ad9dddfe/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23a04d95031bd0bd109a5d1a0ad9dddfe/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3a04d95031bd0bd109a5d1a0ad9dddfelepsky" style="display:none;border:1px dotted grey;">
Automatic terminology processing appeared 10 years ago when electronic corpora became widely available. Such processing may be statistically or linguistically based and produces terminology resources that can be used in a number of applications : indexing, information retrieval, technology watch, etc. We present the tools that have been developed in the IRIN Institute. They all take as input texts (or collection of texts) and reflect different states of terminology processing: term acquisition, term recognition and term structuring.
</div>
<div style="position:relative">						
	<div id="bib_3a04d95031bd0bd109a5d1a0ad9dddfelepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An iterative approach to the terminology extraction from Ukrainian-language scientific text corpora</b>. <br/>
<i>Cybernetics and Systems Analysis</i>, 50(6):866-873+, 2014.

<br/>
AM Glybovets and IV Reshetnov.
<br/>
<a href="http://dx.doi.org/10.1007/s10559-014-9677-6">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ae265e00d33be4f076d91077122089df'); return false;" href="https://www.bibsonomy.org/bibtex/2ae265e00d33be4f076d91077122089df/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ae265e00d33be4f076d91077122089df', 'https://www.bibsonomy.org/bibtex/2ae265e00d33be4f076d91077122089df/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ae265e00d33be4f076d91077122089df/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ae265e00d33be4f076d91077122089dflepsky" style="display:none;border:1px dotted grey;">
This article describes a combined method for the acquisition of valuable terms and relations from raw texts with the help of an iterative algorithm for automated terminology extraction from Ukrainian-language scientific texts. Special attention is paid to the analysis of lexicographical features of characteristic text fragments of documents. Specific features of Ukrainian-language documents are taken into account. The emphasis is on solving the applied problem of terminology acquisition from input texts in the widely used pdf format with obtaining output term relations in the RDF format.
</div>
<div style="position:relative">						
	<div id="bib_ae265e00d33be4f076d91077122089dflepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Accurate keyphrase extraction by discriminating overlapping phrases</b>. <br/>
<i>Journal of Information Science</i>, 2014.

<br/>
Mounia Haddoud and Said Abdeddaim.
<br/>
<a href="http://dx.doi.org/10.1177/0165551514530210">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ee7dff586c09bb33673a60e63c3e5b66'); return false;" href="https://www.bibsonomy.org/bibtex/2ee7dff586c09bb33673a60e63c3e5b66/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ee7dff586c09bb33673a60e63c3e5b66', 'https://www.bibsonomy.org/bibtex/2ee7dff586c09bb33673a60e63c3e5b66/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ee7dff586c09bb33673a60e63c3e5b66/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ee7dff586c09bb33673a60e63c3e5b66lepsky" style="display:none;border:1px dotted grey;">
In this paper we define the document phrase maximality index (DPM-index), a new measure to discriminate overlapping keyphrase candidates in a text document. As an application we developed a supervised learning system that uses 18 statistical features, among them the DPM-index and five other new features. We experimentally compared our results with those of 21 keyphrase extraction methods on SemEval-2010/Task-5 scientific articles corpus. When all the systems extract 10 keyphrases per document, our method enhances by 13%the F-score of the best system. In particular, the DPM-index feature increases the F-score of our keyphrase extraction system by a rate of 9 This makes the DPM-index contribution comparable to that of the well-known TFIDF measure on such a system.
</div>
<div style="position:relative">						
	<div id="bib_ee7dff586c09bb33673a60e63c3e5b66lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Yet another ranking function for automatic multiword term extraction</b>. <br/>
, 2014.

<br/>
Juan Lossio-Ventura, Clement Jonquet, Mathieu Roche and Maguelonne Teisseire.
<br/>
<a href="http://www.lirmm.fr/~jonquet/publications/documents/Article_PolTAL2014_Lossio.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '026799f12b8d5fe9ab2638afd5239ef8'); return false;" href="https://www.bibsonomy.org/bibtex/2026799f12b8d5fe9ab2638afd5239ef8/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '026799f12b8d5fe9ab2638afd5239ef8', 'https://www.bibsonomy.org/bibtex/2026799f12b8d5fe9ab2638afd5239ef8/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2026799f12b8d5fe9ab2638afd5239ef8/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_026799f12b8d5fe9ab2638afd5239ef8lepsky" style="display:none;border:1px dotted grey;">
Term extraction is an essential task in domain knowledge acquisition. We propose two new measures to extract multiword terms from a domain-specific text. The first measure is both linguistic and sta- tistical based. The second measure is graph-based, allowing assessment of the importance of a multiword term of a domain. Existing measures often solve some problems related (but not completely) to term extrac- tion, e.g., noise, silence, low frequency, large-corpora, complexity of the multiword term extraction process. Instead, we focus on managing the entire set of problems, e.g., detecting rare terms and overcoming the low frequency issue. We show that the two proposed measures outperform precision results previously reported for automatic multiword extraction by comparing them with the state-of-the-art reference measures.
</div>
<div style="position:relative">						
	<div id="bib_026799f12b8d5fe9ab2638afd5239ef8lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Noun phrases in automatic indexing : a structural analysis of the distribution of relevant terms in doctoral theses</b>. <br/>
In: , pages 327-334.
Ergon, 2014.

<br/>
Luiz Mesquita, Renato Souza and Renata Baracho Porto.
<br/>


<a onclick="toggleAbstract('lepsky', '605b75f1d6d9156e1a778cb961245dd3'); return false;" href="https://www.bibsonomy.org/bibtex/2605b75f1d6d9156e1a778cb961245dd3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '605b75f1d6d9156e1a778cb961245dd3', 'https://www.bibsonomy.org/bibtex/2605b75f1d6d9156e1a778cb961245dd3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2605b75f1d6d9156e1a778cb961245dd3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_605b75f1d6d9156e1a778cb961245dd3lepsky" style="display:none;border:1px dotted grey;">
The main objective of this research was to analyze whether there was a characteristic distribution behavior of relevant terms over a scientific text that could contribute as a criterion for their process of automatic indexing. The terms considered in this study were only full noun phrases contained in the texts themselves. The texts were considered a total of 98 doctoral theses of the eight areas of knowledge in a same university. Initially, 20 full noun phrases were automatically extracted from each text as candidates to be the most relevant terms, and each author of each text assigned a relevance value 0-6 (not relevant and highly relevant, respectively) for each of the 20 noun phrases sent. Only, 22.1 %of noun phrases were considered not relevant. A relevance values of the terms assigned by the authors were associated with their positions in the text. Each full noun phrases found in the text was considered as a valid linear position. The results that were obtained showed values resulting from this distribution by considering two types of position: linear, with values consolidated into ten equal consecutive parts; and structural, considering parts of the text (such as introduction, development and conclusion). As a result of considerable importance, all areas of knowledge related to the Natural Sciences showed a characteristic behavior in the distribution of relevant terms, as well as all areas of knowledge related to Social Sciences showed the same characteristic behavior of distribution, but distinct from the Natural Sciences. The difference of the distribution behavior between the Natural and Social Sciences can be clearly visualized through graphs. All behaviors, including the general behavior of all areas of knowledge together, were characterized in polynomial equations and can be applied in future as criteria for automatic indexing. Until the present date this work has become inedited of for two reasons: to present a method for characterizing the distribution of relevant terms in a scientific text, and also, through this method, pointing out a quantitative trait difference between the Natural and Social Sciences.
</div>
<div style="position:relative">						
	<div id="bib_605b75f1d6d9156e1a778cb961245dd3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>POS tagging and its applications for mathematics</b>. <br/>
<i>arXiv:1406.2880 [cs]</i>, 2014.

<br/>
Ulf Schöneberg and Wolfram Sperber.
<br/>
<a href="http://arxiv.org/abs/1406.2880">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1ba32f69243bad15fd6c586c539431d8'); return false;" href="https://www.bibsonomy.org/bibtex/21ba32f69243bad15fd6c586c539431d8/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1ba32f69243bad15fd6c586c539431d8', 'https://www.bibsonomy.org/bibtex/21ba32f69243bad15fd6c586c539431d8/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21ba32f69243bad15fd6c586c539431d8/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1ba32f69243bad15fd6c586c539431d8lepsky" style="display:none;border:1px dotted grey;">
Content analysis of scientific publications is a nontrivial task, but a useful and important one for scientific information services. In the Gutenberg era it was a domain of human experts; in the digital age many machine-based methods, e.g., graph analysis tools and machine-learning techniques, have been developed for it. Natural Language Processing (NLP) is a powerful machine-learning approach to semiautomatic speech and language processing, which is also applicable to mathematics. The well established methods of NLP have to be adjusted for the special needs of mathematics, in particular for handling mathematical formulae. We demonstrate a mathematics-aware part of speech tagger and give a short overview about our adaptation of NLP methods for mathematical publications. We show the use of the tools developed for key phrase extraction and classification in the database zbMATH.
</div>
<div style="position:relative">						
	<div id="bib_1ba32f69243bad15fd6c586c539431d8lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Understanding trends in the patent domain : user perceptions on trends and trend related concepts</b>. <br/>
In: C. Womser-Hacker, T. Mandl, H. Jung and S. Xu, editors, <i>Proceedings of the First International Workshop on Patent Mining and Its Applications (IPaMin 2014) co-located with Konvens 2014</i>, volume 1292.
Hildesheim, 2014.

<br/>
Julia M. Struß, Thomas Mandl, Michael Schwantner and Christa Womser-Hacker.
<br/>

<a href="http://ceur-ws.org/Vol-1292/ipamin2014_paper9.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ae2978d6abdb3f7ccb77bc516695b546'); return false;" href="https://www.bibsonomy.org/bibtex/2ae2978d6abdb3f7ccb77bc516695b546/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ae2978d6abdb3f7ccb77bc516695b546', 'https://www.bibsonomy.org/bibtex/2ae2978d6abdb3f7ccb77bc516695b546/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ae2978d6abdb3f7ccb77bc516695b546/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ae2978d6abdb3f7ccb77bc516695b546lepsky" style="display:none;border:1px dotted grey;">
The proceeding globalization in combination with an in- creasing competition in research conducted at universities and other research institutes as well as in industry, empha- sises the necessity of identifying trends at an early stage, not only in industry but by universities and governments. One of the resources to be considered are patents, as most of the information contained therein is not published any- where else. The existing research focuses on the technical perspective of identifying trends in patents. This work ad- dresses the user perspective of the problem, in particular the user’s working environments, understanding of trends, the underlying tasks and the user requirements regarding a trend mining system are examined.
</div>
<div style="position:relative">						
	<div id="bib_ae2978d6abdb3f7ccb77bc516695b546lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Evaluation of technology term recognition with random indexing</b>. <br/>
, 2014.

<br/>
Behrang Zadeh and Siegfried Handschuh.
<br/>
<a href="http://atmykitchen.info/sites/default/files/publications/lrec_2014_automatic_term_classification.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7d126aea9e47782d3b19800ad8c88802'); return false;" href="https://www.bibsonomy.org/bibtex/27d126aea9e47782d3b19800ad8c88802/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7d126aea9e47782d3b19800ad8c88802', 'https://www.bibsonomy.org/bibtex/27d126aea9e47782d3b19800ad8c88802/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27d126aea9e47782d3b19800ad8c88802/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7d126aea9e47782d3b19800ad8c88802lepsky" style="display:none;border:1px dotted grey;">
In this paper, we propose a method that combines the principles of automatic term recognition and the distributional hypothesis to identify technology terms from a corpus of scientific publications. We employ the random indexing technique to model terms' surrounding words, which we call the context window, in a vector space at reduced dimension. The constructed vector space and a set of reference vectors, which represents manually annotated technology terms, in a k -nearest-neighbour voting classification scheme are used for term classification. In this paper, we examine a number of parameters that influence the obtained results. First, we inspect several context configurations, i.e. the effect of the context window size, the direction in which co-occurrence counts are collected, and information about the order of words within the context windows. Second, in the k -nearest-neighbour voting scheme, we study the role that neighbourhood size selection plays, i.e. the value of k . The obtained results are similar to word space models. The performed experiments suggest the best performing context are small (i.e. not wider than 3 words), are extended in both directions and encode the word order information. Moreover, the accomplished experiments suggest that the obtained results, to a great extent, are independent of the value of k .
</div>
<div style="position:relative">						
	<div id="bib_7d126aea9e47782d3b19800ad8c88802lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Investigating context parameters in technology term recognition</b>. <br/>
In: , pages 1-10.
2014.

<br/>
Behrang Zadeh and Siegfried Handschuh.
<br/>

<a href="http://www.aclweb.org/anthology/W/W14/W14-60.pdf#page=11">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '17fa5f66804a27dec4febf87b5136d6e'); return false;" href="https://www.bibsonomy.org/bibtex/217fa5f66804a27dec4febf87b5136d6e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '17fa5f66804a27dec4febf87b5136d6e', 'https://www.bibsonomy.org/bibtex/217fa5f66804a27dec4febf87b5136d6e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/217fa5f66804a27dec4febf87b5136d6e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_17fa5f66804a27dec4febf87b5136d6elepsky" style="display:none;border:1px dotted grey;">
We propose and evaluate the task of technology term recognition: a method to extract technology terms at a synchronic level from a corpus of scientific publications. The proposed method is built on the principles of terminology extraction and distributional semantics. It is realized as a regression task in a vector space model. In this method, candidate terms are first extracted from text. Subsequently, using the random indexing technique, the extracted candidate terms are represented as vectors in a Euclidean vector space of reduced dimensionality. These vectors are derived from the frequency of co-occurrences of candidate terms and words in windows of text surrounding candidate terms in the input corpus (context window). The constructed vector space and a set of manually tagged technology terms (reference vectors) in a k-nearest neighbours regression framework is then used to identify terms that signify technology concepts. We examine a number of factors that play roles in the performance of the proposed method, i.e. the configuration of context windows, neighborhood size (k) selection, and reference vector size.
</div>
<div style="position:relative">						
	<div id="bib_17fa5f66804a27dec4febf87b5136d6elepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2013" class="bibsonomy_quicknav_group"><a name="2013">2013</a></h3>
<div style="margin-bottom:1em"><b>Terminologieextraktion von Mehrwortgruppen in kunsthistorischen Fachtexten</b>.
<br/>
PhD thesis, Fachhochschule Köln; Fakultät für Informations- und Kommunikationswissenschaften, Köln, 2013.

<br/>
Juliane Bredack.
<br/>
<a href="http://ixtrieve.fh-koeln.de/lehre/bredack-2013.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3cd63468cb914d78411e7e44a8f1d6d2'); return false;" href="https://www.bibsonomy.org/bibtex/23cd63468cb914d78411e7e44a8f1d6d2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3cd63468cb914d78411e7e44a8f1d6d2', 'https://www.bibsonomy.org/bibtex/23cd63468cb914d78411e7e44a8f1d6d2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23cd63468cb914d78411e7e44a8f1d6d2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3cd63468cb914d78411e7e44a8f1d6d2lepsky" style="display:none;border:1px dotted grey;">
Mit Hilfe eines algorithmisch arbeitenden Verfahrens können Mehrwortgruppen aus elektronisch vorliegenden Texten identifiziert und extrahiert werden. Als Datengrundlage für diese Arbeit dienen kunsthistorische Lexikonartikel des Reallexikons zur Deutschen Kunstgeschichte. Die linguistisch, wörterbuchbasierte Open-Source-Software Lingo wurde in dieser Studie genutzt. Mit Lingo ist es möglich, auf Basis erstellter Wortmuster, bestimmte Wortfolgen aus elektronisch vorliegenden Daten algorithmisch zu identifizieren und zu extrahieren. Die erstellten Wortmuster basieren auf Wortklassen, mit denen die lexikalisierten Einträge in den Wörterbüchern getaggt sind und dadurch näher definiert werden. So wurden individuelle Wortklassen für Fachterminologie, Eigennamen, oder Adjektive vergeben. In der vorliegenden Arbeit werden zusätzlich Funktionswörter in die Musterbildung mit einbezogen. Dafür wurden neue Wortklassen definiert. Funktionswörter bestimmen Artikel, Konjunktionen und Präpositionen. Ziel war es fachterminologische Mehrwortgruppen mit kunsthistorischen Inhalten zu extrahieren unter der gezielten Einbindung von Funktionswörtern. Anhand selbst gebildeter Kriterien, wurden die extrahierten Mehrwortgruppen qualitativ analysiert. Es konnte festgestellt werden, dass die Verwendung von Funktionswörtern fachterminologische Mehrwortgruppen erzeugt, die als potentielle Indexterme weitere Verwendung im Information Retrieval finden können.
</div>
<div style="position:relative">						
	<div id="bib_3cd63468cb914d78411e7e44a8f1d6d2lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Extracting and displaying temporal and geospatial entities from articles on historical events</b>. <br/>
<i>The Computer Journal</i>:bxt112+, 2013.

<br/>
Rachel Chasin, Daryl Woodward, Jeremy Witmer and Jugal Kalita.
<br/>
<a href="http://dx.doi.org/10.1093/comjnl/bxt112">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '073cb208b614108ff6bcfb5f6ff71388'); return false;" href="https://www.bibsonomy.org/bibtex/2073cb208b614108ff6bcfb5f6ff71388/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '073cb208b614108ff6bcfb5f6ff71388', 'https://www.bibsonomy.org/bibtex/2073cb208b614108ff6bcfb5f6ff71388/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2073cb208b614108ff6bcfb5f6ff71388/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_073cb208b614108ff6bcfb5f6ff71388lepsky" style="display:none;border:1px dotted grey;">
This paper discusses a system that extracts and displays temporal and geospatial entities in text. The first task involves identification of all events in a document followed by identification of important events using a classifier. The second task involves identifying named entities associated with the document. In particular, we extract geospatial named entities. We disambiguate the set of geospatial named entities and geocode them to determine the correct coordinates for each place name, often called grounding. We resolve ambiguity based on sentence and article context. Finally, we present a user with the key events and their associated people, places and organizations within a document in terms of a timeline and a map. For purposes of testing, we use Wikipedia articles about historical events, such as those describing wars, battles and invasions. We focus on extracting major events from the articles, although our ideas and tools can be easily used with articles from other sources such as news articles. We use several existing tools such as Evita, Google Maps, publicly available implementations of Support Vector Machines, Hidden Markov Model and Conditional Random Field, and the MIT SIMILE Timeline.
</div>
<div style="position:relative">						
	<div id="bib_073cb208b614108ff6bcfb5f6ff71388lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Exploring the inference role in automatic information extraction from texts</b>. <br/>
In: , pages 33-40.
2013.

<br/>
Denis de Araujo, Sandro Rigo, Carolina Muller and Rove Chishman.
<br/>

<a href="http://www.aclweb.org/anthology/W/W13/W13-52.pdf#page=45">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '89639e903f47b886afaa816b36fa0f0b'); return false;" href="https://www.bibsonomy.org/bibtex/289639e903f47b886afaa816b36fa0f0b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '89639e903f47b886afaa816b36fa0f0b', 'https://www.bibsonomy.org/bibtex/289639e903f47b886afaa816b36fa0f0b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/289639e903f47b886afaa816b36fa0f0b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_89639e903f47b886afaa816b36fa0f0blepsky" style="display:none;border:1px dotted grey;">
In this paper we present a novel methodology for automatic information extraction from natural language texts, based on the integration of linguistic rules, multiple ontologies and inference resources, integrated with an abstraction layer for linguistic annotation and data representation. The SAURON system was developed to implement and integrate the methodology phases. The knowledge domain of legal realm has been used for the case study scenario through a corpus collected from the State Superior Court website in Brazil. The main contribution presented is related to the exploration of the flexibility of linguistic rules and domain knowledge representation, through their manipulation and integration by a reasoning system. Therefore, it is possible to the system to continuously interact with linguistic and domain experts in order to improve the set of linguistic rules or the ontology components. The results from the case study indicate that the proposed approach is effective for the legal domain.
</div>
<div style="position:relative">						
	<div id="bib_89639e903f47b886afaa816b36fa0f0blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Contextual word spotting in historical manuscripts using markov logic networks</b>. <br/>
In: , pages 36-43.
ACM, 2013.

<br/>
David Fernández, Simone Marinai, Josep Lladόs and Alicia Fornés.
<br/>

<a href="http://dx.doi.org/10.1145/2501115.2501119">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1d9a3ed797026e73fd52ccf9763de207'); return false;" href="https://www.bibsonomy.org/bibtex/21d9a3ed797026e73fd52ccf9763de207/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1d9a3ed797026e73fd52ccf9763de207', 'https://www.bibsonomy.org/bibtex/21d9a3ed797026e73fd52ccf9763de207/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21d9a3ed797026e73fd52ccf9763de207/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1d9a3ed797026e73fd52ccf9763de207lepsky" style="display:none;border:1px dotted grey;">
Natural languages can often be modelled by suitable grammars whose knowledge can improve the word spotting results. The implicit contextual information is even more useful when dealing with information that is intrinsically described as one collection of records. In this paper, we present one approach to word spotting which uses the contextual information of records to improve the results. The method relies on Markov Logic Networks to probabilistically model the relational organization of handwritten records. The performance has been evaluated on the Barcelona Marriages Dataset that contains structured handwritten records that summarize marriage information.
</div>
<div style="position:relative">						
	<div id="bib_1d9a3ed797026e73fd52ccf9763de207lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Improving term extraction with linguistic analysis in the biomedical domain</b>. <br/>
In: , pages 24-30.
2013.

<br/>
Wiktoria Golik, Robert Bossy, Zorana Ratkovic and Nédellec Claire.
<br/>

<a href="http://pics.cicling.org/2013/rcs/Improving%20term%20extraction%20with%20linguistic%20analysis%20in%20the%20biomedical%20domain.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c46e4e5345fcc5664ed814b8ee3aa2fb'); return false;" href="https://www.bibsonomy.org/bibtex/2c46e4e5345fcc5664ed814b8ee3aa2fb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c46e4e5345fcc5664ed814b8ee3aa2fb', 'https://www.bibsonomy.org/bibtex/2c46e4e5345fcc5664ed814b8ee3aa2fb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c46e4e5345fcc5664ed814b8ee3aa2fb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c46e4e5345fcc5664ed814b8ee3aa2fblepsky" style="display:none;border:1px dotted grey;">
This paper presents a linguistic-based approach to term extraction from corpora in the biomedical domain. The method is based on an analysis of terms and their context that verify linguistic constraints. It focuses on participles and prepositional complements. The purpose of our approach is to obtain terms that are relevant for knowledge acquisition applications, such as the creation and en- richment of terminologies and ontologies. We report on the evaluations we conducted by applying two complementary strategies, using a reference termi- nology and a manual validation. They were applied to two corpora of differing genres and Life Science domains, namely pharmacology patents and animal physiology scientific articles. Our work shows that the linguistic analysis-based developments significantly improve the extraction results. The method is espe- cially efficient when dealing withgerunds and to prepositionalmodifiers.
</div>
<div style="position:relative">						
	<div id="bib_c46e4e5345fcc5664ed814b8ee3aa2fblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Extraction of semantic relationships from academic papers using syntactic patterns</b>. <br/>
In: .
2013.

<br/>
Akihiro Kameda, Kiyoko Uchiyama, Hideaki Takeda and Akiko Aizawa.
<br/>


<a onclick="toggleAbstract('lepsky', '38bfda67b8229fe74125cb1bb1695ac9'); return false;" href="https://www.bibsonomy.org/bibtex/238bfda67b8229fe74125cb1bb1695ac9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '38bfda67b8229fe74125cb1bb1695ac9', 'https://www.bibsonomy.org/bibtex/238bfda67b8229fe74125cb1bb1695ac9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/238bfda67b8229fe74125cb1bb1695ac9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_38bfda67b8229fe74125cb1bb1695ac9lepsky" style="display:none;border:1px dotted grey;">
Integrating concept and citation networks on a specific research subject can help researchers focus their own work or use methods described in prior works. In this paper, we propose a method to extract semantic relations from concepts and citation in the descriptions of related work. Specifically, we examined (i) topic-paper relations between research topics and reference papers and (ii) method-purpose relations between research topics. We also defined 15 lexico-syntactic patterns for the relation extraction. Results of experiments using a manually annotated dataset of 15 papers demonstrated the effectiveness of using the proposed lexico-syntactic patterns.
</div>
<div style="position:relative">						
	<div id="bib_38bfda67b8229fe74125cb1bb1695ac9lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A novel topic model for automatic term extraction</b>. <br/>
In: , pages 885-888.
ACM, 2013.

<br/>
Sujian Li, Jiwei Li, Tao Song, Wenjie Li and Baobao Chang.
<br/>

<a href="http://dx.doi.org/10.1145/2484028.2484106">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '89c2bc271dc10b7d307b832755b0757e'); return false;" href="https://www.bibsonomy.org/bibtex/289c2bc271dc10b7d307b832755b0757e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '89c2bc271dc10b7d307b832755b0757e', 'https://www.bibsonomy.org/bibtex/289c2bc271dc10b7d307b832755b0757e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/289c2bc271dc10b7d307b832755b0757e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_89c2bc271dc10b7d307b832755b0757elepsky" style="display:none;border:1px dotted grey;">
Automatic term extraction (ATE) aims at extracting domain-specific terms from a corpus of a certain domain. Termhood is one essential measure for judging whether a phrase is a term. Previous researches on termhood mainly depend on the word frequency information. In this paper, we propose to compute termhood based on semantic representation of words. A novel topic model, namely i-SWB, is developed to map the domain corpus into a latent semantic space, which is composed of some general topics, a background topic and a documents-specific topic. Experiments on four domains demonstrate that our approach outperforms the state-of-the-art ATE approaches.
</div>
<div style="position:relative">						
	<div id="bib_89c2bc271dc10b7d307b832755b0757elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Multilingual compound splitting combining language dependent and independent features</b>. <br/>
, 2013.

<br/>
Elizaveta Loginova-Clouet and Béatrice Daille.
<br/>
<a href="http://www.dialog-21.ru/digests/dialog2013/materials/pdf/Loginova-ClouetEA.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7d0410ed01a121ab19a90ea2c11665cd'); return false;" href="https://www.bibsonomy.org/bibtex/27d0410ed01a121ab19a90ea2c11665cd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7d0410ed01a121ab19a90ea2c11665cd', 'https://www.bibsonomy.org/bibtex/27d0410ed01a121ab19a90ea2c11665cd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27d0410ed01a121ab19a90ea2c11665cd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7d0410ed01a121ab19a90ea2c11665cdlepsky" style="display:none;border:1px dotted grey;">
Compounding is a common phenomenon for many languages, especially those with rich morphology. Dealing with compounds is a challenge for NLP systems since compounds are not often included in the dictionaries and other lexical sources. We present a compound splitting method combining language independent features (similarity measure, corpus data) and language specific component transformation rules. Due to the usage of language independent features, the method can be applied to different languages. We report on our experiments in splitting of German and Russian compound words, giving positive results compared to matching of compound parts in a lexicon. To the best of our knowledge. elaborated compound splitting is a rare component of NLP systems for Russian, yet our experiments show that it could be beneficial to use a specialized vocabulary.
</div>
<div style="position:relative">						
	<div id="bib_7d0410ed01a121ab19a90ea2c11665cdlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An experimental study of term extraction for real information-retrieval thesauri</b>. <br/>
In: , pages 69-76.
2013.

<br/>
Natalia Loukachevitch and Michael Nokel.
<br/>

<a href="https://lipn.univ-paris13.fr/tia2013/Proceedings/actesTIA2013.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'fa5a6cc80a2e5f4b4b155bca6183f07a'); return false;" href="https://www.bibsonomy.org/bibtex/2fa5a6cc80a2e5f4b4b155bca6183f07a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'fa5a6cc80a2e5f4b4b155bca6183f07a', 'https://www.bibsonomy.org/bibtex/2fa5a6cc80a2e5f4b4b155bca6183f07a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2fa5a6cc80a2e5f4b4b155bca6183f07a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_fa5a6cc80a2e5f4b4b155bca6183f07alepsky" style="display:none;border:1px dotted grey;">
Models for effective term extraction can de- pend on the type of a terminological re- source under construction. In this paper we study term extraction models for real- working information-retrieval thesauri. The first thesaurus is the English version of Eu- roVoc thesaurus, the second one is the Rus- sian Banking thesaurus. We study single- word and two-word term extraction sepa- rately to reveal the best features and fea- ture combinations, compare best models for two thesauri. In particular, we found for this type of terminological resources that the use of association measures does not im- prove the quality of two-word term extrac- tion based on combining multiple features.
</div>
<div style="position:relative">						
	<div id="bib_fa5a6cc80a2e5f4b4b155bca6183f07alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic construction of lexicons, taxonomies, ontologies, and other knowledge structures</b>. <br/>
<i>WIREs Data Mining Knowl Discov</i>, 3(4):257-279, 2013.

<br/>
Olena Medelyan, Ian Witten, Anna Divoli and Jeen Broekstra.
<br/>
<a href="http://dx.doi.org/10.1002/widm.1097">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e0f15a986fa30a2b8aa7eaea218b2e7e'); return false;" href="https://www.bibsonomy.org/bibtex/2e0f15a986fa30a2b8aa7eaea218b2e7e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e0f15a986fa30a2b8aa7eaea218b2e7e', 'https://www.bibsonomy.org/bibtex/2e0f15a986fa30a2b8aa7eaea218b2e7e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e0f15a986fa30a2b8aa7eaea218b2e7e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e0f15a986fa30a2b8aa7eaea218b2e7elepsky" style="display:none;border:1px dotted grey;">
Abstract, structured, representations of knowledge such as lexicons, taxonomies, and ontologies have proven to be powerful resources not only for the systematization of knowledge in general, but to support practical technologies of document organization, information retrieval, natural language understanding, and question-answering systems. These resources are extremely time consuming for people to create and maintain, yet demand for them is growing, particularly in specialized areas ranging from legacy documents of large enterprises to rapidly changing domains such as current affairs and celebrity news. Consequently, researchers are investigating methods of creating such structures automatically from document collections, calling on the proliferation of interlinked resources already available on the web for background knowledge and general information about the world. This review surveys what is possible, and also outlines current research directions.
</div>
<div style="position:relative">						
	<div id="bib_e0f15a986fa30a2b8aa7eaea218b2e7elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic text analysis by artificial intelligence</b>. <br/>
, 2013.

<br/>
Dunja Mladenić and Marko Grobelnik.
<br/>
<a href="http://www.informatica.si/PDF/37-1/05_Mladenic-Automatic%20Text%20Analysis%20by%20Artificial%20Intell.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '55165fc7f1f9dcfa0741cd5292d20818'); return false;" href="https://www.bibsonomy.org/bibtex/255165fc7f1f9dcfa0741cd5292d20818/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '55165fc7f1f9dcfa0741cd5292d20818', 'https://www.bibsonomy.org/bibtex/255165fc7f1f9dcfa0741cd5292d20818/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/255165fc7f1f9dcfa0741cd5292d20818/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_55165fc7f1f9dcfa0741cd5292d20818lepsky" style="display:none;border:1px dotted grey;">
Text is one of the traditional ways of communication between people. With the growing availability of text data in electronic form, handling and analysis of text by means of computers gained popularity. Handling text data with machine learning methods brought interesting challenges to the area that got further extended by incorporation of some natural language specifics. As the methods were capable of addressing more complex problems related to text data, the expectations become bigger calling for more sophisticated methods, in particular a combination of methods from different research areas including information retrieval, machine learning, statistical data analysis, data mining, natural language processing, semantic technologies. Automatic text analysis become an integral part of many systems, pushing boundaries of research capabilities towards what one can refer to as an artificial intelligence dream - never ending learning from text aiming at mimicking ways of human learning. The paper presents development of text analysis research in Slovenian that we have been personally involved in, pointing out interesting research problems that have been and are still addressed by the research, example tasks that have been addressed and some challenges on the way.
</div>
<div style="position:relative">						
	<div id="bib_55165fc7f1f9dcfa0741cd5292d20818lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Introduction to the special issue on multiword expressions : from theory to practice and use</b>. <br/>
<i>ACM Trans. Speech Lang. Process.</i>, 10(2), 2013.

<br/>
Carlos Ramisch, Aline Villavicencio and Valia Kordoni.
<br/>
<a href="http://dx.doi.org/10.1145/2483691.2483692">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a99606c725721df9f4fbe4de61333158'); return false;" href="https://www.bibsonomy.org/bibtex/2a99606c725721df9f4fbe4de61333158/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a99606c725721df9f4fbe4de61333158', 'https://www.bibsonomy.org/bibtex/2a99606c725721df9f4fbe4de61333158/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a99606c725721df9f4fbe4de61333158/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a99606c725721df9f4fbe4de61333158lepsky" style="display:none;border:1px dotted grey;">
We are in 2013, and multiword expressions have been around for a while in the computational linguistics research community. Since the first ACL workshop on MWEs 12 years ago in Sapporo, Japan, much has been discussed, proposed, experimented, evaluated and argued about MWEs. And yet, they deserve the publication of a whole special issue of the ACM Transactions on Speech and Language Processing. But what is it about multiword expressions that keeps them in fashion? Who are the people and the institutions who perform and publish groundbreaking fundamental and applied research in this field? What is the place and the relevance of our lively research community in the bigger picture of computational linguistics? Where do we come from as a community, and most importantly, where are we heading? In this introductory article, we share our point of view about the answers to these questions and introduce the articles that compose the current special issue.
</div>
<div style="position:relative">						
	<div id="bib_a99606c725721df9f4fbe4de61333158lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Rule-based versus training-based extraction of index terms from business documents : how to combine the results</b>. <br/>
:865813, 2013.

<br/>
Daniel Schuster, Marcel Hanke, Klemens Muthmann and Daniel Esser.
<br/>
<a href="http://dx.doi.org/10.1117/12.2002509">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b2824eb0a91fd457e48cfeb9f581d409'); return false;" href="https://www.bibsonomy.org/bibtex/2b2824eb0a91fd457e48cfeb9f581d409/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b2824eb0a91fd457e48cfeb9f581d409', 'https://www.bibsonomy.org/bibtex/2b2824eb0a91fd457e48cfeb9f581d409/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b2824eb0a91fd457e48cfeb9f581d409/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b2824eb0a91fd457e48cfeb9f581d409lepsky" style="display:none;border:1px dotted grey;">
Current systems for automatic extraction of index terms from business documents either take a rule-based or training-based approach. As both approaches have their advantages and disadvantages it seems natural to combine both methods to get the best of both worlds. We present a combination method with the steps selection, normalization, and combination based on comparable scores produced during extraction. Furthermore, novel evaluation metrics are developed to support the assessment of each step in an existing extraction system. Our methods were evaluated on an example extraction system with three individual extractors and a corpus of 12,000 scanned business documents.
</div>
<div style="position:relative">						
	<div id="bib_b2824eb0a91fd457e48cfeb9f581d409lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>The DeLiVerMATH project : text analysis in mathematics</b>. <br/>
, 2013.

<br/>
Ulf Schöneberg and Wolfram Sperber.
<br/>
<a href="http://arxiv.org/abs/1306.6944">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ee6cc2c85db348a757a8cf8ac9117eb6'); return false;" href="https://www.bibsonomy.org/bibtex/2ee6cc2c85db348a757a8cf8ac9117eb6/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ee6cc2c85db348a757a8cf8ac9117eb6', 'https://www.bibsonomy.org/bibtex/2ee6cc2c85db348a757a8cf8ac9117eb6/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ee6cc2c85db348a757a8cf8ac9117eb6/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ee6cc2c85db348a757a8cf8ac9117eb6lepsky" style="display:none;border:1px dotted grey;">
A high-quality content analysis is essential for retrieval functionalities but the manual extraction of key phrases and classification is expensive. Natural language processing provides a framework to automatize the process. Here, a machine-based approach for the content analysis of mathematical texts is described. A prototype for key phrase extraction and classification of mathematical texts is presented.
</div>
<div style="position:relative">						
	<div id="bib_ee6cc2c85db348a757a8cf8ac9117eb6lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>On the semantics of noun compounds</b>. <br/>
<i>Natural Language Engineering</i>, FirstView:1-2, 2013.

<br/>
Stan Szpakowicz, Francis Bond, Preslav Nakov and Su Kim.
<br/>
<a href="http://dx.doi.org/10.1017/S1351324913000090">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1e7121544f3e26fe05236fc742f6af8c'); return false;" href="https://www.bibsonomy.org/bibtex/21e7121544f3e26fe05236fc742f6af8c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1e7121544f3e26fe05236fc742f6af8c', 'https://www.bibsonomy.org/bibtex/21e7121544f3e26fe05236fc742f6af8c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21e7121544f3e26fe05236fc742f6af8c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1e7121544f3e26fe05236fc742f6af8clepsky" style="display:none;border:1px dotted grey;">
ABSTRACT The noun compound -- a sequence of nouns which functions as a single noun -- is very common in English texts. No language processing system should ignore expressions like steel soup pot cover if it wants to be serious about such high-end applications of computational linguistics as question answering, information extraction, text summarization, machine translation -- the list goes on. Processing noun compounds, however, is far from trouble-free. For one thing, they can be bracketed in various ways: is it steel soup, steel pot, or steel cover? Then there are relations inside a compound, annoyingly not signalled by any words: does pot contain soup or is it for cooking soup? These and many other research challenges are the subject of this special issue.
</div>
<div style="position:relative">						
	<div id="bib_1e7121544f3e26fe05236fc742f6af8clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Web mining based extraction of problem solution ideas</b>. <br/>
<i>Expert Systems with Applications</i>, 40(10):3961-3969, 2013.

<br/>
D. Thorleuchter and D. Van den Poel.
<br/>

<a onclick="toggleAbstract('lepsky', 'f785a25ab5aaa951e2b5e479e85345dd'); return false;" href="https://www.bibsonomy.org/bibtex/2f785a25ab5aaa951e2b5e479e85345dd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f785a25ab5aaa951e2b5e479e85345dd', 'https://www.bibsonomy.org/bibtex/2f785a25ab5aaa951e2b5e479e85345dd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f785a25ab5aaa951e2b5e479e85345dd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f785a25ab5aaa951e2b5e479e85345ddlepsky" style="display:none;border:1px dotted grey;">
The internet is a valuable source of information where many ideas can be found dealing with different topics. A few numbers of ideas might be able to solve an existing problem. However, it is time-consuming to identify these ideas within the large amount of textual information in the internet. This paper introduces a new web mining approach that enables an automated identification of new technological ideas extracted from internet sources that are able to solve a given problem. It adapts and combines several existing approaches from literature: approaches that extract new technological ideas from a user given text, approaches that investigate the different idea characteristics in different technical domains, and multi-language web mining approaches. In contrast to previous work, the proposed approach enables the identification of problem solution ideas in the internet considering domain dependencies and language aspects. In a case study, new ideas are identified to solve existing technological problems as occurred in research and development (R&amp; projects. This supports the process of research planning and technology development.
</div>
<div style="position:relative">						
	<div id="bib_f785a25ab5aaa951e2b5e479e85345ddlepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2012" class="bibsonomy_quicknav_group"><a name="2012">2012</a></h3>
<div style="margin-bottom:1em">
<b>Introduction to information extraction : basic notions and current trends</b>. <br/>
<i>Datenbank-Spektrum</i>, 12(2):81-88, 2012.

<br/>
Wolf-Tilo Balke.
<br/>
<a href="http://dx.doi.org/10.1007/s13222-012-0090-x">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '5c5802be632cbf4e8236dea9fa25647c'); return false;" href="https://www.bibsonomy.org/bibtex/25c5802be632cbf4e8236dea9fa25647c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5c5802be632cbf4e8236dea9fa25647c', 'https://www.bibsonomy.org/bibtex/25c5802be632cbf4e8236dea9fa25647c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25c5802be632cbf4e8236dea9fa25647c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5c5802be632cbf4e8236dea9fa25647clepsky" style="display:none;border:1px dotted grey;">
Transforming unstructured or semi-structured information into structured knowledge is one of the big challenges of today's knowledge society. While this abstract goal is still unreached and probably unreachable, intelligent information extraction techniques are considered key ingredients on the way to generating and representing knowledge for a wide variety of applications. This is especially true for the current efforts to turn the World Wide Web being the world's largest collection of information into the world's largest knowledge base. This introduction gives a broad overview about the major topics and current trends in information extraction.
</div>
<div style="position:relative">						
	<div id="bib_5c5802be632cbf4e8236dea9fa25647clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>'Without the clutter of unimportant words' : descriptive keyphrases for text visualization</b>. <br/>
<i>ACM Trans. Comput.-Hum. Interact.</i>, 19(3), 2012.

<br/>
Jason Chuang, Christopher Manning and Jeffrey Heer.
<br/>
<a href="http://dx.doi.org/10.1145/2362364.2362367">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '792679b8e4b3b809b610cb9574025723'); return false;" href="https://www.bibsonomy.org/bibtex/2792679b8e4b3b809b610cb9574025723/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '792679b8e4b3b809b610cb9574025723', 'https://www.bibsonomy.org/bibtex/2792679b8e4b3b809b610cb9574025723/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2792679b8e4b3b809b610cb9574025723/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_792679b8e4b3b809b610cb9574025723lepsky" style="display:none;border:1px dotted grey;">
Keyphrases aid the exploration of text collections by communicating salient aspects of documents and are often used to create effective visualizations of text. While prior work in HCI and visualization has proposed a variety of ways of presenting keyphrases, less attention has been paid to selecting the best descriptive terms. In this article, we investigate the statistical and linguistic properties of keyphrases chosen by human judges and determine which features are most predictive of high-quality descriptive phrases. Based on 5,611 responses from 69 graduate students describing a corpus of dissertation abstracts, we analyze characteristics of human-generated keyphrases, including phrase length, commonness, position, and part of speech. Next, we systematically assess the contribution of each feature within statistical models of keyphrase quality. We then introduce a method for grouping similar terms and varying the specificity of displayed phrases so that applications can select phrases dynamically based on the available screen space and current context of interaction. Precision-recall measures find that our technique generates keyphrases that match those selected by human judges. Crowdsourced ratings of tag cloud visualizations rank our approach above other automatic techniques. Finally, we discuss the role of HCI methods in developing new algorithmic techniques suitable for user-facing applications.
</div>
<div style="position:relative">						
	<div id="bib_792679b8e4b3b809b610cb9574025723lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>An approach for named entity recognition in poorly structured data</b>. <br/>
In: E. Simperl, P. Cimiano, A. Polleres, Ó. Corcho and V. Presutti, editors, , volume 7295, pages 718-732.
Springer, 2012.

<br/>
Nuno Freire, José Borbinha and Pável Calado.
<br/>

<a href="http://dblp.uni-trier.de/db/conf/esws/eswc2012.html#FreireBC12">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1c9d1be99bbd133b499cfef49ff594b2'); return false;" href="https://www.bibsonomy.org/bibtex/21c9d1be99bbd133b499cfef49ff594b2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1c9d1be99bbd133b499cfef49ff594b2', 'https://www.bibsonomy.org/bibtex/21c9d1be99bbd133b499cfef49ff594b2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21c9d1be99bbd133b499cfef49ff594b2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1c9d1be99bbd133b499cfef49ff594b2lepsky" style="display:none;border:1px dotted grey;">
This paper describes an approach for the task of named entity recog- nition in structured data containing free text as the values of its elements. We studied the recognition of the entity types of person, location and organization in bibliographic data sets from a concrete wide digital library initiative. Our ap- proach is based on conditional random fields models, using features designed to perform named entity recognition in the absence of strong lexical evidence, and exploiting the semantic context given by the data structure. The evaluation re- sults support that, with the specialized features, named entity recognition can be done in free text within structured data with an acceptable accuracy. Our ap- proach was able to achieve a maximum precision of 0.91 at 0.55 recall and a maximum recall of 0.82 at 0.77 precision. The achieved results were always higher than those obtained with Stanford Named Entity Recognizer, which was developed for grammatically well-formed text. We believe this level of quality in named entity recognition allows the use of this approach to support a wide range of information extraction applications in structured data.
</div>
<div style="position:relative">						
	<div id="bib_1c9d1be99bbd133b499cfef49ff594b2lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatisches Indexieren einer informationswissenschaftlichen Datenbank mit Mehrwortgruppen</b>. <br/>
, 2012.

<br/>
Lena Glaesener.
<br/>

<a onclick="toggleAbstract('lepsky', 'df625681ebfd898d08b3001d5c9dfa47'); return false;" href="https://www.bibsonomy.org/bibtex/2df625681ebfd898d08b3001d5c9dfa47/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'df625681ebfd898d08b3001d5c9dfa47', 'https://www.bibsonomy.org/bibtex/2df625681ebfd898d08b3001d5c9dfa47/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2df625681ebfd898d08b3001d5c9dfa47/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_df625681ebfd898d08b3001d5c9dfa47lepsky" style="display:none;border:1px dotted grey;">
Ein Bericht über die Ergebnisse und die Prozessanalyse einer automatischen Indexierung mit Mehrwortgruppen. Diese Bachelorarbeit beschreibt, inwieweit der Inhalt informationswissenschaftlicher Fachtexte durch informationswissenschaftliches Fachvokabular erschlossen werden kann und sollte und dass in diesen wissenschaftlichen Texten ein Grossteil der fachlichen Inhalte in Mehrwortgruppen vorkommt. Die Ergebnisse wurden durch eine automatische Indexierung mit Mehrwortgruppen mithilfe des Programme Lingo an einer informationswissenschaftlichen Datenbank ermittelt.
</div>
<div style="position:relative">						
	<div id="bib_df625681ebfd898d08b3001d5c9dfa47lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Detecting multiword phrases in mathematical text corpora</b>. <br/>
<i>arXiv</i>, 2012.

<br/>
Winfried Gödert.
<br/>
<a href="http://arxiv.org/abs/1210.0852">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b07ea77151e5f279eaa6240ec524c932'); return false;" href="https://www.bibsonomy.org/bibtex/2b07ea77151e5f279eaa6240ec524c932/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b07ea77151e5f279eaa6240ec524c932', 'https://www.bibsonomy.org/bibtex/2b07ea77151e5f279eaa6240ec524c932/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b07ea77151e5f279eaa6240ec524c932/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b07ea77151e5f279eaa6240ec524c932lepsky" style="display:none;border:1px dotted grey;">
We present an approach for detecting multiword phrases in mathematical text corpora. The method used is based on characteristic features of mathematical terminology. It makes use of a software tool named Lingo which allows to identify words by means of previously defined dictionaries for specific word classes as adjectives, personal names or nouns. The detection of multiword groups is done algorithmically. Possible advantages of the method for indexing and information retrieval and conclusions for applying dictionary-based methods of automatic indexing instead of stemming procedures are discussed.
</div>
<div style="position:relative">						
	<div id="bib_b07ea77151e5f279eaa6240ec524c932lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic multi-word term extraction and its application to web-page summarization</b>. <br/>
, 2012.

<br/>
Weiwei Huo.
<br/>
<a href="https://atrium.lib.uoguelph.ca/xmlui/bitstream/handle/10214/4959/Thesis_21.pdf?sequence=1">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '519086871efb95b81d609067f7a2180a'); return false;" href="https://www.bibsonomy.org/bibtex/2519086871efb95b81d609067f7a2180a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '519086871efb95b81d609067f7a2180a', 'https://www.bibsonomy.org/bibtex/2519086871efb95b81d609067f7a2180a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2519086871efb95b81d609067f7a2180a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_519086871efb95b81d609067f7a2180alepsky" style="display:none;border:1px dotted grey;">
In this thesis we propose three new word association measures for multi-word term extraction. We combine these association measures with LocalMaxs algorithm in our extraction model and compare the results of different multi-word term extraction methods. Our approach is language and domain independent and requires no training data. It can be applied to such tasks as text summarization, information retrieval, and document classification. We further explore the potential of using multi-word terms as an effective representation for general web-page summarization. We extract multi-word terms from human written summaries in a large collection of web-pages, and generate the summaries by aligning document words with these multi-word terms. Our system applies machine translation technology to learn the aligning process from a training set and focuses on selecting high quality multi-word terms from human written summaries to generate suitable results for web-page summarization.
</div>
<div style="position:relative">						
	<div id="bib_519086871efb95b81d609067f7a2180alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>DIKEA : domain-independent keyphrase extraction algorithm</b>. <br/>
In: <i>Proceedings of the 25th Australasian Joint Conference on Advances in Artificial Intelligence</i>, series AI'12, pages 719-730.
Springer, Berlin ; Heidelberg, 2012.

<br/>
David X. Wang, Xiaoying Gao and Peter Andreae.
<br/>

<a href="http://dx.doi.org/10.1007/978-3-642-35101-3_61">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '18cc1f54087a35ad41cebdb658f05ce2'); return false;" href="https://www.bibsonomy.org/bibtex/218cc1f54087a35ad41cebdb658f05ce2/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '18cc1f54087a35ad41cebdb658f05ce2', 'https://www.bibsonomy.org/bibtex/218cc1f54087a35ad41cebdb658f05ce2/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/218cc1f54087a35ad41cebdb658f05ce2/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_18cc1f54087a35ad41cebdb658f05ce2lepsky" style="display:none;border:1px dotted grey;">
This paper introduces a new domain-independent keyphrase extraction system (DIKEA). Keyphrase extraction is a challenging problem that automatically extracts or assigns keyphrases to documents and it can benefit many research areas such as information retrieval, particularly indexing, clustering, and summarization. A landmark research KEA (Keyphrase Extraction Algorithm) formulated the problem as a supervised machine learning problem and successfully applied a Naïve Bayes model to it, which showed great promise but the performance is not satisfactory. Its state-of-the-art extension KEA++ has a significantly improved performance but relies on a domain specific vocabulary which is often not available or not complete. This paper introduces a novel domain-independent approach and has three main contributions: utilising the largest online knowledge source--Wikipedia--for keyphrase candidate selection; presenting new features for keyphrase evaluation, including a Wikipedia-based feature---link probability; and evaluating a number of different learning algorithms, including multilayer perceptrons, for keyphrase selection. Experiments show that our system clearly outperforms KEA and closely matches the performance of KEA++, without requiring any domain-specific knowledge such as KEA++'s vocabulary list.
</div>
<div style="position:relative">						
	<div id="bib_18cc1f54087a35ad41cebdb658f05ce2lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2011" class="bibsonomy_quicknav_group"><a name="2011">2011</a></h3>
<div style="margin-bottom:1em">
<b>Terminus : a workstation for terminology  and corpus management</b>.<br/>
In: 
Institut Porphyre,, Savoir et Connaissance and Institut Porphyre,, editors, 
<i>TOTh 2011 Proceedings - Terminology &amp; Ontology: Theories and applications; Annecy  – 26 &amp; 27 mai 2011</i>, pages 63-74.
Annecy, 2011.

<br/>
María Teresa Cabré and Rogelio Nazar.
<br/>

<a href="https://hal.archives-ouvertes.fr/hal-01354937">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ad23f87b0ae29207e2fd543b991b2059'); return false;" href="https://www.bibsonomy.org/bibtex/2ad23f87b0ae29207e2fd543b991b2059/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ad23f87b0ae29207e2fd543b991b2059', 'https://www.bibsonomy.org/bibtex/2ad23f87b0ae29207e2fd543b991b2059/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ad23f87b0ae29207e2fd543b991b2059/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ad23f87b0ae29207e2fd543b991b2059lepsky" style="display:none;border:1px dotted grey;">
Terminus is a software developed for terminologists to go through the whole process of glossary creation, including tools for corpus compilation, corpus exploration and term management. In this paper, we present a descrip - tion of its functions and examples of its application. Also, we present the algo - rithms of two new functions which are currently being added to a Beta version of the program. The first is the filtering of a set of documents according to lev - el of specialization and relevance with respect to the analyzed thematic do - main. The second algorithm is for automatic term extraction. Needless to say, these represent fields of research that have received a lot of attention from the community of terminologists and corpus linguists in the last decades. We pro - vide a description of our strategies and the results we have obtained with these experimental modules.
</div>
<div style="position:relative">						
	<div id="bib_ad23f87b0ae29207e2fd543b991b2059lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2010" class="bibsonomy_quicknav_group"><a name="2010">2010</a></h3>
<div style="margin-bottom:1em">
<b>Improving term extraction using particle swarm optimization techniques</b>. <br/>
<i>arXiv:1002.4041 [cs]</i>, 2010.

<br/>
Mohammad Syafrullah and Naomie Salim.
<br/>
<a href="http://arxiv.org/abs/1002.4041">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '980ae9f3d62d59e49a08f7b667217a06'); return false;" href="https://www.bibsonomy.org/bibtex/2980ae9f3d62d59e49a08f7b667217a06/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '980ae9f3d62d59e49a08f7b667217a06', 'https://www.bibsonomy.org/bibtex/2980ae9f3d62d59e49a08f7b667217a06/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2980ae9f3d62d59e49a08f7b667217a06/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_980ae9f3d62d59e49a08f7b667217a06lepsky" style="display:none;border:1px dotted grey;">
Term extraction is one of the layers in the ontology development process which has the task to extract all the terms contained in the input document automatically. The purpose of this process is to generate list of terms that are relevant to the domain of the input document. In the literature there are many approaches, techniques and algorithms used for term extraction. In this paper we propose a new approach using particle swarm optimization techniques in order to improve the accuracy of term extraction results. We choose five features to represent the term score. The approach has been applied to the domain of religious document. We compare our term extraction method precision with TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental results show that our propose approach achieve better precision than those four algorithm.
</div>
<div style="position:relative">						
	<div id="bib_980ae9f3d62d59e49a08f7b667217a06lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2009" class="bibsonomy_quicknav_group"><a name="2009">2009</a></h3>
<div style="margin-bottom:1em">
<b>Evaluating term extraction</b>. <br/>
In: <i>RANLP</i>.
2009.

<br/>
Adeline Nazarenko and Haïfa Zargayouna.
<br/>

<a href="https://pdfs.semanticscholar.org/a953/001b2be9b73574418ba42f8f48017629e11e.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7284367d0fec6099ef7000001fbf1a13'); return false;" href="https://www.bibsonomy.org/bibtex/27284367d0fec6099ef7000001fbf1a13/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7284367d0fec6099ef7000001fbf1a13', 'https://www.bibsonomy.org/bibtex/27284367d0fec6099ef7000001fbf1a13/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27284367d0fec6099ef7000001fbf1a13/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7284367d0fec6099ef7000001fbf1a13lepsky" style="display:none;border:1px dotted grey;">
In contrast with other NLP tasks, only few and limited evaluation challenges have been carried out for terminology acquisition. It is nevertheless important to assess the progress made, the quality and limitations of terminological tools. This paper argues that it is possible to define evaluation protocols for tasks as complex as computational terminology. We focus on the core task of term extraction for which we propose evaluation metrics. We take into account the specificity of computational terminology, the complexity of its outputs, the application, the user's role and the absence of well-established gold standard.
</div>
<div style="position:relative">						
	<div id="bib_7284367d0fec6099ef7000001fbf1a13lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2008" class="bibsonomy_quicknav_group"><a name="2008">2008</a></h3>
<div style="margin-bottom:1em">
<b>An evaluation of methods for the extraction of multiword expressions</b>. <br/>
In: , pages 50-53.
2008.

<br/>
Carlos Ramisch, Paulo Schreiner, Marco Idiart and Aline Villavicencio.
<br/>

<a href="http://lrec.elra.info/proceedings/lrec2008/workshops/W20_Proceedings.pdf#page=54">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '53d2a5cd517b872fc251110e211b559a'); return false;" href="https://www.bibsonomy.org/bibtex/253d2a5cd517b872fc251110e211b559a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '53d2a5cd517b872fc251110e211b559a', 'https://www.bibsonomy.org/bibtex/253d2a5cd517b872fc251110e211b559a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/253d2a5cd517b872fc251110e211b559a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_53d2a5cd517b872fc251110e211b559alepsky" style="display:none;border:1px dotted grey;">
This paper focuses on the evaluation of some methods for the automatic acquisition of Multiword Expressions (MWEs). First we investigate the hypothesis that MWEs can be detected solely by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: Mutual Information, χ2 and Permutation Entropy. Moreover, we also look at the impact that the addition of type-specific linguistic information has on the performance of these methods.
</div>
<div style="position:relative">						
	<div id="bib_53d2a5cd517b872fc251110e211b559alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2006" class="bibsonomy_quicknav_group"><a name="2006">2006</a></h3>
<div style="margin-bottom:1em">
<b>Information extraction, automatic</b>. <br/>
<i>Encyclopedia of Language and Linguistics, 2nd Edition</i>, 5:665-677, 2006.

<br/>
Hamish Cunningham.
<br/>


<a onclick="toggleBibtex('lepsky', '6732cd367d587f64867dd1506ec4aa82', 'https://www.bibsonomy.org/bibtex/26732cd367d587f64867dd1506ec4aa82/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26732cd367d587f64867dd1506ec4aa82/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6732cd367d587f64867dd1506ec4aa82lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_6732cd367d587f64867dd1506ec4aa82lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Topic modeling : beyond bag-of-words</b>. <br/>
In: <i>Proceedings of the 23rd International Conference on Machine Learning</i>, series ICML '06, pages 977-984.
ACM, New York, NY, 2006.

<br/>
Hanna M. Wallach.
<br/>

<a href="http://doi.acm.org/10.1145/1143844.1143967">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '853dcaf68ccf30ced4c60fef61b5a048'); return false;" href="https://www.bibsonomy.org/bibtex/2853dcaf68ccf30ced4c60fef61b5a048/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '853dcaf68ccf30ced4c60fef61b5a048', 'https://www.bibsonomy.org/bibtex/2853dcaf68ccf30ced4c60fef61b5a048/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2853dcaf68ccf30ced4c60fef61b5a048/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_853dcaf68ccf30ced4c60fef61b5a048lepsky" style="display:none;border:1px dotted grey;">
Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the "bag-of-words" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.
</div>
<div style="position:relative">						
	<div id="bib_853dcaf68ccf30ced4c60fef61b5a048lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2005" class="bibsonomy_quicknav_group"><a name="2005">2005</a></h3>
<div style="margin-bottom:1em">
<b>Terminology extraction : an analysis of linguistic and statistical approaches</b>.<br/>
In: 
S. Sirmakessis, editor, 
<i>Knowledge Mining Series: Studies in Fuzziness and Soft Computing</i>.
Springer, 2005.

<br/>
M Pazienza, M Pennacchiotti and F Zanzotto.
<br/>

<a href="http://terra.info.uniroma2.it/pennacchiotti/publications/SFSC_2005.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'a11fd54fd2d7f09dd0db604cf6d7cd9d', 'https://www.bibsonomy.org/bibtex/2a11fd54fd2d7f09dd0db604cf6d7cd9d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a11fd54fd2d7f09dd0db604cf6d7cd9d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a11fd54fd2d7f09dd0db604cf6d7cd9dlepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_a11fd54fd2d7f09dd0db604cf6d7cd9dlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Terminology extraction and automatic indexing : comparison and qualitative evaluation of methods</b>. <br/>
In: .
2005.

<br/>
Hans Friedrich Witschel.
<br/>

<a href="http://wortschatz.uni-leipzig.de/~fwitschel/papers/TKEIndexing.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '0b29ef26b5fbeafcc1668403748c65e9'); return false;" href="https://www.bibsonomy.org/bibtex/20b29ef26b5fbeafcc1668403748c65e9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0b29ef26b5fbeafcc1668403748c65e9', 'https://www.bibsonomy.org/bibtex/20b29ef26b5fbeafcc1668403748c65e9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20b29ef26b5fbeafcc1668403748c65e9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0b29ef26b5fbeafcc1668403748c65e9lepsky" style="display:none;border:1px dotted grey;">
Many terminology engineering processes involve the task of automatic terminology extraction: before the terminology of a given domain can be modelled, organised or standardised, important concepts (or terms) of this domain have to be identified and fed into terminological databases. On the other hand, many machine learning or information retrieval applications require automatic indexing techniques. In Machine Learning applications concerned with the automatic clustering or classification of texts, often feature vectors are needed that describe the contents of a given text briefly but meaningfully. Short but meaningful descriptions of document contents as provided by good index terms are also useful to humans: some knowledge management applications (e.g. topic maps) use them as a set of basic concepts (topics). The author believes that the tasks of terminology extraction and automatic indexing have much in common and can thus benefit from the same set of basic algorithms. It is the goal of this paper to outline some methods that may be used in both contexts, but also to find the discriminating factors between the two tasks that call for the variation of parameters or application of different techniques.
</div>
<div style="position:relative">						
	<div id="bib_0b29ef26b5fbeafcc1668403748c65e9lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Text, Wörter, Morpheme : Möglichkeiten einer automatischen Terminologie-Extraktion</b>. <br/>
In: B. Fisseni, H. Schmitz, B. Schröder and P. Wagner, editors, , pages 659-672.
Peter Lang, 2005.

<br/>
Hans Friedrich Witschel.
<br/>


<a onclick="toggleAbstract('lepsky', 'c8f84ed74938491417ec9453d49c9257'); return false;" href="https://www.bibsonomy.org/bibtex/2c8f84ed74938491417ec9453d49c9257/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c8f84ed74938491417ec9453d49c9257', 'https://www.bibsonomy.org/bibtex/2c8f84ed74938491417ec9453d49c9257/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c8f84ed74938491417ec9453d49c9257/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c8f84ed74938491417ec9453d49c9257lepsky" style="display:none;border:1px dotted grey;">
This paper describes the possibility to automatically extract terminology (i.e. technical terms) from text using various statistical and pattern-based methods. Ideas are derived from definitions and theoretical characteristics of terminology and from existing approaches to terminology extraction. An own implementation of a selection of these ideas is being described and evaluated.
</div>
<div style="position:relative">						
	<div id="bib_c8f84ed74938491417ec9453d49c9257lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2004" class="bibsonomy_quicknav_group"><a name="2004">2004</a></h3>
<div style="margin-bottom:1em">
<b>Keyword extraction from a single document using word co-occurrence statistical information</b>. <br/>
<i>International Journal on Artificial Intelligence Tools</i>, 13:2004, 2004.

<br/>
Y Matsuo and M Ishizuka.
<br/>

<a onclick="toggleAbstract('lepsky', '4d2d886adc1ed629b70e943e99f5e4c1'); return false;" href="https://www.bibsonomy.org/bibtex/24d2d886adc1ed629b70e943e99f5e4c1/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4d2d886adc1ed629b70e943e99f5e4c1', 'https://www.bibsonomy.org/bibtex/24d2d886adc1ed629b70e943e99f5e4c1/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24d2d886adc1ed629b70e943e99f5e4c1/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4d2d886adc1ed629b70e943e99f5e4c1lepsky" style="display:none;border:1px dotted grey;">
This paper explains a keyword extraction algorithm based solely on a single document. First, frequent terms are extracted. Co-occurrences of a term and frequent terms are counted. If a term appears frequently with a particular subset of terms, the term is likely to have important meaning. The degree of bias of the cooccurrence distribution is measured by the  -measure. We show that our keyword extraction performs well without the need for a corpus. In this paper, a term is defined as a word or a word sequence. We do not intend to limit the meaning in a terminological sense. A word sequence is written as a phrase
</div>
<div style="position:relative">						
	<div id="bib_4d2d886adc1ed629b70e943e99f5e4c1lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>KIM : a semantic platform for information extraction and retrieval</b>. <br/>
<i>Natural language engineering</i>, 10(3-4):375-392, 2004.

<br/>
B Popov, A Kiryakov, D Ognyanoff, D Manov and A Kirilov.
<br/>


<a onclick="toggleBibtex('lepsky', '832e9af5ddf78c3a640fa1c094ac679b', 'https://www.bibsonomy.org/bibtex/2832e9af5ddf78c3a640fa1c094ac679b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2832e9af5ddf78c3a640fa1c094ac679b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_832e9af5ddf78c3a640fa1c094ac679blepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_832e9af5ddf78c3a640fa1c094ac679blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Terminologie-Extraktion : Möglichkeiten der Kombination statistischer und musterbasierter Verfahren</b>.
<br/>
PhD thesis, Universität Leipzig; Fakultät für Mathematik und Informatik, Leipzig, 2004.

<br/>
Hans Friedrich Witschel.
<br/>
<a href="http://wortschatz.uni-leipzig.de/~fwitschel/Diplomarbeit.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'abceb2e96e2f385c57857e546abb98da', 'https://www.bibsonomy.org/bibtex/2abceb2e96e2f385c57857e546abb98da/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2abceb2e96e2f385c57857e546abb98da/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_abceb2e96e2f385c57857e546abb98dalepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_abceb2e96e2f385c57857e546abb98dalepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2003" class="bibsonomy_quicknav_group"><a name="2003">2003</a></h3>
<div style="margin-bottom:1em">
<b>Term extraction and automatic indexing</b>.<br/>
In: 
R. Mitkov, editor, 
<i>Oxford Handbook of Computational Linguistics</i>, pages 599-615.
Oxford University Press, Oxford, 2003.

<br/>
Didier Bourigault and Christian Jacquemin.
<br/>



<a onclick="toggleBibtex('lepsky', 'a48c303fd1991e52f47a12de77586f51', 'https://www.bibsonomy.org/bibtex/2a48c303fd1991e52f47a12de77586f51/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a48c303fd1991e52f47a12de77586f51/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a48c303fd1991e52f47a12de77586f51lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_a48c303fd1991e52f47a12de77586f51lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Conceptual structuring through term variations</b>. <br/>
In: , pages 9-16.
2003.

<br/>
Béatrice Daille.
<br/>


<a onclick="toggleAbstract('lepsky', 'fded2de8339ba3f3eeaad6b95d73a69f'); return false;" href="https://www.bibsonomy.org/bibtex/2fded2de8339ba3f3eeaad6b95d73a69f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'fded2de8339ba3f3eeaad6b95d73a69f', 'https://www.bibsonomy.org/bibtex/2fded2de8339ba3f3eeaad6b95d73a69f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2fded2de8339ba3f3eeaad6b95d73a69f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_fded2de8339ba3f3eeaad6b95d73a69flepsky" style="display:none;border:1px dotted grey;">
Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95 
</div>
<div style="position:relative">						
	<div id="bib_fded2de8339ba3f3eeaad6b95d73a69flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic term recognition based on statistics of compound nouns and their components</b>. <br/>
<i>Terminology</i>, 9(2):201-219, 2003.

<br/>
Hiroshi Nakagawa and Tatsunori Mori.
<br/>
<a href="http://dx.doi.org/10.1075/term.9.2.04nak">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '7bd28fe4380f7607970026a37bc63ecb'); return false;" href="https://www.bibsonomy.org/bibtex/27bd28fe4380f7607970026a37bc63ecb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7bd28fe4380f7607970026a37bc63ecb', 'https://www.bibsonomy.org/bibtex/27bd28fe4380f7607970026a37bc63ecb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27bd28fe4380f7607970026a37bc63ecb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7bd28fe4380f7607970026a37bc63ecblepsky" style="display:none;border:1px dotted grey;">
In this paper, we propose a new approach to enhance automatic recognition systems for domain-specific terms. The approach is based on the statistics about the relation between a compound noun and its constituents that are simple nouns. More precisely, we focus on how many nouns adjoin the noun in question to form compound nouns. We propose several scoring methods based on this approach and experimentally evaluate them on the NTCIR1 TMREC test collection. The results are very promising, especially in low and high recall.
</div>
<div style="position:relative">						
	<div id="bib_7bd28fe4380f7607970026a37bc63ecblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A language model approach to keyphrase extraction</b>. <br/>
In: , pages 33-40.
Association for Computational Linguistics, 2003.

<br/>
Takashi Tomokiyo and Matthew Hurst.
<br/>

<a href="http://dx.doi.org/10.3115/1119282.1119287">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a6e6ee86b3731e3e2781b63efa9029fa'); return false;" href="https://www.bibsonomy.org/bibtex/2a6e6ee86b3731e3e2781b63efa9029fa/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a6e6ee86b3731e3e2781b63efa9029fa', 'https://www.bibsonomy.org/bibtex/2a6e6ee86b3731e3e2781b63efa9029fa/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a6e6ee86b3731e3e2781b63efa9029fa/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a6e6ee86b3731e3e2781b63efa9029falepsky" style="display:none;border:1px dotted grey;">
We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be unified into a single score to rank extracted phrases.
</div>
<div style="position:relative">						
	<div id="bib_a6e6ee86b3731e3e2781b63efa9029falepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2002" class="bibsonomy_quicknav_group"><a name="2002">2002</a></h3>
<div style="margin-bottom:1em">
<b>Multiword expressions : a pain in the neck for NLP</b>. <br/>
In: A. Gelbukh, editor, , volume 2276, pages 1-15.
Springer, 2002.

<br/>
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake and Dan Flickinger.
<br/>

<a href="http://dblp.uni-trier.de/db/conf/cicling/cicling2002.html#SagBBCF02">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'fc56eaf883a35fb9b3b0170d54ce311a', 'https://www.bibsonomy.org/bibtex/2fc56eaf883a35fb9b3b0170d54ce311a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2fc56eaf883a35fb9b3b0170d54ce311a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_fc56eaf883a35fb9b3b0170d54ce311alepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_fc56eaf883a35fb9b3b0170d54ce311alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2001" class="bibsonomy_quicknav_group"><a name="2001">2001</a></h3>
<div style="margin-bottom:1em"><b>Recent advances in computational terminology</b>.<br/>
2001. 

<br/>


<a onclick="toggleBibtex('lepsky', '9d4b14ca6abea40b6cce22c2844e2121', 'https://www.bibsonomy.org/bibtex/29d4b14ca6abea40b6cce22c2844e2121/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/29d4b14ca6abea40b6cce22c2844e2121/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_9d4b14ca6abea40b6cce22c2844e2121lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_9d4b14ca6abea40b6cce22c2844e2121lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Spotting and discovering terms through natural language processing</b>.<br/>
2001. 
<br/>Christian Jacquemin.
<br/>

<a onclick="toggleAbstract('lepsky', '608a38a8f9a6108b2b6675fde05f5dd3'); return false;" href="https://www.bibsonomy.org/bibtex/2608a38a8f9a6108b2b6675fde05f5dd3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '608a38a8f9a6108b2b6675fde05f5dd3', 'https://www.bibsonomy.org/bibtex/2608a38a8f9a6108b2b6675fde05f5dd3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2608a38a8f9a6108b2b6675fde05f5dd3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_608a38a8f9a6108b2b6675fde05f5dd3lepsky" style="display:none;border:1px dotted grey;">
In this book Christian Jacquemin shows how the power of natural language processing (NLP) can be used to advance text indexing and information retrieval (IR). Jacquemin's novel tool is FASTR, a parser that normalizes terms and recognizes term variants. Since there are more meanings in a language than there are words, FASTR uses a metagrammar composed of shallow linguistic transformations that describe the morphological, syntactic, semantic, and pragmatic variations of words and terms. The acquired parsed terms can then be applied for precise retrieval and assembly of information. The use of a corpus-based unification grammar to define, recognize, and combine term variants from their base forms allows for intelligent information access to, or "linguistic data tuning" of, heterogeneous texts. FASTR can be used to do automatic controlled indexing, to carry out content-based Web searches through conceptually related alternative query formulations, to abstract scientific and technical extracts, and even to translate and collect terms from multilingual material. Jacquemin provides a comprehensive account of the method and implementation of this innovative retrieval technique for text processing.
</div>
<div style="position:relative">						
	<div id="bib_608a38a8f9a6108b2b6675fde05f5dd3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Automatic term recognition based on statistics of compound nouns</b>. <br/>
<i>Terminology</i>, 6(2):195-210, 2001.

<br/>
H Nakagawa.
<br/>
<a href="http://dx.doi.org/10.1075/term.6.1.05nak">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c7c1b2c93b6b3f81f58b4d4f3abe5047'); return false;" href="https://www.bibsonomy.org/bibtex/2c7c1b2c93b6b3f81f58b4d4f3abe5047/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c7c1b2c93b6b3f81f58b4d4f3abe5047', 'https://www.bibsonomy.org/bibtex/2c7c1b2c93b6b3f81f58b4d4f3abe5047/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c7c1b2c93b6b3f81f58b4d4f3abe5047/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c7c1b2c93b6b3f81f58b4d4f3abe5047lepsky" style="display:none;border:1px dotted grey;">
The NTCIR1 TMREC group called for participation of the term recognition task which is a part of NTCIR1 held in 1999. As an activity of TMREC, they have provided us with the test collection of the term recognition task. The goal of this task is to automatically recognize and extract terms from the text corpus which consists of 1,870 abstracts gathered from the NACSIS Academic Conference Database. This article describes the term extraction method we have proposed to extract terms consisting of simple and compound nouns and the experimental evaluation of the proposed method with this NTCIR TMREC test collection. The basic idea of scoring a simple noun N of our term extraction method is to count how many nouns are conjoined with N to make compound nouns. Then we extend this score to measure the score of compound nouns because most of technical terms are compound nouns. Our method has a parameter to tune the degree of preference either for longer compound nouns or for shorter compound nouns. As for term candidates, in addition to noun sequences, we may add variations such as patterns of 'A no B' that roughly means 'B of A' or 'A's B' and/or 'A na B' where 'A na' is an adjective. Experimental results of our method are promising, namely recall of 0.83, precision of 0.46 and F-value of 0.59 for exactly matched extracted terms when we take into account top scoring 16,000 extracted terms.
</div>
<div style="position:relative">						
	<div id="bib_c7c1b2c93b6b3f81f58b4d4f3abe5047lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Experimental evaluation of ranking and selection methods in term extraction</b>.<br/>
In: 
D. Bourigault and C. Jacquemin, editors, 
<i>Recent advances in computational terminology</i>, pages 303-325.
Benjamin/Cummings Publ., Amsterdam, 2001.

<br/>
Hiroshi Nakagawa.
<br/>

<a href="http://www.r.dl.itc.u-tokyo.ac.jp/~nakagawa/academic-res/NAK.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ec8ec2a2ac4efe7967ce99bd36deb526'); return false;" href="https://www.bibsonomy.org/bibtex/2ec8ec2a2ac4efe7967ce99bd36deb526/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ec8ec2a2ac4efe7967ce99bd36deb526', 'https://www.bibsonomy.org/bibtex/2ec8ec2a2ac4efe7967ce99bd36deb526/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ec8ec2a2ac4efe7967ce99bd36deb526/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ec8ec2a2ac4efe7967ce99bd36deb526lepsky" style="display:none;border:1px dotted grey;">
An automatic term extraction system consists of a term candidate extraction subsystem, a ranking subsystem and a selection subsystem. In this paper, we experimentally evaluate two ranking methods and two selection methods. As for ranking, a dichotomy of unithood and termhood is a key notion. We evaluate these two notions experimentally by comparing Imp based ranking method that is based directly on termhood and C-value based method that is indirectly based on both termhood and unithood. As for selection, we compare the simple threshold method with the window method that we propose. We did the experimental evaluation with several Japanese technical manuals. The result does not show much difference in recall and precision. The small difference between the extracted terms by these two ranking methods depends upon their ranking mechanism per se.
</div>
<div style="position:relative">						
	<div id="bib_ec8ec2a2ac4efe7967ce99bd36deb526lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A statistical corpus-based term extractor</b>. <br/>
In: E. Stroulia and S. Matwin, editors, , volume 2056, pages 36-46.
Springer, 2001.

<br/>
Patrick Pantel and Dekang Lin.
<br/>

<a href="http://dblp.uni-trier.de/db/conf/ai/ai2001.html#PantelL01">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'c4543e92693c3d0967adb23d8a73037a', 'https://www.bibsonomy.org/bibtex/2c4543e92693c3d0967adb23d8a73037a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c4543e92693c3d0967adb23d8a73037a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c4543e92693c3d0967adb23d8a73037alepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_c4543e92693c3d0967adb23d8a73037alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2000" class="bibsonomy_quicknav_group"><a name="2000">2000</a></h3>
<div style="margin-bottom:1em">
<b>Combining linguistics with statistics for multiword term extraction : a fruitful association?</b>. <br/>
, 2000.

<br/>
G Dias, S Guilloré, JC Bassano and Lopes.
<br/>
<a href="http://133.23.229.11/~ysuzuki/Proceedingsall/RIAO2000/Friday/122DP2.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f0e77b615346e8b5edfb40dfe45db33a'); return false;" href="https://www.bibsonomy.org/bibtex/2f0e77b615346e8b5edfb40dfe45db33a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f0e77b615346e8b5edfb40dfe45db33a', 'https://www.bibsonomy.org/bibtex/2f0e77b615346e8b5edfb40dfe45db33a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f0e77b615346e8b5edfb40dfe45db33a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f0e77b615346e8b5edfb40dfe45db33alepsky" style="display:none;border:1px dotted grey;">
The acquisition of multiword terms from large text collections is a fundamental issue in the context of Information Retrieval. Indeed, their identification leads to improvements in the indexing process and allows guiding the user in his search for information. In this paper, we present an original methodology that allows extracting multiword terms by either (1) exclusively considering statistical word regularities or by (2) combining word statistics with endogenously acquired linguistic information. For that purpose, we conjugate a new association measure called the Mutual Expectation with a new acquisition process called the LocalMaxs. On one hand, the Mutual Expectation, based on the concept of Normalised Expectation, evaluates the degree of cohesiveness that links together all the textual units contained in an n-gram (i.e. "n, n 2). On the other hand, the LocalMaxs retrieves the candidate terms from the set of all the valued n-grams by evidencing local maxima of association measure...
</div>
<div style="position:relative">						
	<div id="bib_f0e77b615346e8b5edfb40dfe45db33alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1998" class="bibsonomy_quicknav_group"><a name="1998">1998</a></h3>
<div style="margin-bottom:1em">
<b>A domain-specific terminology-extraction system</b>. <br/>
<i>Terminology</i>, 5(2):183-201, 1998.

<br/>
Maria Pazienza.
<br/>
<a href="http://dx.doi.org/10.1075/term.5.2.07paz">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4fb300905e73fcec0bbc9312f9b65754'); return false;" href="https://www.bibsonomy.org/bibtex/24fb300905e73fcec0bbc9312f9b65754/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4fb300905e73fcec0bbc9312f9b65754', 'https://www.bibsonomy.org/bibtex/24fb300905e73fcec0bbc9312f9b65754/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24fb300905e73fcec0bbc9312f9b65754/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4fb300905e73fcec0bbc9312f9b65754lepsky" style="display:none;border:1px dotted grey;">
Recently, new technologies have combined to produce a sharp increase in the availability of on-line texts and their access involves end-users with different skills. The demand for tools for information retrieval, extraction, organization and integration is becoming more and more pressing to filter relevance and sort the large number of retrieved documents. It is not the amount of information that gives the value but the access at the right time and in the most suitable form to an acceptable amount of relevant documents. textlessbr / textgreaterMoreover the increasing availability of new information-transmission technologies demands more personal information filtering. Users are no longer interested in standard summaries and choices; they need (and most of the current user interests are on) filters that they can (directly) specify to fit their preferences and requirements. One such approach could only work on information sources dynamically adaptable to changing application needs. As a consequence the interaction between application domains and information sources needs to be stressed. textlessbr / textgreaterNatural Language Processing (NLP) tools may be helpful in identifying relevant aspects of documents providing not only the identification of morphologic or structural properties of the texts but also stressing what are salient passages in them. The identification of subsentences capturing relevant terms may be a first step to be acquainted with specific topics of a document. The availability of a system able to capture such terms, in different languages and application domains, could optimize users ' requirements.
</div>
<div style="position:relative">						
	<div id="bib_4fb300905e73fcec0bbc9312f9b65754lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1996" class="bibsonomy_quicknav_group"><a name="1996">1996</a></h3>
<div style="margin-bottom:1em">
<b>LEXTER : a natural language processing tool for terminology extraction</b>.<br/>
In: 

<i>EURALEX '96: proceedings I-II; papers to the seventh EURALEX International Congress on Lexicography in Göteborg, Sweden</i>, pages 771-779.
Univ., Department of Swedish, Göteborg, 1996.

<br/>
Didier Bourigault, Isabelle Gonzalez-Mullier and Cécile Gros.
<br/>

<a href="http://www.euralex.org/elx_proceedings/Euralex1996_2/040_Didier%20Bourigault,%20Isabelle%20Gonzalez-Mullier%20&amp;%20Cecile%20Gros%20-LEXTER,%20a%20Natural%20Language%20Processing.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b133574d28607797ef96d6b3fba8dcd0'); return false;" href="https://www.bibsonomy.org/bibtex/2b133574d28607797ef96d6b3fba8dcd0/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b133574d28607797ef96d6b3fba8dcd0', 'https://www.bibsonomy.org/bibtex/2b133574d28607797ef96d6b3fba8dcd0/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b133574d28607797ef96d6b3fba8dcd0/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b133574d28607797ef96d6b3fba8dcd0lepsky" style="display:none;border:1px dotted grey;">
LEXTER is a terminology extraction software. It performs a morpho-syntactical analysis of a corpus of French texts on any technical domain and yields a gram­ matical network of noun phrases which are likely to be terminological units. This network of candidate terms, together with the corpus it has been extracted from, is then passed on to an expert for a validation by the means of a terminological hypertext web. The basic principle of LEXTER is that of splitting by locating terminological noun phrases boundaries. Non supervised corpus-based learning procedures allow the system to acquire lexico-syntactical information and to solve the problem of adjectives and prepositional phrases attachment. LEXTER is used in several Electronic Document Management projects to build different kinds of terminological products.
</div>
<div style="position:relative">						
	<div id="bib_b133574d28607797ef96d6b3fba8dcd0lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Methods of automatic term recognition : a review</b>. <br/>
<i>Terminology</i>, 3(2):259-289, 1996.

<br/>
Kyo Kageura and Bin Umino.
<br/>
<a href="http://dx.doi.org/10.1075/term.3.2.03kag">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c1148afdb8007d3fa105f4b8a1676928'); return false;" href="https://www.bibsonomy.org/bibtex/2c1148afdb8007d3fa105f4b8a1676928/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c1148afdb8007d3fa105f4b8a1676928', 'https://www.bibsonomy.org/bibtex/2c1148afdb8007d3fa105f4b8a1676928/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c1148afdb8007d3fa105f4b8a1676928/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c1148afdb8007d3fa105f4b8a1676928lepsky" style="display:none;border:1px dotted grey;">
Following the growing interest in 'corpus-based' approaches to computational linguistics, a number of studies have recently appeared on the topic of automatic term recognition or extraction. Because a successful term-recognition method has to be based on proper insights into the nature of terms, studies of automatic term recognition not only contribute to the applications of computational linguistics but also to the theoretical foundation of terminology. Many studies on automatic term recognition treat interesting aspects of terms, but most of them are not well founded and described. This paper tries to give an overview of the principles and methods of automatic term recognition. For that purpose, two major trends are examined, i.e., studies in automatic recognition of significant elements for indexing mainly carried out in information-retrieval circles and current research in automatic term recognition in the field of computational linguistics.
</div>
<div style="position:relative">						
	<div id="bib_c1148afdb8007d3fa105f4b8a1676928lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Information calculus for information retrieval</b>. <br/>
<i>Journal of the American Society for Information Science</i>, 47(5):385-398, 1996.

<br/>
Cj Van Rijsbergen and M Lalmas.
<br/>


<a onclick="toggleBibtex('lepsky', '86dfee199e00e63757b2505b55f8bb6a', 'https://www.bibsonomy.org/bibtex/286dfee199e00e63757b2505b55f8bb6a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/286dfee199e00e63757b2505b55f8bb6a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_86dfee199e00e63757b2505b55f8bb6alepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_86dfee199e00e63757b2505b55f8bb6alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1995" class="bibsonomy_quicknav_group"><a name="1995">1995</a></h3>
<div style="margin-bottom:1em">
<b>Technical terminology : some linguistic properties and an algorithm for identification in text.</b>. <br/>
<i>Natural Language Engineering</i>, 1(1):9-27, 1995.

<br/>
John Justeson and Slava Katz.
<br/>
<a href="http://dblp.uni-trier.de/db/journals/nle/nle1.html#JustesonK95">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', '218f54c376e84b0ceece8778f6f0c908', 'https://www.bibsonomy.org/bibtex/2218f54c376e84b0ceece8778f6f0c908/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2218f54c376e84b0ceece8778f6f0c908/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_218f54c376e84b0ceece8778f6f0c908lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_218f54c376e84b0ceece8778f6f0c908lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1994" class="bibsonomy_quicknav_group"><a name="1994">1994</a></h3>
<div style="margin-bottom:1em">
<b>Towards automatic extraction of monolingual and bilingual terminology</b>. <br/>
In: , pages 515-521.
Association for Computational Linguistics, 1994.

<br/>
Béatrice Daille, Έric Gaussier and Jean-Marc Langé.
<br/>

<a href="http://dx.doi.org/10.3115/991886.991975">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '14456eb1915cbc15868f486a77291a43'); return false;" href="https://www.bibsonomy.org/bibtex/214456eb1915cbc15868f486a77291a43/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '14456eb1915cbc15868f486a77291a43', 'https://www.bibsonomy.org/bibtex/214456eb1915cbc15868f486a77291a43/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/214456eb1915cbc15868f486a77291a43/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_14456eb1915cbc15868f486a77291a43lepsky" style="display:none;border:1px dotted grey;">
In this paper, we make use of linguistic knowledge to identify certain noun phrases, both in English and French, which are likely to be terms. We then test and compare different statistical scores to select the "good" ones among the candidate terms, and finally propose a statistical method to build correspondences of multi-words units across languages.
</div>
<div style="position:relative">						
	<div id="bib_14456eb1915cbc15868f486a77291a43lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1992" class="bibsonomy_quicknav_group"><a name="1992">1992</a></h3>
<div style="margin-bottom:1em">
<b>Surface grammatical analysis for the extraction of terminological noun phrases</b>. <br/>
In: <i>Proceedings of the 14th Conference on Computational Linguistics - Volume 3</i>, series COLING '92, pages 977-981.
Association for Computational Linguistics, Stroudsburg, PA, 1992.

<br/>
Didier Bourigault.
<br/>

<a href="http://dx.doi.org/10.3115/992383.992415">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '57e4b0f4ca8f7ff8724a92a793bbcebc'); return false;" href="https://www.bibsonomy.org/bibtex/257e4b0f4ca8f7ff8724a92a793bbcebc/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '57e4b0f4ca8f7ff8724a92a793bbcebc', 'https://www.bibsonomy.org/bibtex/257e4b0f4ca8f7ff8724a92a793bbcebc/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/257e4b0f4ca8f7ff8724a92a793bbcebc/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_57e4b0f4ca8f7ff8724a92a793bbcebclepsky" style="display:none;border:1px dotted grey;">
LEXTER is a software package for extracting terminology. A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated. To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages: analysis, parsing. In the first stage, LEXTER uses a base of rules designed to indentify frontier markers in view to analysing the texts and extracting maximal-length noun phrases. In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units. In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach).
</div>
<div style="position:relative">						
	<div id="bib_57e4b0f4ca8f7ff8724a92a793bbcebclepsky" style="display:inline;position:absolute;"></div>
</div></div>
<!-- 
	This software is distributed under a Creative Commons Attribution 3.0 License
	http://creativecommons.org/licenses/by/3.0/

	*Attribution*
	JavaScript by Mark Schenk, Dominik Benz and Michael Domhardt
	JabRef export filter and css by Michael Domhardt http://mensch-maschine-systemtechnik.de/
	BibSonomy and Typo3 integration by Dominik Benz
	Content by BibSonomy - Lesezeichen und Referenzen teilen - in blau! http://bibsonomy.org/
-->

<script type="text/javascript">
<!--
function toggleAbstract(user,hash) {
	var abs = document.getElementById('abs_'+hash+user);	
	if (abs) {
		if(abs.id.indexOf('abs_') != -1) {
			abs.style.display = ( abs.style.display == 'none' ? '' : 'none' );
		}
	} 
	return;
}

function toggleBibtex(user,hash,biburl) {
    var f = document.getElementById('bib_' + hash + user + '_src');
	if (undefined != f) {
		f.parentNode.removeChild(f);
		return;
	}
	var el = document.getElementById('bib_' + hash + user);
    iframe = document.createElement("iframe");
    iframe.setAttribute("src", biburl);
	iframe.setAttribute("id", 'bib_' + hash + user + '_src');
    iframe.style.width = 500+"px";
    iframe.style.height = 200+"px";
	iframe.style.background = "#eee";
    el.appendChild(iframe);
	return;
}
-->
</script>
