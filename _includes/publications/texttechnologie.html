<h3 id="bib:year-2021" class="bibsonomy_quicknav_group"><a name="2021">2021</a></h3>
<div style="margin-bottom:1em">
<b>Effects of pre- and post-processing on type-based embeddings in lexical semantic change detection</b>. <br/>
<i>arXiv:2101.09368 [cs]</i>, 2021.
arXiv: 2101.09368
<br/>
Jens Kaiser, Sinan Kurtyigit, Serge Kotchourko and Dominik Schlechtweg.
<br/>
<a href="http://arxiv.org/abs/2101.09368">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'dcd1a574d99b21a884b14675344df4a4'); return false;" href="https://www.bibsonomy.org/bibtex/2dcd1a574d99b21a884b14675344df4a4/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'dcd1a574d99b21a884b14675344df4a4', 'https://www.bibsonomy.org/bibtex/2dcd1a574d99b21a884b14675344df4a4/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2dcd1a574d99b21a884b14675344df4a4/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_dcd1a574d99b21a884b14675344df4a4lepsky" style="display:none;border:1px dotted grey;">
Lexical semantic change detection is a new and innovative research field. The optimal fine-tuning of models including pre- and post-processing is largely unclear. We optimize existing models by (i) pre-training on large corpora and refining on diachronic target corpora tackling the notorious small data problem, and (ii) applying post-processing transformations that have been shown to improve performance on synchronic tasks. Our results provide a guide for the application and optimization of lexical semantic change detection models across various learning scenarios.
</div>
<div style="position:relative">						
	<div id="bib_dcd1a574d99b21a884b14675344df4a4lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>RuThes Thesaurus for Natural Language Processing</b>.<br/>
In: 
D. Gritsenko, M. Wijermars and M. Kopotev, editors, 
<i>The Palgrave Handbook of Digital Russia Studies</i>, pages 319-334.
Springer International Publishing, Cham, 2021.

<br/>
Natalia Loukachevitch and Boris Dobrov.
<br/>

<a href="https://doi.org/10.1007/978-3-030-42855-6_18">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3a3d222f9a5f4e30cadbc686f60146e7'); return false;" href="https://www.bibsonomy.org/bibtex/23a3d222f9a5f4e30cadbc686f60146e7/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3a3d222f9a5f4e30cadbc686f60146e7', 'https://www.bibsonomy.org/bibtex/23a3d222f9a5f4e30cadbc686f60146e7/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23a3d222f9a5f4e30cadbc686f60146e7/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3a3d222f9a5f4e30cadbc686f60146e7lepsky" style="display:none;border:1px dotted grey;">
This chapter describes the Russian RuThes thesaurus created as a linguistic and terminological resource for automatic document processing. Its structure utilizes two popular paradigms for computer thesauri: concept-based units, a small set of relation types, rules for including multiword expression as in information retrieval thesauri; and language-motivated units, detailed sets of synonyms, description of ambiguous words as in WordNet-like thesauri. The development of the RuThes thesaurus is supported for many years: new concepts, new senses, and multiword expressions found in contemporary texts are introduced regularly. The chapter shows some examples of representing newly appeared concepts related to important internal and international events.
</div>
<div style="position:relative">						
	<div id="bib_3a3d222f9a5f4e30cadbc686f60146e7lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A primer on word embedding</b>. <br/>
In: I. Jeena Jacob, S. Kolandapalayam Shanmugam, S. Piramuthu and P. Falkowski-Gilski, editors, <i>Data Intelligence and Cognitive Informatics</i>, series Algorithms for Intelligent Systems, pages 525-541.
Springer, Singapore, 2021.

<br/>
Satvika, Vikas Thada and Jaswinder Singh.
<br/>


<a onclick="toggleAbstract('lepsky', '22e658aa05b4784c411cbf58fb02ed19'); return false;" href="https://www.bibsonomy.org/bibtex/222e658aa05b4784c411cbf58fb02ed19/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '22e658aa05b4784c411cbf58fb02ed19', 'https://www.bibsonomy.org/bibtex/222e658aa05b4784c411cbf58fb02ed19/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/222e658aa05b4784c411cbf58fb02ed19/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_22e658aa05b4784c411cbf58fb02ed19lepsky" style="display:none;border:1px dotted grey;">
The current research on the topic of machine learning and especially the domain of natural language processing has gained much popularity in the modern era. One such framework for attaining NLP tasks is word embedding, which represents data as vectors, i.e., real numbers rather than words of natural language because neural networks do not understand them naturally. Word embeddings try to capture both syntactic and semantic information of words and capture relationships according to context and morphology. This paper reviews each word embedding technique available in the contemporary world ranging from traditional embeddings based on the frequency of terms to pre-trained embeddings like prediction-based embeddings. The goal of this paper is to present the myriad methods available for word embedding, classify their working patterns, also identify their pros and cons for working on text classification and detect their hegemony over the traditional methods of NLP.
</div>
<div style="position:relative">						
	<div id="bib_22e658aa05b4784c411cbf58fb02ed19lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Knowledge modeling : a survey of processes and techniques</b>. <br/>
<i>International Journal of Intelligent Systems</i>, n/a(n/a), 2021.
eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22357
<br/>
Wei Yun, Xuan Zhang, Zhudong Li, Hui Liu and Mengting Han.
<br/>
<a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22357">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ab574a17e3130deae6a5886272099eff'); return false;" href="https://www.bibsonomy.org/bibtex/2ab574a17e3130deae6a5886272099eff/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ab574a17e3130deae6a5886272099eff', 'https://www.bibsonomy.org/bibtex/2ab574a17e3130deae6a5886272099eff/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ab574a17e3130deae6a5886272099eff/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ab574a17e3130deae6a5886272099efflepsky" style="display:none;border:1px dotted grey;">
Knowledge modeling is an important step in building knowledge-based applications. Understanding the processes of knowledge modeling and the techniques involved can help developers to grasp the knowledge modeling task as a whole and improve the efficiency of execution and management of modeling tasks. However, previous reviews on knowledge modeling mainly focus on ontology-based knowledge modeling. At present, there is no research work to summarize nonontology knowledge modeling methods, nor to systematically summarize the processes and techniques of knowledge modeling. In this paper, the processes, techniques, and characteristics of knowledge modeling methods based on ontology and nonontology are surveyed. Three research questions related to knowledge modeling are proposed. (1) What methods can be used for knowledge modeling? (2) What processes are involved in knowledge modeling? (3) What techniques are used in the processes of knowledge modeling? By answering these questions, the results of the survey help developers choose appropriate knowledge modeling methods in their work and complete modeling tasks effectively. Meanwhile, it is also conducive to the research work of improving knowledge modeling methods in the future.
</div>
<div style="position:relative">						
	<div id="bib_ab574a17e3130deae6a5886272099efflepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2020" class="bibsonomy_quicknav_group"><a name="2020">2020</a></h3>
<div style="margin-bottom:1em">
<b>Man-machine knowledge mediation : overview of deep learning methods for natural language processing</b>. <br/>
In: T. Antipova and Á. Rocha, editors, <i>Digital Science 2019</i>, series Advances in Intelligent Systems and Computing, pages 44-52.
Springer International Publishing, Cham, 2020.

<br/>
Ekaterina Isaeva and Vadim Bakhtin.
<br/>


<a onclick="toggleAbstract('lepsky', 'cdde14f7e0d916af580a6a0a8678233b'); return false;" href="https://www.bibsonomy.org/bibtex/2cdde14f7e0d916af580a6a0a8678233b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cdde14f7e0d916af580a6a0a8678233b', 'https://www.bibsonomy.org/bibtex/2cdde14f7e0d916af580a6a0a8678233b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cdde14f7e0d916af580a6a0a8678233b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cdde14f7e0d916af580a6a0a8678233blepsky" style="display:none;border:1px dotted grey;">
In this paper we suggest that efficient processing of natural language should be considered as a complex procedure, which comprises such stages as knowledge encoding by means of natural language units, its digitalization, and machine processing, which make up man – machine knowledge mediation. For this reason, methods chosen for natural language processing should be efficient for both knowledge encoding and knowledge retrieval, as well as for working with textual data of different length, since knowledge can be compressed either to words or to larger syntactic units.The paper provides an overview of modern trends in Natural Language Processing, namely methods for deep learning. We have studied them as potential add-ons to the TSBuilder program based on the Rosenblatt’s Perceptron initiated for automated term system building. Such methods as word embedding, convolutional, recurrent, and recursive neural networks are viewed through the prism of their application for the refinement of term extraction and categorization, as well as subsequent neural network training. Statistics on these methods’ representation in scientific databases is also provided.
</div>
<div style="position:relative">						
	<div id="bib_cdde14f7e0d916af580a6a0a8678233blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Natural language processing and text mining</b>.<br/>
In: 

<i>Concepts and Methods for a Librarian of the Web</i>, pages 35-52.
Springer International Publishing, Cham, 2020.

<br/>
Mario Kubek.
<br/>

<a href="https://doi.org/10.1007/978-3-030-23136-1_4">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '485de24494efc6f8cd293b0847644791'); return false;" href="https://www.bibsonomy.org/bibtex/2485de24494efc6f8cd293b0847644791/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '485de24494efc6f8cd293b0847644791', 'https://www.bibsonomy.org/bibtex/2485de24494efc6f8cd293b0847644791/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2485de24494efc6f8cd293b0847644791/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_485de24494efc6f8cd293b0847644791lepsky" style="display:none;border:1px dotted grey;">
Web search engines of any kind have to mainly deal with natural language text to carry out their tasks. Therefore, this chapter is dedicated to basic and advanced methods for state-of-the-art natural language processing and text mining. Here, the focus is set on graph-based methods approaches to determine characteristic terms or words in texts and to measure their semantic relatedness. Furthermore, algorithms for the clustering of words and texts are discussed.
</div>
<div style="position:relative">						
	<div id="bib_485de24494efc6f8cd293b0847644791lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Pretrained transformers for text ranking : BERT and beyond</b>. <br/>
<i>arXiv:2010.06467 [cs]</i>, 2020.
arXiv: 2010.06467
<br/>
Jimmy Lin, Rodrigo Nogueira and Andrew Yates.
<br/>
<a href="http://arxiv.org/abs/2010.06467">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c3d6d77b618e3011fbaf9d6c15121b72'); return false;" href="https://www.bibsonomy.org/bibtex/2c3d6d77b618e3011fbaf9d6c15121b72/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c3d6d77b618e3011fbaf9d6c15121b72', 'https://www.bibsonomy.org/bibtex/2c3d6d77b618e3011fbaf9d6c15121b72/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c3d6d77b618e3011fbaf9d6c15121b72/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c3d6d77b618e3011fbaf9d6c15121b72lepsky" style="display:none;border:1px dotted grey;">
The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that attempt to perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond the typical sentence-by-sentence processing approaches used in NLP, and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.
</div>
<div style="position:relative">						
	<div id="bib_c3d6d77b618e3011fbaf9d6c15121b72lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Contextual word representations : putting words into computers</b>. <br/>
<i>Communications of the ACM</i>, 63(6):66-74, 2020.

<br/>
Noah A. Smith.
<br/>
<a href="https://doi.org/10.1145/3347145">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b52559e59816cbb074b1e608ebd3cf5f'); return false;" href="https://www.bibsonomy.org/bibtex/2b52559e59816cbb074b1e608ebd3cf5f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b52559e59816cbb074b1e608ebd3cf5f', 'https://www.bibsonomy.org/bibtex/2b52559e59816cbb074b1e608ebd3cf5f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b52559e59816cbb074b1e608ebd3cf5f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b52559e59816cbb074b1e608ebd3cf5flepsky" style="display:none;border:1px dotted grey;">
Advances in how programs treat natural language words have a big impact in AI.  This article aims to tell the story of how we put words into computers. It is part of the story of the field of natural language processing (NLP), a branch of artificial intelligence.a It targets a wide audience with a basic understanding of computer programming, but avoids a detailed mathematical treatment, and it does not present any algorithms. It also does not focus on any particular NLP application, such as translation, question answering, or information extraction. The ideas presented here were developed by many researchers over many decades, so the citations are not exhaustive but rather direct the reader to a handful of papers that are, in the author's view, seminal. After reading this article, you should have a general understanding of word vectors (also known as word embeddings): why they exist, what problems they solve, where they come from, how they have changed over time, and what open questions exist about them.
</div>
<div style="position:relative">						
	<div id="bib_b52559e59816cbb074b1e608ebd3cf5flepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2019" class="bibsonomy_quicknav_group"><a name="2019">2019</a></h3>
<div style="margin-bottom:1em">
<b>Word sense disambiguation focusing on POS tag disambiguation in Persian : a rule-based approach</b>. <br/>
<i>International Journal of Information Science &amp; Management</i>, 17(2):119-134, 2019.

<br/>
Elham Alayiaboozar, Amirsaeid Moloodi and Manouchehr Kouhestani.
<br/>

<a onclick="toggleAbstract('lepsky', '6caf67d9655109160ba25361b5a2aac1'); return false;" href="https://www.bibsonomy.org/bibtex/26caf67d9655109160ba25361b5a2aac1/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6caf67d9655109160ba25361b5a2aac1', 'https://www.bibsonomy.org/bibtex/26caf67d9655109160ba25361b5a2aac1/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26caf67d9655109160ba25361b5a2aac1/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6caf67d9655109160ba25361b5a2aac1lepsky" style="display:none;border:1px dotted grey;">
The present study deals with ambiguity at word level focusing on homographs. In different languages, homographs may cause ambiguity in text processing. In Persian, the number of homographs is high due to its orthographic structure as well as its complex derivational and inflectional morphology. In this study, a broad list of homographs was extracted from some Persian corpora first. The list indicates that the number of homographs in Persian corpora is high and homographs with high frequency are those that occur as a result of the identical orthographic representation of some inflectional and derivational morphemes. Based on the list, the most frequent homographs are nouns and adjectives ending in textless یtextgreater/i/. POS tag disambiguation of such homographs would make word sense disambiguation easier and lead to better text processing. In this study, a list of noun and adjective homographs ending in textless یtextgreater is extracted in order to decide their correct POS tag. The result was studied to extract context-sensitive rules for allocating the right POS tag to the homograph in syntactic structures. The accuracy of rules was checked, and the result showed that the accuracy of most rules is high which proves most rules are true.
</div>
<div style="position:relative">						
	<div id="bib_6caf67d9655109160ba25361b5a2aac1lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Reproduzierbarkeit von Studien in der Computerlinguistik</b>.
<br/>
PhD thesis, Hochschule Hannover; Fakultät III – Medien, Information und Design; Abteilung Information und Kommunikation, Hannover, 2019.

<br/>
Amelie Andresen.
<br/>
<a href="https://serwiss.bib.hs-hannover.de/frontdoor/deliver/index/docId/1517/file/Bachelorarbeit_Reproduzierbarkeit.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f18ce3f18b5dfb3cf4e008f14757413f'); return false;" href="https://www.bibsonomy.org/bibtex/2f18ce3f18b5dfb3cf4e008f14757413f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f18ce3f18b5dfb3cf4e008f14757413f', 'https://www.bibsonomy.org/bibtex/2f18ce3f18b5dfb3cf4e008f14757413f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f18ce3f18b5dfb3cf4e008f14757413f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f18ce3f18b5dfb3cf4e008f14757413flepsky" style="display:none;border:1px dotted grey;">
Die Reproduzierbarkeit von Studien ist wichtig, um ihre Ergebnisse prüfen zu können. Auch bei Forschung, die auf frühere Ergebnisse aufbaut, wird zuweilen ein Zugang zu den alten Daten oder dem Source Code benötigt. Diese Arbeit analysiert Studien aus der Computerlinguistik hinsichtlich ihrer Reproduzierbarkeit. Zunächst werden die Begrifflichkeiten zu diesem speziellen Gebiet definiert und im folgenden Schritt wird ein Datensatz erstellt, in dem ausgewählteOpen-Access-Studien aus dem Jahre 2018 auf der Basis zuvor festgelegter Kriterien bewertet werden. Diese sind unter anderem die Zugänglichkeit des benutzten Materials, der angewendeten Methoden und der Ergebnisse. Neben den Kriterien werden auch Hypothesen zu diesem Datensatz aufgestellt. Schließlich werden die Ergebnisse visualisiert und hinsichtlich besagter Hypothesen interpretiert. Basierend auf der resultierenden Auswertung sind die meisten Studien reproduzierbar. Im Ausblick werden mögliche Weiterführungen und Erweiterungen dieser Untersuchung erläutert.
</div>
<div style="position:relative">						
	<div id="bib_f18ce3f18b5dfb3cf4e008f14757413flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Mining scientific papers : NLP-enhanced bibliometrics</b>. <br/>
<i>Frontiers in Research Metrics and Analytics</i>, 4, 2019.

<br/>
Iana Atanassova, Marc Bertin and Philipp Mayr.
<br/>
<a href="https://www.frontiersin.org/articles/10.3389/frma.2019.00002/full">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '0189342794a0c2b93d1d3aacb345ff93'); return false;" href="https://www.bibsonomy.org/bibtex/20189342794a0c2b93d1d3aacb345ff93/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0189342794a0c2b93d1d3aacb345ff93', 'https://www.bibsonomy.org/bibtex/20189342794a0c2b93d1d3aacb345ff93/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20189342794a0c2b93d1d3aacb345ff93/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0189342794a0c2b93d1d3aacb345ff93lepsky" style="display:none;border:1px dotted grey;">
Editorial: Mining Scientific Papers: NLP-enhanced Bibliometrics
</div>
<div style="position:relative">						
	<div id="bib_0189342794a0c2b93d1d3aacb345ff93lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Natural language processing of German clinical colorectal cancer notes for guideline-based treatment evaluation</b>. <br/>
<i>International Journal of Medical Informatics</i>, 127:141-146, 2019.

<br/>
Matthias Becker, Stefan Kasper, Britta Böckmann, Karl-Heinz Jöckel and Isabel Virchow.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/S1386505619301145">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a3c96cf1f7199819eb35f1162f2462fd'); return false;" href="https://www.bibsonomy.org/bibtex/2a3c96cf1f7199819eb35f1162f2462fd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a3c96cf1f7199819eb35f1162f2462fd', 'https://www.bibsonomy.org/bibtex/2a3c96cf1f7199819eb35f1162f2462fd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a3c96cf1f7199819eb35f1162f2462fd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a3c96cf1f7199819eb35f1162f2462fdlepsky" style="display:none;border:1px dotted grey;">
Background Colorectal cancer is the most commonly occurring cancer in Germany, and the second and third most commonly diagnosed cancer in women and men, respectively. The therapy for this disease is based primarily on the tumor stages, which are usually documented in an unstructured form in medical information systems. In order to re-use this knowledge, the information must be extracted and annotated using the correct terminology. Methods In this study, a natural language processing pipeline is developed to identify specific guideline-based patient information and to annotate it with Unified Medical Language System concepts for manual evaluation by a physician. The gold standard for one-time evaluation is determined using the human abstraction of 2513 German clinical notes from electronic health records. Results Using this approach to process the narrative clinical notes on colorectal cancer for retrospective evaluation of the therapy recommendation, the algorithm achieves a precision value of 96.64%for tumor stage detection and 97.95%for diagnosis recognition with recall values of 94.89%and 99.54 respectively. The average precision value across all concepts relevant to treatment decisions for patients with known cancer diagnoses (11 concept groups) achieved a precision value of 82.05%with a recall value of 82.45%and an F1-score of 81.81 respectively. Conclusions The identification of guideline-based information from narrative clinical notes has the potential for implementation as clinical decision support tools.
</div>
<div style="position:relative">						
	<div id="bib_a3c96cf1f7199819eb35f1162f2462fdlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>BERT : pre-training of deep bidirectional transformers for language understanding</b>. <br/>
<i>arXiv:1810.04805 [cs]</i>, 2019.
arXiv: 1810.04805
<br/>
Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
<br/>
<a href="http://arxiv.org/abs/1810.04805">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '356e49e86f719124d471a137983893a0'); return false;" href="https://www.bibsonomy.org/bibtex/2356e49e86f719124d471a137983893a0/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '356e49e86f719124d471a137983893a0', 'https://www.bibsonomy.org/bibtex/2356e49e86f719124d471a137983893a0/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2356e49e86f719124d471a137983893a0/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_356e49e86f719124d471a137983893a0lepsky" style="display:none;border:1px dotted grey;">
We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5%(7.7%point absolute improvement), MultiNLI accuracy to 86.7%(4.6%absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
</div>
<div style="position:relative">						
	<div id="bib_356e49e86f719124d471a137983893a0lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Maschinelles Lernen und NLP : reif für die industrielle Anwendung!</b>. <br/>
<i>Information - Wissenschaft &amp; Praxis</i>, 70(2-3):134-140, 2019.

<br/>
Stefan Geißler.
<br/>
<a href="https://www.degruyter.com/view/j/iwp.2019.70.issue-2-3/iwp-2019-2007/iwp-2019-2007.xml">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a08914daaf052187b6815b53aa60ea57'); return false;" href="https://www.bibsonomy.org/bibtex/2a08914daaf052187b6815b53aa60ea57/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a08914daaf052187b6815b53aa60ea57', 'https://www.bibsonomy.org/bibtex/2a08914daaf052187b6815b53aa60ea57/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a08914daaf052187b6815b53aa60ea57/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a08914daaf052187b6815b53aa60ea57lepsky" style="display:none;border:1px dotted grey;">
Anwendungen von maschinellen Lernverfahren (ML) haben in jüngster Zeit aufsehenerregende Durchbrüche bei einer ganzen Reihe von Aufgaben in der maschinellen Sprachverarbeitung (NLP) erzielt. Der Fokus vieler Arbeiten liegt hierbei in der Entwicklung immer besserer Modelle, während der Anteil der Aufgaben in praktischen Projekten, der sich nicht mit Modellbildung, sondern mit Themen wie Datenbereitstellung sowie Evaluierung, Wartung und Deployment von Modellen beschäftigt, oftmals noch nicht ausreichend Beachtung erfährt. Im Ergebnis fehlen gerade Unternehmen, die nicht die Möglichkeit haben, eigene Plattformen für den Einsatz von ML und NLP zu entwerfen, oft geeignete Werkzeuge und Best Practices. Es ist zeichnet sich ab, dass in den kommenden Monaten eine gerade diesen praktischen Fragen zugewandte Ingenieurssicht auf ML und ihren Einsatz im Unternehmen an Bedeutung gewinnen wird.
</div>
<div style="position:relative">						
	<div id="bib_a08914daaf052187b6815b53aa60ea57lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Akku-Bohr-Hammer vs. Akku-Bohrhammer : Experiments towards the Evaluation of Compound Splitting Tools for General Language and Speciﬁc Domains</b>. <br/>
In: , pages 10.
Erlangen, 2019.

<br/>
Anna Hatty, Ulrich Heid, Anna Moskvina, Julia Bettinger and Michael Dorna.
<br/>

<a href="https://www.researchgate.net/profile/Sabine_Schulte_Im_Walde/publication/336209080_AkkuBohrHammer_vs_AkkuBohrhammer_Experiments_towards_the_Evaluation_of_Compound_Splitting_Tools_for_General_Language_and_Specific_Domains/links/5d94430fa6fdcc2554abd92a/AkkuBohrHammer-vs-AkkuBohrhammer-Experiments-towards-the-Evaluation-of-Compound-Splitting-Tools-for-General-Language-and-Specific-Domains.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a3a2afab1147099c8c679b98d657276a'); return false;" href="https://www.bibsonomy.org/bibtex/2a3a2afab1147099c8c679b98d657276a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a3a2afab1147099c8c679b98d657276a', 'https://www.bibsonomy.org/bibtex/2a3a2afab1147099c8c679b98d657276a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a3a2afab1147099c8c679b98d657276a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a3a2afab1147099c8c679b98d657276alepsky" style="display:none;border:1px dotted grey;">
We present a comparative evaluation study for splitting German compounds which belong to general language or to a speciﬁc domain. For the domain, we focus on DIY (”do-it-yourself”). The study consists of two parts: First, we evaluate three tools for compound splitting in German, one based on lexicons and corpus frequencies and two based on language-independent statistical processing. We introduce the tools, discuss the data and the construction of a gold standard, and show ﬁrst results for binary and ternary noun compounds, as well as for the handling of non-splittable items. In a second experiment, we post-train one of the splitters with text data from the DIYdomain, and evaluate the splitting performance on domain-speciﬁc compounds.
</div>
<div style="position:relative">						
	<div id="bib_a3a2afab1147099c8c679b98d657276alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Automatic detection of contradictions in texts</b>.
<br/>
PhD thesis, Universitätsbibliothek, Gießen, 2019.

<br/>
Natali Karlova-Bourbonus.
<br/>
<a href="http://geb.uni-giessen.de/geb/volltexte/2019/14447">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd209a1ccc1fa8d5f3b612466749db0cb'); return false;" href="https://www.bibsonomy.org/bibtex/2d209a1ccc1fa8d5f3b612466749db0cb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd209a1ccc1fa8d5f3b612466749db0cb', 'https://www.bibsonomy.org/bibtex/2d209a1ccc1fa8d5f3b612466749db0cb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d209a1ccc1fa8d5f3b612466749db0cb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d209a1ccc1fa8d5f3b612466749db0cblepsky" style="display:none;border:1px dotted grey;">
The main purpose of news is to inform the reader about the current political, economic, and cultural events in the world. By that, the main requirements for the process of news produc-tion is an objective, uninvolved news reporting and an accurate, i.e. correct and consistent (contradiction-free) use of facts. A violation of the latter leads to the misinformation of the reader and, if detected, to a negative impact on the credibility and trustworthiness of the newspaper.  The recognition of contradictions in a (news) text is a challenging task for a human as it presupposes concentrated reading and requires world knowledge and the ability to analyti-cally process the information obtained. Also, the age and mental capability of the reader plays an important role. Further, the task of contradiction recognition becomes even more difficult when dealing with contradictory facts occurring in texts that are separated by space and time. For this reason, the main aim of the present study was to propose a system for the automatic detection of contradictions occurring in news texts written in English.
</div>
<div style="position:relative">						
	<div id="bib_d209a1ccc1fa8d5f3b612466749db0cblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A wind of change : detecting and evaluating lexical semantic change across times and domains</b>. <br/>
<i>arXiv:1906.02979 [cs]</i>, 2019.
arXiv: 1906.02979
<br/>
Dominik Schlechtweg, Anna Hätty, Marco del Tredici and Sabine Schulte im Walde.
<br/>
<a href="http://arxiv.org/abs/1906.02979">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '42a6077a2e257bc6630015c5c2b92e08'); return false;" href="https://www.bibsonomy.org/bibtex/242a6077a2e257bc6630015c5c2b92e08/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '42a6077a2e257bc6630015c5c2b92e08', 'https://www.bibsonomy.org/bibtex/242a6077a2e257bc6630015c5c2b92e08/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/242a6077a2e257bc6630015c5c2b92e08/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_42a6077a2e257bc6630015c5c2b92e08lepsky" style="display:none;border:1px dotted grey;">
We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction.
</div>
<div style="position:relative">						
	<div id="bib_42a6077a2e257bc6630015c5c2b92e08lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Learning taxonomies of concepts and not words using contextualized word representations : a position paper</b>. <br/>
<i>arXiv:1902.02169 [cs, stat]</i>, 2019.
arXiv: 1902.02169
<br/>
Lukas Schmelzeisen and Steffen Staab.
<br/>
<a href="http://arxiv.org/abs/1902.02169">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b9552f0b56d5cafa5422e3760bfcd5cc'); return false;" href="https://www.bibsonomy.org/bibtex/2b9552f0b56d5cafa5422e3760bfcd5cc/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b9552f0b56d5cafa5422e3760bfcd5cc', 'https://www.bibsonomy.org/bibtex/2b9552f0b56d5cafa5422e3760bfcd5cc/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b9552f0b56d5cafa5422e3760bfcd5cc/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b9552f0b56d5cafa5422e3760bfcd5cclepsky" style="display:none;border:1px dotted grey;">
Taxonomies are semantic hierarchies of concepts. One limitation of current taxonomy learning systems is that they define concepts as single words. This position paper argues that contextualized word representations, which recently achieved state-of-the-art results on many competitive NLP tasks, are a promising method to address this limitation. We outline a novel approach for taxonomy learning that (1) defines concepts as synsets, (2) learns density-based approximations of contextualized word representations, and (3) can measure similarity and hypernymy among them.
</div>
<div style="position:relative">						
	<div id="bib_b9552f0b56d5cafa5422e3760bfcd5cclepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Then and now : 25 years of progress in natural language engineering</b>. <br/>
<i>Natural Language Engineering</i>, 25(3):405-418, 2019.

<br/>
John Tait and Yorick Wilks.
<br/>
<a href="https://www.cambridge.org/core/journals/natural-language-engineering/article/anniversary-article-then-and-now-25-years-of-progress-in-natural-language-engineering/45C185853DD9C6178BE473F73B765DE0">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '5cdc75c4100a59ed4d8ad2548a9afeb8'); return false;" href="https://www.bibsonomy.org/bibtex/25cdc75c4100a59ed4d8ad2548a9afeb8/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5cdc75c4100a59ed4d8ad2548a9afeb8', 'https://www.bibsonomy.org/bibtex/25cdc75c4100a59ed4d8ad2548a9afeb8/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25cdc75c4100a59ed4d8ad2548a9afeb8/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5cdc75c4100a59ed4d8ad2548a9afeb8lepsky" style="display:none;border:1px dotted grey;">
The paper reviews the state of the art of natural language engineering (NLE) around 1995, when this journal first appeared, and makes a critical comparison with the current state of the art in 2018, as we prepare the 25th Volume. Specifically the then state of the art in parsing, information extraction, chatbots, and dialogue systems, speech processing and machine translation are briefly reviewed. The emergence in the 1980s and 1990s of machine learning (ML) and statistical methods (SM) is noted. Important trends and areas of progress in the subsequent years are identified. In particular, the move to the use of n-grams or skip grams and/or chunking with part of speech tagging and away from whole sentence parsing is noted, as is the increasing dominance of SM and ML. Some outstanding issues which merit further research are briefly pointed out, including metaphor processing and the ethical implications of NLE.
</div>
<div style="position:relative">						
	<div id="bib_5cdc75c4100a59ed4d8ad2548a9afeb8lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Natural language processing applications in library and information science</b>. <br/>
<i>Online Information Review</i>, 2019.

<br/>
Zehra Taskin and Umut Al.
<br/>
<a href="https://www.emerald.com/insight/content/doi/10.1108/OIR-07-2018-0217/full/html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '1ea85d691c392ccf71f750ab50cacd4e'); return false;" href="https://www.bibsonomy.org/bibtex/21ea85d691c392ccf71f750ab50cacd4e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1ea85d691c392ccf71f750ab50cacd4e', 'https://www.bibsonomy.org/bibtex/21ea85d691c392ccf71f750ab50cacd4e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21ea85d691c392ccf71f750ab50cacd4e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1ea85d691c392ccf71f750ab50cacd4elepsky" style="display:none;border:1px dotted grey;">
With the recent developments in information technologies, natural language processing (NLP) practices have made tasks in many areas easier and more practical. Nowadays, especially when big data are used in most research, NLP provides fast and easy methods for processing these data. The purpose of this paper is to identify subfields of library and information science (LIS) where NLP can be used and to provide a guide based on bibliometrics and social network analyses for researchers who intend to study this subject.,Within the scope of this study, 6,607 publications, including NLP methods published in the field of LIS, are examined and visualized by social network analysis methods.,After evaluating the obtained results, the subject categories of publications, frequently used keywords in these publications and the relationships between these words are revealed. Finally, the core journals and articles are classified thematically for researchers working in the field of LIS and planning to apply NLP in their research.,The results of this paper draw a general framework for LIS field and guides researchers on new techniques that may be useful in the field.
</div>
<div style="position:relative">						
	<div id="bib_1ea85d691c392ccf71f750ab50cacd4elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>TopicZoom GmbH – Semantische Suche – Semantic Technology</b>.<br/>
2019. 
<br/>TopicZoom.
<br/>
<a href="http://www.topiczoom.de/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a623d4f096d3b9114fb9417401737366'); return false;" href="https://www.bibsonomy.org/bibtex/2a623d4f096d3b9114fb9417401737366/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a623d4f096d3b9114fb9417401737366', 'https://www.bibsonomy.org/bibtex/2a623d4f096d3b9114fb9417401737366/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a623d4f096d3b9114fb9417401737366/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a623d4f096d3b9114fb9417401737366lepsky" style="display:none;border:1px dotted grey;">
TopicZoom bietet als spezialisiertes Unternehmen im Bereich Text- und Dokumentenanalyse schlagkräftige Antworten unter Einsatz modernster Methoden der Artificial Intelligence und umfangreicher Ontologien an. Unsere Lösungen sind aufgrund ihrer Effzienz auch für extrem umfangreiche Textkollektionen (Big Data) sofort einsetzbar.
</div>
<div style="position:relative">						
	<div id="bib_a623d4f096d3b9114fb9417401737366lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Übersetzer überflüssig?</b>.<br/>
2019. 
<br/>Oliver Voss.
<br/>
<a href="https://www.tagesspiegel.de/wirtschaft/sprachsoftware-deepl-und-acrolinx-uebersetzer-ueberfluessig/23884348.html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f12f24b8a87b5fe92d83e413defeb167'); return false;" href="https://www.bibsonomy.org/bibtex/2f12f24b8a87b5fe92d83e413defeb167/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f12f24b8a87b5fe92d83e413defeb167', 'https://www.bibsonomy.org/bibtex/2f12f24b8a87b5fe92d83e413defeb167/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f12f24b8a87b5fe92d83e413defeb167/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f12f24b8a87b5fe92d83e413defeb167lepsky" style="display:none;border:1px dotted grey;">
Deutsche Sprachsoftware ist besser als Google. Sogar professionelle Übersetzer diskutieren schon, ob sie überflüssig werden.
</div>
<div style="position:relative">						
	<div id="bib_f12f24b8a87b5fe92d83e413defeb167lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2018" class="bibsonomy_quicknav_group"><a name="2018">2018</a></h3>
<div style="margin-bottom:1em"><b>Transkribus entziffert Uromas Handschrift</b>.<br/>
2018. 
<br/>Daniel Berger.
<br/>
<a href="https://www.heise.de/newsticker/meldung/Transkribus-entziffert-Uromas-Handschrift-4056211.html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4c1038fc74531b836ce8f44163883bb5'); return false;" href="https://www.bibsonomy.org/bibtex/24c1038fc74531b836ce8f44163883bb5/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4c1038fc74531b836ce8f44163883bb5', 'https://www.bibsonomy.org/bibtex/24c1038fc74531b836ce8f44163883bb5/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24c1038fc74531b836ce8f44163883bb5/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4c1038fc74531b836ce8f44163883bb5lepsky" style="display:none;border:1px dotted grey;">
Transkribus digitalisiert historische Dokumente, die sich nur noch schwer lesen lassen. Je mehr Text die Software auswertet, desto besser das Ergebnis. Jeder kann das Tool nutzen – auch Laien, die Uromas Briefe entziffern wollen.
</div>
<div style="position:relative">						
	<div id="bib_4c1038fc74531b836ce8f44163883bb5lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>LinguaKit : a Big Data-based multilingual tool for linguistic analysis and information extraction</b>.<br/>
2018. Citation Key: gamallolinguakit.
<br/>Pablo Gamallo, Marcos Garcia, César Pineiro, Rodrigo Martınez-Castano and Juan C Pichel.
<br/>
<a href="http://persoal.citius.usc.es/jcpichel/docs/2018_ANLP_PGamallo.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'cfc1206cf31eef91ff427341d88fe41e'); return false;" href="https://www.bibsonomy.org/bibtex/2cfc1206cf31eef91ff427341d88fe41e/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'cfc1206cf31eef91ff427341d88fe41e', 'https://www.bibsonomy.org/bibtex/2cfc1206cf31eef91ff427341d88fe41e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2cfc1206cf31eef91ff427341d88fe41e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_cfc1206cf31eef91ff427341d88fe41elepsky" style="display:none;border:1px dotted grey;">
This paper presents LinguaKit, a multilingual suite of tools for analysis, extraction, annotation and linguistic correction, as well as its integration into a Big Data infrastructure. LinguaKit allows the user to perform different tasks such as PoS-tagging, syntactic parsing, coreference resolution (among others), including applications for relation extraction, sentimentanalysis, summarization, extraction of multiword expr essions, or entity linking to DBpedia. Most modules work in four languages: Portuguese, Spanish, English, and Galician. The system is programmed in Perl and is freely available under a GPLv3 license.
</div>
<div style="position:relative">						
	<div id="bib_cfc1206cf31eef91ff427341d88fe41elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Wenn Algorithmen Zeitschriften lesen</b>. <br/>
<i>o-bib. Das offene Bibliotheksjournal / Herausgeber VDB</i>, 5(4):181-192, 2018.

<br/>
Michael Gasser, Regina Wanger and Ismail Prada.
<br/>
<a href="https://www.o-bib.de/article/view/5382">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4fa1f1edc1f0895b1abb92932ab7366a'); return false;" href="https://www.bibsonomy.org/bibtex/24fa1f1edc1f0895b1abb92932ab7366a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4fa1f1edc1f0895b1abb92932ab7366a', 'https://www.bibsonomy.org/bibtex/24fa1f1edc1f0895b1abb92932ab7366a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24fa1f1edc1f0895b1abb92932ab7366a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4fa1f1edc1f0895b1abb92932ab7366alepsky" style="display:none;border:1px dotted grey;">
In Zusammenarbeit mit dem Institut für Computerlinguistik der Universität Zürich (ICL UZH) lancierte die ETH-Bibliothek Zürich ein Pilotprojekt im Bereich automatisierter Textanreicherung. Grundlage für den Piloten bildeten Volltextdateien der Schweizer Zeitschriftenplattform E-Periodica. Anhand eines ausgewählten Korpus dieser OCR-Daten wurden mit automatisierten Verfahren Tests in den Bereichen OCR-Korrektur, Erkennung von Personen-, Orts- und Ländernamen sowie Verlinkung identifizierter Personen mit der Gemeinsamen Normdatei GND durchgeführt. Insgesamt wurden sehr positive Resultate erzielt. Das verwendete System dient nun als Grundlage für den weiteren Kompetenzausbau der ETH-Bibliothek auf diesem Gebiet. Das gesamte bestehende Angebot der Plattform E-Periodica soll automatisiert angereichert und um neue Funktionalitäten erweitert werden. Dies mit dem Ziel, Forschenden einen Mehrwert bei der Informationsbeschaffung zu bieten. Im vorliegenden Beitrag werden Projektinhalt, Methodik und Resultate erläutert sowie das weitere Vorgehen skizziert.
</div>
<div style="position:relative">						
	<div id="bib_4fa1f1edc1f0895b1abb92932ab7366alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Measuring innovation in speech and language processing publications.</b>. <br/>
In: N. Calzolari, K. Choukri, C. Cieri, T. Declerck, S. Goggi, K. Hasida, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, S. Piperidis and T. Tokunaga, editors, <i>Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</i>.
European Language Resources Association (ELRA), Miyazaki, Japan, 2018.
Citation Key: MARIANI18.522
<br/>
Joseph Mariani, Gil Francopoulo and Patrick Paroubek.
<br/>

<a href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/522.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'c15484333a3b909c582b054af38fc63e', 'https://www.bibsonomy.org/bibtex/2c15484333a3b909c582b054af38fc63e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c15484333a3b909c582b054af38fc63e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c15484333a3b909c582b054af38fc63elepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_c15484333a3b909c582b054af38fc63elepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Clinical natural language processing in languages other than English : opportunities and challenges</b>. <br/>
<i>Journal of Biomedical Semantics</i>, 9:12, 2018.

<br/>
Aurélie Névéol, Hercules Dalianis, Sumithra Velupillai, Guergana Savova and Pierre Zweigenbaum.
<br/>
<a href="https://doi.org/10.1186/s13326-018-0179-8">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'aabbc3f109fd0c40f9e7e4bfa3dd17da'); return false;" href="https://www.bibsonomy.org/bibtex/2aabbc3f109fd0c40f9e7e4bfa3dd17da/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'aabbc3f109fd0c40f9e7e4bfa3dd17da', 'https://www.bibsonomy.org/bibtex/2aabbc3f109fd0c40f9e7e4bfa3dd17da/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2aabbc3f109fd0c40f9e7e4bfa3dd17da/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_aabbc3f109fd0c40f9e7e4bfa3dd17dalepsky" style="display:none;border:1px dotted grey;">
Natural language processing applied to clinical text or aimed at a clinical outcome has been thriving in recent years. This paper offers the first broad overview of clinical Natural Language Processing (NLP) for languages other than English. Recent studies are summarized to offer insights and outline opportunities in this area.
</div>
<div style="position:relative">						
	<div id="bib_aabbc3f109fd0c40f9e7e4bfa3dd17dalepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Turing tests in natural language processing and information retrieval : contributions to the theory, development and evaluation of textual information processing systems for decision support</b>.
<br/>
PhD thesis, Avignon Université; Laboratoire d’Informatique d’Avignon, Avignon, 2018.

<br/>
Eric Sanjuan.
<br/>
<a href="https://dev.termwatch.es/esj/turing_tests_esj.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', 'e8ddefe202e771809599d2f5df9e822e', 'https://www.bibsonomy.org/bibtex/2e8ddefe202e771809599d2f5df9e822e/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e8ddefe202e771809599d2f5df9e822e/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e8ddefe202e771809599d2f5df9e822elepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_e8ddefe202e771809599d2f5df9e822elepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2017" class="bibsonomy_quicknav_group"><a name="2017">2017</a></h3>
<div style="margin-bottom:1em"><b>Mozilla Common Voice : Sprachsteuerung für alle und ohne Rückgriff auf die Cloud</b>.<br/>
2017. 
<br/>Herbert Braun.
<br/>
<a href="https://www.heise.de/ho/meldung/Mozilla-Common-Voice-Sprachsteuerung-fuer-alle-und-ohne-Rueckgriff-auf-die-Cloud-3904454.html">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '5f26da3a06c6e3e2d19ff21598a23fab'); return false;" href="https://www.bibsonomy.org/bibtex/25f26da3a06c6e3e2d19ff21598a23fab/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '5f26da3a06c6e3e2d19ff21598a23fab', 'https://www.bibsonomy.org/bibtex/25f26da3a06c6e3e2d19ff21598a23fab/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/25f26da3a06c6e3e2d19ff21598a23fab/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_5f26da3a06c6e3e2d19ff21598a23fablepsky" style="display:none;border:1px dotted grey;">
Spracherkennung statt Abhörwanzen: Mozillas Sprachdatenbank Common Voice und die Speech-to-Text-Engine DeepSpeech kommen ohne Umweg über die Cloud aus.
</div>
<div style="position:relative">						
	<div id="bib_5f26da3a06c6e3e2d19ff21598a23fablepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Wie Zeitungsinhalte Forschung und Entwicklung befördern (FAZ-Archiv)</b>.<br/>
2017. 
<br/>Olivera Kipcic and Corinna Cramer.
<br/>
<a href="http://www.password-online.de/?wysija-page=1&amp;controller=email&amp;action=view&amp;email_id=294&amp;wysijap=subscriptions">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '3cfb98ba7feaf81d0d3928c7ffcafdfb'); return false;" href="https://www.bibsonomy.org/bibtex/23cfb98ba7feaf81d0d3928c7ffcafdfb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3cfb98ba7feaf81d0d3928c7ffcafdfb', 'https://www.bibsonomy.org/bibtex/23cfb98ba7feaf81d0d3928c7ffcafdfb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23cfb98ba7feaf81d0d3928c7ffcafdfb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3cfb98ba7feaf81d0d3928c7ffcafdfblepsky" style="display:none;border:1px dotted grey;">
Das F.A.Z.-Archiv ist nach innen das Informationszentrum der F.A.Z. Hier ist seine oberste Aufgabe die Informationsversorgung der Redaktionen der F.A.Z. GmbH und der Nachweis der F.A.Z. mit allen Teilen und Ausgaben. Nach außen tritt es als Vermarkter von Zeitungsdaten auf, dies sowohl für das eigene Haus wie auch für Dritte. Klarer Auftrag ist dabei die Generierung von Erlösen für die F.A.Z.-Gruppe durch Informations- und Datenbankdienste für externe Kunden.
</div>
<div style="position:relative">						
	<div id="bib_3cfb98ba7feaf81d0d3928c7ffcafdfblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Sprachautomaten</b>.<br/>
2017. 
<br/>Henning Lobin.
<br/>
<a href="https://scilogs.spektrum.de/engelbart-galaxis/sprachautomaten/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '0fd5dd78c1bac8a5b835645b55249b3f'); return false;" href="https://www.bibsonomy.org/bibtex/20fd5dd78c1bac8a5b835645b55249b3f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0fd5dd78c1bac8a5b835645b55249b3f', 'https://www.bibsonomy.org/bibtex/20fd5dd78c1bac8a5b835645b55249b3f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20fd5dd78c1bac8a5b835645b55249b3f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0fd5dd78c1bac8a5b835645b55249b3flepsky" style="display:none;border:1px dotted grey;">
Der Informatiker Wolfgang Coy hat den Computer einmal als ein Gerät beschrieben, das drei verschiedene Erscheinungsformen aufweist: als Automat, als Werkzeug und als Medium.[i] Auch bei der maschinellen Sprachverarbeitung lassen sich diese drei Entfaltungsstufen erkennen. In … Weiterlesen
</div>
<div style="position:relative">						
	<div id="bib_0fd5dd78c1bac8a5b835645b55249b3flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Formalizing natural languages with NooJ and its natural language processing applications : 11th International Conference, NooJ 2017, Kenitra and Rabat, Morocco, May 18–20, 2017, revised selected papers</b>.<br/>
2017. 
<br/>Samir Mbarki, Mohammed Mourchid and Max Silberztein.
<br/>
<a href="https://www.springerprofessional.de/formalizing-natural-languages-with-nooj-and-its-natural-language/15337346">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '2606cfbb452d8599364e9da4e6c833c6'); return false;" href="https://www.bibsonomy.org/bibtex/22606cfbb452d8599364e9da4e6c833c6/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '2606cfbb452d8599364e9da4e6c833c6', 'https://www.bibsonomy.org/bibtex/22606cfbb452d8599364e9da4e6c833c6/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22606cfbb452d8599364e9da4e6c833c6/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2606cfbb452d8599364e9da4e6c833c6lepsky" style="display:none;border:1px dotted grey;">
This book constitutes the refereed proceedings of the 11th International Conference, NooJ 2017, held in Kenitra and Rabat, Morocco, in May 2017. The 20 revised full papers presented in this volume were carefully reviewed and selected from 56 submissions. NooJ is a linguistic development environment that provides tools for linguists to construct linguistic resources that formalize a large gamut of linguistic phenomena: typography, orthography, lexicons for simple words, multiword units and discontinuous expressions, inflectional and derivational morphology, local, structural and transformational syntax, and semantics. The papers in this volume are organized in topical sections on vocabulary and morphology; syntactic analysis; natural language processing applications; NooJ’s future.
</div>
<div style="position:relative">						
	<div id="bib_2606cfbb452d8599364e9da4e6c833c6lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Arabic natural language processing : models, systems and applications</b>. <br/>
<i>Journal of King Saud University - Computer and Information Sciences</i>, 29(2):A1-A3, 2017.

<br/>
Vito Pirrelli and Arsalane Zarghili.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/S1319157817301155">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', '296acc9b1cfb0fc1ead8430d1670332a', 'https://www.bibsonomy.org/bibtex/2296acc9b1cfb0fc1ead8430d1670332a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2296acc9b1cfb0fc1ead8430d1670332a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_296acc9b1cfb0fc1ead8430d1670332alepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_296acc9b1cfb0fc1ead8430d1670332alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A language modeling approach to information retrieval</b>. <br/>
<i>SIGIR Forum</i>, 51:202-208, 2017.

<br/>
Jay M. Ponte and W. Bruce Croft.
<br/>

<a onclick="toggleAbstract('lepsky', '0ed892d97ed5f3617765696988a51111'); return false;" href="https://www.bibsonomy.org/bibtex/20ed892d97ed5f3617765696988a51111/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0ed892d97ed5f3617765696988a51111', 'https://www.bibsonomy.org/bibtex/20ed892d97ed5f3617765696988a51111/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20ed892d97ed5f3617765696988a51111/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0ed892d97ed5f3617765696988a51111lepsky" style="display:none;border:1px dotted grey;">
Models of document indexing and document retrieval have been extensively studied. The integration of these two classes of models has been the goal of several researchers but it is a very difficult problem. We argue that much of the reason for this is the lack of an adequate indexing model. This suggests that perhaps a better indexing model would help solve the problem. However, we feel that making unwarranted parametric assumptions will not lead to better retrieval performance. Furthermore, making prior assumptions about the similarity of documents is not warranted either. Instead, we propose an approach to retrieval based on probabilistic language modeling. We estimate models for each document individually. Our approach to modeling is non-parametric and integrates document indexing and document retrieval into a single model. One advantage of our approach is that collection statistics which are used heuristically in many other retrieval models are an integral part of our model. We have implemented our model and tested it empirically. Our approach significantly outperforms standard tf.idf weighting on two different collections and query sets.
</div>
<div style="position:relative">						
	<div id="bib_0ed892d97ed5f3617765696988a51111lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>IDF for word n-grams</b>. <br/>
<i>ACM Trans. Inf. Syst.</i>, 36(1):5:1-5:38, 2017.

<br/>
Masumi Shirakawa, Takahiro Hara and Shojiro Nishio.
<br/>
<a href="http://doi.acm.org/10.1145/3052775">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '344d955f8d3a09cad6819b72ea54f979'); return false;" href="https://www.bibsonomy.org/bibtex/2344d955f8d3a09cad6819b72ea54f979/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '344d955f8d3a09cad6819b72ea54f979', 'https://www.bibsonomy.org/bibtex/2344d955f8d3a09cad6819b72ea54f979/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2344d955f8d3a09cad6819b72ea54f979/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_344d955f8d3a09cad6819b72ea54f979lepsky" style="display:none;border:1px dotted grey;">
Inverse Document Frequency (IDF) is widely accepted term weighting scheme whose robustness is supported by many theoretical justifications. However, applying IDF to word N-grams (or simply N-grams) of any length without relying on heuristics has remained a challenging issue. This article describes a theoretical extension of IDF to handle N-grams. First, we elucidate the theoretical relationship between IDF and information distance, a universal metric defined by the Kolmogorov complexity. Based on our understanding of this relationship, we propose N-gram IDF, a new IDF family that gives fair weights to words and phrases of any length. Based only on the magnitude relation of N-gram IDF weights, dominant N-grams among overlapping N-grams can be determined. We also propose an efficient method to compute the N-gram IDF weights of all N-grams by leveraging the enhanced suffix array and wavelet tree. Because the exact computation of N-gram IDF provably requires significant computational cost, we modify it to a fast approximation method that can estimate weight errors analytically and maintain application-level performance. Empirical evaluations with unsupervised/supervised key term extraction and web search query segmentation with various experimental settings demonstrate the robustness and language-independent nature of the proposed N-gram IDF.
</div>
<div style="position:relative">						
	<div id="bib_344d955f8d3a09cad6819b72ea54f979lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2016" class="bibsonomy_quicknav_group"><a name="2016">2016</a></h3>
<div style="margin-bottom:1em">
<b>When is the time ripe for natural language processing for patent passage retrieval?</b>. <br/>
In: <i>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</i>, series CIKM '16, pages 1453-1462.
ACM, New York, NY, 2016.

<br/>
Linda Andersson, Mihai Lupu, João Palotti, Allan Hanbury and Andreas Rauber.
<br/>

<a href="http://doi.acm.org/10.1145/2983323.2983858">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '09f38353e154e326e55db79f528ff778'); return false;" href="https://www.bibsonomy.org/bibtex/209f38353e154e326e55db79f528ff778/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '09f38353e154e326e55db79f528ff778', 'https://www.bibsonomy.org/bibtex/209f38353e154e326e55db79f528ff778/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/209f38353e154e326e55db79f528ff778/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_09f38353e154e326e55db79f528ff778lepsky" style="display:none;border:1px dotted grey;">
Patent text is a mixture of legal terms and domain specific terms. In technical English text, a multi-word unit method is often deployed as a word formation strategy in order to expand the working vocabulary, i.e. introducing a new concept without the invention of an entirely new word. In this paper we explore query generation using natural language processing technologies in order to capture domain specific concepts represented as multi-word units. In this paper we examine a range of query generation methods using both linguistic and statistical information. We also propose a new method to identify domain specific terms from other more general phrases. We apply a machine learning approach using domain knowledge and corpus linguistic information in order to learn domain specific terms in relation to phrases' Termhood values. The experiments are conducted on the English part of the CLEF-IP 2013 test collection. The outcome of the experiments shows that the favoured method in terms of PRES and recall is when a language model is used and search terms are extracted with a part-of-speech tagger and a noun phrase chunker. With our proposed methods we improve each evaluation metric significantly compared to the existing state-of-the-art for the CLEP-IP 2013 test collection: for PRES@100 by 26%(0.544 from 0.433), for recall@100 by 17%(0.631 from 0.540) and on document MAP by 57%(0.300 from 0.191).
</div>
<div style="position:relative">						
	<div id="bib_09f38353e154e326e55db79f528ff778lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Hybrid approach for short form detection and expansion to long forms</b>.<br/>
2016. 
<br/>Md Faisal M. Chowdhury.
<br/>
<a href="https://patents.google.com/patent/US20170371862A1/en">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '24251763aa0b53bf2a8abae3a156d921'); return false;" href="https://www.bibsonomy.org/bibtex/224251763aa0b53bf2a8abae3a156d921/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '24251763aa0b53bf2a8abae3a156d921', 'https://www.bibsonomy.org/bibtex/224251763aa0b53bf2a8abae3a156d921/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/224251763aa0b53bf2a8abae3a156d921/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_24251763aa0b53bf2a8abae3a156d921lepsky" style="display:none;border:1px dotted grey;">
Embodiments provide a system and method for short form and long form detection. Using a language-independent process, the detection system can ingest a corpus of documents, pre-process those documents by tokenizing the documents and performing a part-of-speech analysis, and can filter one or more candidate short forms using one or more filters that select for semantic criteria. Semantic criteria can include the part of speech of a token, whether the token contains more than a pre-determined amount of symbols or digits, whether the token appears too frequently in the corpus of documents, and whether the token has at least one uppercase letter. The detection system can detect short forms independent of case and punctuation, and independent of language-specific metaphone variants.
</div>
<div style="position:relative">						
	<div id="bib_24251763aa0b53bf2a8abae3a156d921lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Adam Kilgarriff’s legacy to computational linguistics and beyond</b>.<br/>
In: 

<i>Proceedings of the 17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING 2016), April 2016, Konya, Turkey</i>.
Konya, 2016.
bibtex: evansadam
<br/>
Roger Evans, Alexander Gelbukh, Gregory Grefenstette, Patrick Hanks, Milǒs Jakubícek, Diana McCarthy, Martha Palmer, Ted Pedersen, Michael Rundell, Pavel Rychl̀y, Serge Sharoff and David Tugwell.
<br/>


<a onclick="toggleAbstract('lepsky', '54b4caa139e04ccc7dcf2fbcaeb59330'); return false;" href="https://www.bibsonomy.org/bibtex/254b4caa139e04ccc7dcf2fbcaeb59330/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '54b4caa139e04ccc7dcf2fbcaeb59330', 'https://www.bibsonomy.org/bibtex/254b4caa139e04ccc7dcf2fbcaeb59330/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/254b4caa139e04ccc7dcf2fbcaeb59330/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_54b4caa139e04ccc7dcf2fbcaeb59330lepsky" style="display:none;border:1px dotted grey;">
Abstract. This year, the CICLing conference is dedicated to the memory of Adam Kilgarriff who died last year. Adam leaves behind a tremendous scientific legacy and those working in computational linguistics, other fields of linguistics and lexicography are indebted to him. This paper is a summary review of some of Adam’s main scientific contributions. It is not and cannot be exhaustive. It is writ- ten by only a small selection of his large network of collaborators. Nevertheless we hope this will provide a useful summary for readers wanting to know more about the origins of work, events and software that are so widely relied upon by scientists today, and undoubtedly will continue to be so in the foreseeable future.
</div>
<div style="position:relative">						
	<div id="bib_54b4caa139e04ccc7dcf2fbcaeb59330lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Corpus analysis without prior linguistic knowledge : unsupervised mining of phrases and subphrase structure</b>. <br/>
<i>arXiv:1602.05772 [cs]</i>, 2016.
arXiv: 1602.05772
<br/>
Stefan Gerdjikov and Klaus U. Schulz.
<br/>
<a href="http://arxiv.org/abs/1602.05772">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '8aface747639c55069d59f4c9afe38e7'); return false;" href="https://www.bibsonomy.org/bibtex/28aface747639c55069d59f4c9afe38e7/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '8aface747639c55069d59f4c9afe38e7', 'https://www.bibsonomy.org/bibtex/28aface747639c55069d59f4c9afe38e7/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/28aface747639c55069d59f4c9afe38e7/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_8aface747639c55069d59f4c9afe38e7lepsky" style="display:none;border:1px dotted grey;">
When looking at the structure of natural language, "phrases" and "words" are central notions. We consider the problem of identifying such "meaningful subparts" of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying "phrases", mining subphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a "general dictionary and grammar of the corpus". We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields.
</div>
<div style="position:relative">						
	<div id="bib_8aface747639c55069d59f4c9afe38e7lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Semantic annotation of the ACL anthology corpus for the automatic analysis of scientific literature</b>. <br/>
In: N. C. (C. Chair), K. Choukri, T. Declerck, S. Goggi, M. Grobelnik, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk and S. Piperidis, editors, <i>Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</i>, pages 23-28.
European Language Resources Association (ELRA), Paris, 2016.
bibtex: GBOR16.870 bibtex[date-added=2016-09-11 15:04:45 +0000;date-modified=2016-09-11 19:23:07 +0000;year=2016]
<br/>
Kata Gábor, Haifa Zargayouna, Davide Buscaldi, Isabelle Tellier and Thierry Charnois.
<br/>


<a onclick="toggleAbstract('lepsky', '92793dfec5e273342c77bf85ed48ceb7'); return false;" href="https://www.bibsonomy.org/bibtex/292793dfec5e273342c77bf85ed48ceb7/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '92793dfec5e273342c77bf85ed48ceb7', 'https://www.bibsonomy.org/bibtex/292793dfec5e273342c77bf85ed48ceb7/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/292793dfec5e273342c77bf85ed48ceb7/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_92793dfec5e273342c77bf85ed48ceb7lepsky" style="display:none;border:1px dotted grey;">
This paper describes the process of creating a corpus annotated for concepts and semantic relations in the scientific domain. A part of the ACL Anthology Corpus was selected for annotation, but the annotation process itself is not specific to the computational linguistics domain and could be applied to any scientific corpora. Concepts were identified and annotated fully automatically, based on a combination of terminology extraction and available ontological resources. A typology of semantic relations between concepts is also proposed. This typology, consisting of 18 domain-specific and 3 generic relations, is the result of a corpus-based investigation of the text sequences occurring between concepts in sentences. A sample of 500 abstracts from the corpus is currently being manually annotated with these semantic relations. Only explicit relations are taken into account, so that the data could serve to train or evaluate pattern-based semantic relation classification systems.
</div>
<div style="position:relative">						
	<div id="bib_92793dfec5e273342c77bf85ed48ceb7lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Open knowledge maps : creating a visual interface to the world’s scientific knowledge based on natural language processing</b>. <br/>
<i>027.7 Zeitschrift für Bibliothekskultur / Journal for Library Culture</i>, 4(2):98-103, 2016.

<br/>
Peter Kraker, Christopher Kittel and Asura Enkhbayar.
<br/>
<a href="http://0277.ch/ojs/index.php/cdrs_0277/article/view/157">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '22053e8b450fadaa9468232d1aac385f'); return false;" href="https://www.bibsonomy.org/bibtex/222053e8b450fadaa9468232d1aac385f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '22053e8b450fadaa9468232d1aac385f', 'https://www.bibsonomy.org/bibtex/222053e8b450fadaa9468232d1aac385f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/222053e8b450fadaa9468232d1aac385f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_22053e8b450fadaa9468232d1aac385flepsky" style="display:none;border:1px dotted grey;">
The goal of Open Knowledge Maps is to create a visual interface to the world’s scientific knowledge. The base for this visual interface consists of so-called knowledge maps, which enable the exploration of existing knowledge and the discovery of new knowledge. Our open source knowledge mapping software applies a mixture of summarization techniques and similarity measures on article metadata, which are iteratively chained together. After processing, the representation is saved in a database for use in a web visualization. In the future, we want to create a space for collective knowledge mapping that brings together individuals and communities involved in exploration and discovery. We want to enable people to guide each other in their discovery by collaboratively annotating and modifying the automatically created maps.Das Ziel von Open Knowledge Map ist es, ein visuelles Interface zum wissenschaftlichen Wissen der Welt bereitzustellen. Die Basis für die dieses Interface sind sogenannte “knowledge maps”, zu deutsch Wissenslandkarten. Wissenslandkarten ermöglichen die Exploration bestehenden Wissens und die Entdeckung neuen Wissens. Unsere Open Source Software wendet für die Erstellung der Wissenslandkarten eine Reihe von Text Mining Verfahren iterativ auf die Metadaten wissenschaftlicher Artikel an. Die daraus resultierende Repräsentation wird in einer Datenbank für die Anzeige in einer Web-Visualisierung abgespeichert. In Zukunft wollen wir einen Raum für das kollektive Erstellen von Wissenslandkarten schaffen, der die Personen und Communities, welche sich mit der Exploration und Entdeckung wissenschaftlichen Wissens beschäftigen, zusammenbringt. Wir wollen es den NutzerInnen ermöglichen, einander in der Literatursuche durch kollaboratives Annotieren und Modifizieren von automatisch erstellten Wissenslandkarten zu unterstützen.
</div>
<div style="position:relative">						
	<div id="bib_22053e8b450fadaa9468232d1aac385flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Natural language processing and information systems</b>.<br/>
2016. 

<br/>
<a href="http://link.springer.com/10.1007/978-3-319-41754-7">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '6147c63c2980c45bf338d2f5190af2c0'); return false;" href="https://www.bibsonomy.org/bibtex/26147c63c2980c45bf338d2f5190af2c0/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6147c63c2980c45bf338d2f5190af2c0', 'https://www.bibsonomy.org/bibtex/26147c63c2980c45bf338d2f5190af2c0/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26147c63c2980c45bf338d2f5190af2c0/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6147c63c2980c45bf338d2f5190af2c0lepsky" style="display:none;border:1px dotted grey;">
This book constitutes the refereed proceedings of the 21st International Conference on Applications of Natural Language to Information Systems, NLDB 2016, held in Salford, UK, in June 2016. The 17 full papers, 22 short papers, and 13 poster papers presented were carefully reviewed and selected from 83 submissions. The papers cover the following topics: theoretical aspects, algorithms, applications, architectures for applied and integrated NLP, resources for applied NLP, and other aspects of NLP.
</div>
<div style="position:relative">						
	<div id="bib_6147c63c2980c45bf338d2f5190af2c0lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>BabelNet 3.0</b>.<br/>
In: 

<i>A core for linguistic linked data and NLP</i>, pages XV-XVI.
Springer, 2016.

<br/>
Roberto Navigli.
<br/>

<a href="http://www.scopus.com/inward/record.url?scp=84964048415&amp;partnerID=8YFLogxK">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '51b327a5f184e92e8d38d1e8cdc398d6'); return false;" href="https://www.bibsonomy.org/bibtex/251b327a5f184e92e8d38d1e8cdc398d6/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '51b327a5f184e92e8d38d1e8cdc398d6', 'https://www.bibsonomy.org/bibtex/251b327a5f184e92e8d38d1e8cdc398d6/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/251b327a5f184e92e8d38d1e8cdc398d6/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_51b327a5f184e92e8d38d1e8cdc398d6lepsky" style="display:none;border:1px dotted grey;">
This tutorial will introduce BabelNet 3.0, the largest multilingual encyclopedic dictionary and semantic network, which covers 271 languages. BabelNet is a core component of the Linked Open Data cloud and a powerful engine for virtually any Natural Language Processing task in desperate need of wide-coverage lexical semantics in arbitrary languages.
</div>
<div style="position:relative">						
	<div id="bib_51b327a5f184e92e8d38d1e8cdc398d6lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>TextFlows : a visual programming platform for text mining and natural language processing</b>. <br/>
<i>Science of Computer Programming</i>, 2016.

<br/>
Matic Perovšek, Janez Kranjc, Tomaž Erjavec, Bojan Cestnik and Nada Lavrač.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/S0167642316000113">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b347bca9842ae2332af7224e0449dc17'); return false;" href="https://www.bibsonomy.org/bibtex/2b347bca9842ae2332af7224e0449dc17/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b347bca9842ae2332af7224e0449dc17', 'https://www.bibsonomy.org/bibtex/2b347bca9842ae2332af7224e0449dc17/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b347bca9842ae2332af7224e0449dc17/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b347bca9842ae2332af7224e0449dc17lepsky" style="display:none;border:1px dotted grey;">
Text mining and natural language processing are fast growing areas of research, with numerous applications in business, science and creative industries. This paper presents TextFlows, a web-based text mining and natural language processing platform supporting workflow construction, sharing and execution. The platform enables visual construction of text mining workflows through a web browser, and the execution of the constructed workflows on a processing cloud. This makes TextFlows an adaptable infrastructure for the construction and sharing of text processing workflows, which can be reused in various applications. The paper presents the implemented text mining and language processing modules, and describes some precomposed workflows. Their features are demonstrated on three use cases: comparison of document classifiers and of different part-of-speech taggers on a text categorization problem, and outlier detection in document corpora.
</div>
<div style="position:relative">						
	<div id="bib_b347bca9842ae2332af7224e0449dc17lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Bibliometrics, information retrieval and natural language processing : natural synergies to support digital library research.</b>.<br/>
In: 

<i>BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries</i>, pages 6-13.
2016.
bibtex: wolfram2016bibliometrics
<br/>
Dietmar Wolfram.
<br/>

<a href="http://ceur-ws.org/Vol-1610/paper1.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '92b4b9b496f80f1805d791615ad3a79a'); return false;" href="https://www.bibsonomy.org/bibtex/292b4b9b496f80f1805d791615ad3a79a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '92b4b9b496f80f1805d791615ad3a79a', 'https://www.bibsonomy.org/bibtex/292b4b9b496f80f1805d791615ad3a79a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/292b4b9b496f80f1805d791615ad3a79a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_92b4b9b496f80f1805d791615ad3a79alepsky" style="display:none;border:1px dotted grey;">
Historically, researchers have not fully capitalized on the potential synergies that exist between bibliometrics and information retrieval (IR). Knowledge of regularities in information production and use, as well as citation relationships in bibliographic databases that are studied in bibliometrics, can benefit IR system design and evaluation. Similarly, techniques developed for IR and database technology have made the investigation of large-scale bibliometric phenomena feasible. Both fields of study have also benefitted directly from de- velopments in natural language processing (NLP), which have provided new tools and techniques to explore research problems in bibliometrics and IR. Digi- tal libraries, with their full text, multimedia content, along with searching and browsing capabilities, represent ideal environments in which to investigate the mutually beneficial relationships that can be forged among bibliometrics, IR and NLP. This brief presentation highlights the symbiotic relationship that ex- ists among bibliometrics, IR and NLP.
</div>
<div style="position:relative">						
	<div id="bib_92b4b9b496f80f1805d791615ad3a79alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Text data management and analysis : a practical introduction to information retrieval and text mining</b>.<br/>
2016. 
<br/>Chengxiang Zhai and Sean Massung.
<br/>


<a onclick="toggleBibtex('lepsky', '657443c72ad6e95c8fd631e24869d01f', 'https://www.bibsonomy.org/bibtex/2657443c72ad6e95c8fd631e24869d01f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2657443c72ad6e95c8fd631e24869d01f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_657443c72ad6e95c8fd631e24869d01flepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_657443c72ad6e95c8fd631e24869d01flepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2015" class="bibsonomy_quicknav_group"><a name="2015">2015</a></h3>
<div style="margin-bottom:1em">
<b>Ein einfacher und transparenter Zugang zur sprachtechnologischen Arbeit mit Textdaten : Beispiele aus dem Projekt e-Identity</b>. <br/>
<i>Information - Wissenschaft &amp; Praxis</i>, 66(5-6):345, 2015.

<br/>
Fritz Kliche and Ulrich Heid.
<br/>
<a href="/brokenurl#//www.degruyter.com/view/j/iwp.2015.66.issue-5-6/iwp-2015-0059/iwp-2015-0059.xml">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '751543fd7412465a5ed2a9496e62df0f'); return false;" href="https://www.bibsonomy.org/bibtex/2751543fd7412465a5ed2a9496e62df0f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '751543fd7412465a5ed2a9496e62df0f', 'https://www.bibsonomy.org/bibtex/2751543fd7412465a5ed2a9496e62df0f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2751543fd7412465a5ed2a9496e62df0f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_751543fd7412465a5ed2a9496e62df0flepsky" style="display:none;border:1px dotted grey;">
Die Computerlinguistik hat verschiedene Werkzeuge für die Textanalyse entwickelt, die allerdings aus der Sicht geisteswissenschaftlicher Nutzer oft nicht unmittelbar anwendbar sind. Wir beschreiben in diesem Beitrag eine Web-Anwendung, mit der digitale Textsammlungen möglichst einfach für die Arbeit mit computerlinguistischen Methoden zugänglich gemacht werden können, und wir diskutieren die Konzepte, mit denen dieser einfache Zugang erreicht wird.
</div>
<div style="position:relative">						
	<div id="bib_751543fd7412465a5ed2a9496e62df0flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Electronic lexicography in the 21st century : linking lexical data in the digital age. Proceedings of the eLex 2015 conference, 11-13 August 2015, Herstmonceux Castle, United Kingdom</b>.<br/>
2015. 
<br/>I. Kosem, M. Jakubíček, J. Kallas and S. Krek.
<br/>
<a href="https://elex.link/elex2015/conference-proceedings/">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', '467e29f13c2eca2cd44ca82c73da4111', 'https://www.bibsonomy.org/bibtex/2467e29f13c2eca2cd44ca82c73da4111/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2467e29f13c2eca2cd44ca82c73da4111/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_467e29f13c2eca2cd44ca82c73da4111lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_467e29f13c2eca2cd44ca82c73da4111lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Growing trees from morphs : towards data-driven morphologi cal parsing</b>.<br/>
In: 

<i>Proceedings of the Int. Conference of the German Society for Computational Linguistics and Language Technology</i>, pages 49-57.
Universität Duisburg-Essen, 2015.

<br/>
Petra Steiner and Josef Ruppenhofer.
<br/>

<a href="http://gscl2015.inf.uni-due.de/wp-content/uploads/2016/02/GSCL-201508.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '8f3149da589a24580423a2a74896d3fd'); return false;" href="https://www.bibsonomy.org/bibtex/28f3149da589a24580423a2a74896d3fd/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '8f3149da589a24580423a2a74896d3fd', 'https://www.bibsonomy.org/bibtex/28f3149da589a24580423a2a74896d3fd/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/28f3149da589a24580423a2a74896d3fd/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_8f3149da589a24580423a2a74896d3fdlepsky" style="display:none;border:1px dotted grey;">
We present a quantitative approach to disambiguating flat morphological analy- ses and producing more deeply structured analyses. Based on existing morphologi- cal segmentations, possible combinations of resulting word trees for the next level are filtered first by criteria of linguistic plausibility and then by weighting proce- dures based on the geometric mean. The frequencies for weighting are derived from three different sources (counts of morphs in a lexicon, counts of largest con- stituents in a lexicon, counts of token fre- quencies in a corpus) and can be used ei- ther to find the best analysis on the level of morphs or on the next higher constituent level. The evaluation shows that for this task corpus-based frequency counts are slightly superior to counts of lexical data.
</div>
<div style="position:relative">						
	<div id="bib_8f3149da589a24580423a2a74896d3fdlepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2014" class="bibsonomy_quicknav_group"><a name="2014">2014</a></h3>
<div style="margin-bottom:1em">
<b>How to combine text-mining methods to validate induced verb-object relations?</b>. <br/>
<i>Computer Science and Information Systems</i>(00):21, 2014.

<br/>
Nicolas Béchet, Jacques Chauché, Violaine Prince and Mathieu Roche.
<br/>

<a onclick="toggleAbstract('lepsky', '25aabbe92986dd7735b93805d549178b'); return false;" href="https://www.bibsonomy.org/bibtex/225aabbe92986dd7735b93805d549178b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '25aabbe92986dd7735b93805d549178b', 'https://www.bibsonomy.org/bibtex/225aabbe92986dd7735b93805d549178b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/225aabbe92986dd7735b93805d549178b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_25aabbe92986dd7735b93805d549178blepsky" style="display:none;border:1px dotted grey;">
This paper describes methods using Natural Language Processing ap- proaches to extract and validate induced syntactic relations (here restricted to the Verb-Object relation). These methods use a syntactic parser and a semantic close- ness measure to extract such relations. Then, their validation is based on two dif- ferent techniques: A Web Validation system on one part, then a Semantic-Vector- based approach, and finally different combinations of both techniques in order to rank induced Verb-Object relations. The Semantic Vector approach is a Roget-based method which computes a syntactic relation as a vector. Web Validation uses a search engine to determine the relevance of a syntactic relation according to its popularity. An experimental protocol is set up to judge automatically the relevance of the sorted induced relations. We finally apply our approach on a French corpus of news by using ROC Curves to evaluate the results.
</div>
<div style="position:relative">						
	<div id="bib_25aabbe92986dd7735b93805d549178blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Natural language processing through different classes of machine learning</b>. <br/>
In: , pages 307-315.
2014.

<br/>
Harsh Jain and Keshav Mathur.
<br/>

<a href="http://airccj.org/CSCP/vol4/csit41926.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ea1d272a123fbb7f32210d00d1388c2a'); return false;" href="https://www.bibsonomy.org/bibtex/2ea1d272a123fbb7f32210d00d1388c2a/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ea1d272a123fbb7f32210d00d1388c2a', 'https://www.bibsonomy.org/bibtex/2ea1d272a123fbb7f32210d00d1388c2a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ea1d272a123fbb7f32210d00d1388c2a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ea1d272a123fbb7f32210d00d1388c2alepsky" style="display:none;border:1px dotted grey;">
The internet community has been benefitting tremendously from the works of various researchers in the field of Natural Language Processing. Semantic orientation analysis, sentiment analysis, etc. has served the social networks as well as companies relying on user reviews well. Flame identification has made the internet less hostile for some users. Spam filtering has made the electronic mail a more efficient means of communication. But with the incessant growth of the internet, NLP using machine learning working on massive sets of raw and unprocessed data is an ever-growing challenge. Semi-supervised machine learning can overcome this problem by using a large set of unlabeled data in conjunction with a small set of labeled data. Also, focusing on developing NLP systems that can contribute to developing a unified architecture could pave the way towards General Intelligence in the future.
</div>
<div style="position:relative">						
	<div id="bib_ea1d272a123fbb7f32210d00d1388c2alepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Handbook of terminology</b>.<br/>
2014. 

<br/>
<a href="https://benjamins.com/online/hot/">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a36cf95faf94135db87d339606ded3c9'); return false;" href="https://www.bibsonomy.org/bibtex/2a36cf95faf94135db87d339606ded3c9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a36cf95faf94135db87d339606ded3c9', 'https://www.bibsonomy.org/bibtex/2a36cf95faf94135db87d339606ded3c9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a36cf95faf94135db87d339606ded3c9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a36cf95faf94135db87d339606ded3c9lepsky" style="display:none;border:1px dotted grey;">
Terminology has started to explore unbeaten paths since Wüster, and has nowadays grown into a multifacetted science, which seems to have reached adulthood, thanks to integrating multiple contributions not only from different linguistic schools, including computer, corpus, variational, sociocognitive and sociocommunicative linguistics, and framebased semantics, but also from engineering and formal language developers. In this ever changing and diverse context, Terminology offers a wide range of opportunities ranging from standardized and prescriptive to prototype and userbased approaches. At this point of its road map, Terminology can nowadays claim to offer userbased and useroriented, hence userfriendly, approaches to terminological phenomenona, when searching, extracting and analysing relevant terminology in online corpora, when building term bases that contribute to efficient communication among domain experts in languages for special purposes, or even when proposing terms and definitions formed on the basis of a generally agreed consensus in international standard bodies. Terminology is now ready to advance further, thanks to the integration of meaning description taking into account dynamic natural language phenomena, and of consensusbased terminology management in order to help experts communicate in their domainspecific languages. In this Handbook of Terminology (HoT), the symbiosis of Terminology with Linguistics allows a mature and multidimensional reflection on terminological phenomena, which will eventually generate future applications which have not been tested yet in natural language. The HoT aims at disseminating knowledge about terminology (management) and at providing easy access to a large range of topics, traditions, best practices, and methods to a broad audience: students, researchers, professionals and lecturers in Terminology, scholars and experts from other disciplines (among which linguistics, life sciences, metrology, chemistry, law studies, machine engineering, and actually any expert domain). In addition, the HoT addresses any of those with a professional or personal interest in (multilingual) terminology, translation, interpreting, localization, editing, etc., such as communication specialists, translators, scientists, editors, public servants, brand managers, engineers, (intercultural) organization specialists, and experts in any field. Moreover, the HoT offers added value, in that it is the first handbook with this scope in Terminology which has both a print edition (also available as a PDF ebook) and an online version. For access to the Handbook of Terminology Online , please visit benjamins.com/online/hot/ . The HoT is linked to the Handbook of Translation Studies, not in the least because of its interdisciplinary approaches, but also because of the inevitable intertwining between translation and terminology. All chapters are written by specialists in the different subfields and are peerreviewed.
</div>
<div style="position:relative">						
	<div id="bib_a36cf95faf94135db87d339606ded3c9lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>SHELDON : Semantic Holistic framEwork for LinkeD ONtology data</b>. <br/>
, 2014.

<br/>
Diego Recupero, Andrea Nuzzolese, Sergio Consoli, Aldo Gangemi and Valentina Presutti.
<br/>
<a href="http://challenge.semanticweb.org/2014/submissions/swc2014_submission_1.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '90b82d59a427ce87146e91d586e0b316'); return false;" href="https://www.bibsonomy.org/bibtex/290b82d59a427ce87146e91d586e0b316/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '90b82d59a427ce87146e91d586e0b316', 'https://www.bibsonomy.org/bibtex/290b82d59a427ce87146e91d586e0b316/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/290b82d59a427ce87146e91d586e0b316/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_90b82d59a427ce87146e91d586e0b316lepsky" style="display:none;border:1px dotted grey;">
SHELDON is the first true hybridization of NLP machine reading and Semantic Web. It is a framework that builds upon a machine reader for extracting RDF graphs from text so that the output is compliant to Semantic Web and Linked Data patterns. It extends the current human-readable web by using Semantic Web practices and technologies in a machine-processable form. Given a sentence in any language, it provides different semantic functionalities (frame detection, topic extraction, named entity recognition, resolution and coreference, terminology extraction, sense tagging and disambiguation, taxonomy induction, semantic role labeling, type induction, sentiment analysis, citation inference, relation and event extraction) as well as nice visualization tools which make use of the JavaScript infoVis Toolkit and RelFinder, as well as a knowledge enrichment component that extends machine reading to Semantic Web data. The system can be freely used at http://wit.istc.cnr.it/stlab-tools/sheldon.
</div>
<div style="position:relative">						
	<div id="bib_90b82d59a427ce87146e91d586e0b316lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Using compound lists for German decompounding in a back-off scenario</b>. <br/>
, 2014.

<br/>
Pedro Santos.
<br/>
<a href="http://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2014/2014_CCLCC_Decompounding_PSa_CameraReady.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'ba41c78428ea76c25fc35d265fd92014'); return false;" href="https://www.bibsonomy.org/bibtex/2ba41c78428ea76c25fc35d265fd92014/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ba41c78428ea76c25fc35d265fd92014', 'https://www.bibsonomy.org/bibtex/2ba41c78428ea76c25fc35d265fd92014/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ba41c78428ea76c25fc35d265fd92014/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ba41c78428ea76c25fc35d265fd92014lepsky" style="display:none;border:1px dotted grey;">
Lexical resources like GermaNet offer compound lists of reasonable size. These lists can be used as a prior step to exist- ing decompounding algorithms, wherein decompounding algorithms would func- tion as a back-off mechanism. We inves- tigate whether the use of compound lists can enhance dictionary and corpus-based decompounding algorithms. We analyze the effect of using an initial decompound- ing step based on a compound list de- rived from GermaNet with a gold standard in German. The obtained results show that applying information from GermaNet can significantly improve all tested de- compounding approaches across all met- rics. Precision and recall increases statis- tically significant by .004-.018 and .011- .022 respectively.
</div>
<div style="position:relative">						
	<div id="bib_ba41c78428ea76c25fc35d265fd92014lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Die kleinen Verräter : gerade die unscheinbarsten aller Wörter erlauben tiefe Einblick in unser Innerstes – und wir bemerken nichts</b>. <br/>
<i>Die Zeit</i>(51), 2014.

<br/>
Stefanie Schramm.
<br/>
<a href="http://www.zeit.de/2014/51/woerter-kommunikation-wortwahl/komplettansicht">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '4522dbb4e9c9ab19f4118adb165108bf'); return false;" href="https://www.bibsonomy.org/bibtex/24522dbb4e9c9ab19f4118adb165108bf/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4522dbb4e9c9ab19f4118adb165108bf', 'https://www.bibsonomy.org/bibtex/24522dbb4e9c9ab19f4118adb165108bf/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24522dbb4e9c9ab19f4118adb165108bf/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4522dbb4e9c9ab19f4118adb165108bflepsky" style="display:none;border:1px dotted grey;">
Selbstversuch. "Wie sieht Ihr Alltag aus?", fragt die Stimme am anderen Ende der Telefonleitung. Ich antworte: "Wecker um halb acht, Dusche, Kaffee und Zeitung, ins Büro, recherchieren, schreiben, abends joggen oder Türkisch-Kurs oder Kino." Die Stimme stellt weitere Fragen. Und ich antworte, exakt 1.447 Wörter. Mein Gesprächspartner zählt mit. Das fällt ihm nicht schwer, er ist ein Computer. Anhand der 1.447 Wörter soll er meine Persönlichkeit erkennen. Der Computer steht bei der Firma Psyware in Aachen. Er gehört zu einer ganz neuen Generation von Maschinen. Sie können aus Wörtern mehr herauslesen als der Mensch. Die Software analysiert nicht den Inhalt des Telefonats. Wann ich dusche, interessiert sie nicht im Geringsten. Sie untersucht aber, welche Wörter ich wie oft benutze und in welcher Weise ich sie aneinanderreihe. Welche Pronomen verwende ich, welche Fragewörter, Adverbien, Konjunktionen und wie viele Negationen? Aus all den winzigen Wörtern setzt sie ein Bild meiner Persönlichkeit zusammen.
</div>
<div style="position:relative">						
	<div id="bib_4522dbb4e9c9ab19f4118adb165108bflepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Texterra : a framework for text analysis</b>. <br/>
, 40(5):288-295, 2014.

<br/>
Turdakov, NA Astrakhantsev, Ya Nedumov, AA Sysoev, IA Andrianov, VD Mayorov, DG Fedorenko, AV Korshunov and SD Kuznetsov.
<br/>
<a href="http://dx.doi.org/10.1134/S0361768814050090">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c0c5488bfee5d090d9b074bef40ccf25'); return false;" href="https://www.bibsonomy.org/bibtex/2c0c5488bfee5d090d9b074bef40ccf25/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c0c5488bfee5d090d9b074bef40ccf25', 'https://www.bibsonomy.org/bibtex/2c0c5488bfee5d090d9b074bef40ccf25/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c0c5488bfee5d090d9b074bef40ccf25/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c0c5488bfee5d090d9b074bef40ccf25lepsky" style="display:none;border:1px dotted grey;">
A framework for fast text analysis, which is developed as a part of the Texterra project, is described. Texterra provides a scalable solution for the fast text processing on the basis of novel methods that exploit knowledge extracted from the Web and text documents. For the developed tools, details of the project, use cases, and evaluation results are presented.
</div>
<div style="position:relative">						
	<div id="bib_c0c5488bfee5d090d9b074bef40ccf25lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Jumping NLP curves : a review of natural language processing research</b>. <br/>
<i>IEEE Computational Intelligence Magazine</i>, 9:2, 2014.

<br/>
Bebo White and Eric Cambria.
<br/>
<a href="http://sentic.net/jumping-nlp-curves.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '62b3943acb7f23a417be3cf70149be18'); return false;" href="https://www.bibsonomy.org/bibtex/262b3943acb7f23a417be3cf70149be18/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '62b3943acb7f23a417be3cf70149be18', 'https://www.bibsonomy.org/bibtex/262b3943acb7f23a417be3cf70149be18/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/262b3943acb7f23a417be3cf70149be18/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_62b3943acb7f23a417be3cf70149be18lepsky" style="display:none;border:1px dotted grey;">
Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of 'jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves --namely Syntactics, Semantics, and Pragmatics Curves-- which will eventually lead NLP research to evolve into natural language understanding.
</div>
<div style="position:relative">						
	<div id="bib_62b3943acb7f23a417be3cf70149be18lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2013" class="bibsonomy_quicknav_group"><a name="2013">2013</a></h3>
<div style="margin-bottom:1em">
<b>NL2KR System</b>. <br/>
<i>Language Processing and Automated Reasoning (NLPAR) 2013</i>, 2013.

<br/>
Chitta Baral, Juraj Dzifcak, Kanchan Kumbhare and Nguyen Vo.
<br/>
<a href="http://ceur-ws.org/Vol-1044/nlpar2013-complete.pdf#page=40">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'c5b3f40045946cb1ac28dcea28b45b88'); return false;" href="https://www.bibsonomy.org/bibtex/2c5b3f40045946cb1ac28dcea28b45b88/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c5b3f40045946cb1ac28dcea28b45b88', 'https://www.bibsonomy.org/bibtex/2c5b3f40045946cb1ac28dcea28b45b88/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c5b3f40045946cb1ac28dcea28b45b88/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c5b3f40045946cb1ac28dcea28b45b88lepsky" style="display:none;border:1px dotted grey;">
In this paper we will describe the NL2KR system that translates natural language sentences to a targeted knowledge representation formalism. The system starts with an initial lexicon and learns meaning of new words from a given set of examples of sentences and their translations. We will describe the first release of our system with several examples.
</div>
<div style="position:relative">						
	<div id="bib_c5b3f40045946cb1ac28dcea28b45b88lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Natural language watermarking for German texts</b>. <br/>
In: <i>Proceedings of the First ACM Workshop on Information Hiding and Multimedia Security</i>, series IH&amp;MMSec '13, pages 193-202.
ACM, New York, NY, 2013.

<br/>
Oren Halvani, Martin Steinebach, Patrick Wolf and Ralf Zimmermann.
<br/>

<a href="http://doi.acm.org/10.1145/2482513.2482522">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'e68e223ff3493f23746aa62fb0dfaeb9'); return false;" href="https://www.bibsonomy.org/bibtex/2e68e223ff3493f23746aa62fb0dfaeb9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e68e223ff3493f23746aa62fb0dfaeb9', 'https://www.bibsonomy.org/bibtex/2e68e223ff3493f23746aa62fb0dfaeb9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e68e223ff3493f23746aa62fb0dfaeb9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e68e223ff3493f23746aa62fb0dfaeb9lepsky" style="display:none;border:1px dotted grey;">
In this paper we present four informed natural language watermark embedding methods, which operate on the lexical and syntactic layer of German texts. Our scheme provides several benefits in comparison to state-of-the-art approaches, as for instance that it is not relying on complex NLP operations like full sentence parsing, word sense disambiguation, named entity recognition or semantic role parsing. Even rich lexical resources (e.g. WordNet or the Collins thesaurus), which play an essential role in many previous approches, are unnecessary for our system. Instead, our methods require only a Part-Of-Speech Tagger, simple wordlists that act as black- and whitelists and a trained classifier, which automatically predicts the ability of potential lexical or syntactic patterns to carry portions of the watermark message. Besides this, a part of the proposed methods can be easily adapted into other Indo-European languages, since the grammar rules the methods rely on are not restricted only to the German language. Because the methods perform only lexical and minor syntactic transformations, the watermarked text is not affected by grammatical distortion and simultaneously the meaning of the text is preserved in 82.14%of the cases.
</div>
<div style="position:relative">						
	<div id="bib_e68e223ff3493f23746aa62fb0dfaeb9lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>A survey on the exchange of linguistic resources : publishing linguistic linked open data on the Web</b>. <br/>
<i>Program</i>, 47(3), 2013.

<br/>
Leonardo Lezcano, Salvador Sanchez and Antonio Roa-Valverde.
<br/>

<a onclick="toggleAbstract('lepsky', '9f3a9a02fadb031f2d01781d14c62195'); return false;" href="https://www.bibsonomy.org/bibtex/29f3a9a02fadb031f2d01781d14c62195/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '9f3a9a02fadb031f2d01781d14c62195', 'https://www.bibsonomy.org/bibtex/29f3a9a02fadb031f2d01781d14c62195/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/29f3a9a02fadb031f2d01781d14c62195/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_9f3a9a02fadb031f2d01781d14c62195lepsky" style="display:none;border:1px dotted grey;">
Purpose - The purpose of this article is to provide a literature review of the principal formats and frameworks that have been used in the last 20 years to exchange linguistic resources. Special attention has been given to the most recent approaches to publishing linguistic linked open data on the Web. Design/methodology/approach - Research papers published since 1990 on the use of various formats, standards, frameworks and methods to exchange linguistic information were divided into two main categories: those proposing specific schemas and syntaxes to suit the requirements of a given type of linguistic data (these are referred to as offline approaches), and those adopting the Linked Data initiative and the Semantic Web technologies to support the interoperability of heterogeneous linguistic resources. For each paper, the type of linguistic resource exchanged, the framework/format used, the interoperability approach taken and the related projects were identified. Findings - The information gathered in the survey reflects an increase in recent years in approaches adopting the LD initiative. This is due to the fact that the structural and syntactic issues which arise when addressing the interoperability of linguistic resources can be solved by applying Semantic Web technologies. What remains an open issue in the field of computational linguistics is the development of knowledge artifacts and mechanisms to support the alignment of the different aspects of LRs in order to guarantee semantic and conceptual interoperability in the LOD cloud. Ontologies have proved to be of great use in achieving this goal. Research limitations/implications - The research presented here is by no means a comprehensive or all-inclusive survey of all existing approaches to the exchange of linguistic resources. Rather, the aim was to highlight, analyze and categorize the most significant advances in the field. Practical implications - This survey has practical implications for computational linguists and for every application requiring new developments in Natural Language Processing. In addition, multilingual issues can be better addressed when semantic interoperability of heterogeneous linguistic resources is achieved. Originality/value - The paper provides a survey of past and present research and developments addressing the interoperability of linguistic resources, including those where the Linked Data initiative has been adopted.
</div>
<div style="position:relative">						
	<div id="bib_9f3a9a02fadb031f2d01781d14c62195lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Modeling semantic relations expressed by prepositions</b>. <br/>
<i>Transactions of the Association for Computational Linguistics</i>, 1(2013):231-242, 2013.

<br/>
Vivek Srikumar and Dan Roth.
<br/>

<a onclick="toggleAbstract('lepsky', '238eb9c83fbb4a6a157b3d5c46d97f7d'); return false;" href="https://www.bibsonomy.org/bibtex/2238eb9c83fbb4a6a157b3d5c46d97f7d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '238eb9c83fbb4a6a157b3d5c46d97f7d', 'https://www.bibsonomy.org/bibtex/2238eb9c83fbb4a6a157b3d5c46d97f7d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2238eb9c83fbb4a6a157b3d5c46d97f7d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_238eb9c83fbb4a6a157b3d5c46d97f7dlepsky" style="display:none;border:1px dotted grey;">
This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments. We define an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions. Given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as a way to support the relation prediction. The annotated data, however, only provides labels for the relation label, and not the arguments and types. We address this by presenting two models for preposition relation labeling. Our generalization of latent structure SVM gives close to 90%accuracy on relation labeling. Further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we can not only improve the relation accuracy, but also significantly improve sense prediction accuracy.
</div>
<div style="position:relative">						
	<div id="bib_238eb9c83fbb4a6a157b3d5c46d97f7dlepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2012" class="bibsonomy_quicknav_group"><a name="2012">2012</a></h3>
<div style="margin-bottom:1em">
<b>Pattern learning for relation extraction with a hierarchical topic model</b>. <br/>
In: <i>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</i>, pages 54-59.
Association for Computational Linguistics, Jeju, 2012.

<br/>
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort and Guillermo Garrido.
<br/>

<a href="http://dl.acm.org/ft_gateway.cfm?id=2390679&amp;ftid=1304477&amp;dwn=1&amp;CFID=203278172&amp;CFTOKEN=56557103">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '71627ef207d9b8620d32853b566303f9'); return false;" href="https://www.bibsonomy.org/bibtex/271627ef207d9b8620d32853b566303f9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '71627ef207d9b8620d32853b566303f9', 'https://www.bibsonomy.org/bibtex/271627ef207d9b8620d32853b566303f9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/271627ef207d9b8620d32853b566303f9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_71627ef207d9b8620d32853b566303f9lepsky" style="display:none;border:1px dotted grey;">
We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision.
</div>
<div style="position:relative">						
	<div id="bib_71627ef207d9b8620d32853b566303f9lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Computational linguistics : applications</b>.<br/>
2012. 
<br/>Adam Przepirkowski, Maciej Piasecki, Krzysztof Jassem and Piotr Fuglewicz.
<br/>

<a onclick="toggleAbstract('lepsky', 'e180f1503f97d8a6a03775e2dbba363f'); return false;" href="https://www.bibsonomy.org/bibtex/2e180f1503f97d8a6a03775e2dbba363f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e180f1503f97d8a6a03775e2dbba363f', 'https://www.bibsonomy.org/bibtex/2e180f1503f97d8a6a03775e2dbba363f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e180f1503f97d8a6a03775e2dbba363f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e180f1503f97d8a6a03775e2dbba363flepsky" style="display:none;border:1px dotted grey;">
The ever-growing popularity of Google over the recent decade has required a specific method of man-machine communication: human query should be short, whereas the machine answer may take a form of a wide range of documents. This type of communication has triggered a rapid development in the domain of Information Extraction, aimed at providing the asker with a more precise information. The recent success of intelligent personal assistants supporting users in searching or even extracting information and answers from large collections of electronic documents signals the onset of a new era in man-machine communication we shall soon explain to our small devices what we need to know and expect valuable answers quickly and automatically delivered. The progress of man-machine communication is accompanied by growth in the significance of applied Computational Linguistics we need machines to understand much more from the language we speak naturally than it is the case of up-to-date search systems. Moreover, we need machine support in crossing language barriers that is necessary more and more often when facing the global character of the Web. This books reports on the latest developments in the field. It contains 15 chapters written by researchers who aim at making linguistic theories work for the better understanding between the man and the machine.
</div>
<div style="position:relative">						
	<div id="bib_e180f1503f97d8a6a03775e2dbba363flepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Big data methods for computational linguistics</b>. <br/>
, 2012.

<br/>
Gerhard Weikum, Johannes Hoffart, Ndapandula Nakashole, Marc Spaniol, Fabian Suchanek and Mohammed Yosef.
<br/>

<a onclick="toggleAbstract('lepsky', '3849bc05693e8a5f0dfdce7b929ab510'); return false;" href="https://www.bibsonomy.org/bibtex/23849bc05693e8a5f0dfdce7b929ab510/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '3849bc05693e8a5f0dfdce7b929ab510', 'https://www.bibsonomy.org/bibtex/23849bc05693e8a5f0dfdce7b929ab510/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/23849bc05693e8a5f0dfdce7b929ab510/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_3849bc05693e8a5f0dfdce7b929ab510lepsky" style="display:none;border:1px dotted grey;">
Many tasks in computational linguistics traditionally rely on hand-crafted or curated resources like thesauri or word-sense-annotated corpora. The availability of big data, from the Web and other sources, has changed this situation. Harnessing these assets requires scalable methods for data and text analytics. This paper gives an overview on our recent work that utilizes big data methods for enhancing semantics-centric tasks dealing with natural language texts. We demonstrate a virtuous cycle in harvesting knowledge from large data and text collections and leveraging this knowledge in order to improve the annotation and interpretation of language in Web pages and social media. Specifically, we show how to build large dictionaries of names and paraphrases for entities and relations, and how these help to disambiguate entity mentions in texts.
</div>
<div style="position:relative">						
	<div id="bib_3849bc05693e8a5f0dfdce7b929ab510lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2010" class="bibsonomy_quicknav_group"><a name="2010">2010</a></h3>
<div style="margin-bottom:1em">
<b>Staying informed : supervised and semisupervised multi-view topical analysis of ideological perspective</b>. <br/>
In: <i>In Proc of</i>, pages 1140-1150.
2010.

<br/>
Amr Ahmed and Eric P. Xing.
<br/>


<a onclick="toggleAbstract('lepsky', '69e27eb1feec9a4db1d5ce4f8ec65a01'); return false;" href="https://www.bibsonomy.org/bibtex/269e27eb1feec9a4db1d5ce4f8ec65a01/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '69e27eb1feec9a4db1d5ce4f8ec65a01', 'https://www.bibsonomy.org/bibtex/269e27eb1feec9a4db1d5ce4f8ec65a01/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/269e27eb1feec9a4db1d5ce4f8ec65a01/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_69e27eb1feec9a4db1d5ce4f8ec65a01lepsky" style="display:none;border:1px dotted grey;">
With the proliferation of user-generated articles over the web, it becomes imperative to develop automated methods that are aware of the ideological-bias implicit in a document collection. While there exist methods that can classify the ideological bias of a given document, little has been done toward understanding the nature of this bias on a topical-level. In this paper we address the problem of modeling ideological perspective on a topical level using a factored topic model. We develop efficient inference algorithms using Collapsed Gibbs sampling for posterior inference, and give various evaluations and illustrations of the utility of our model on various document collections with promising results. Finally we give a Metropolis-Hasting inference algorithm for a semi-supervised extension with decent results. 1
</div>
<div style="position:relative">						
	<div id="bib_69e27eb1feec9a4db1d5ce4f8ec65a01lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Computerlinguistik und Sprachtechnologie : eine Einführung</b>.<br/>
2010. 
<br/>Kai-Uwe Carstensen.
<br/>

<a onclick="toggleAbstract('lepsky', '8d4d2b37870f459aea01f14df5d328de'); return false;" href="https://www.bibsonomy.org/bibtex/28d4d2b37870f459aea01f14df5d328de/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '8d4d2b37870f459aea01f14df5d328de', 'https://www.bibsonomy.org/bibtex/28d4d2b37870f459aea01f14df5d328de/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/28d4d2b37870f459aea01f14df5d328de/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_8d4d2b37870f459aea01f14df5d328delepsky" style="display:none;border:1px dotted grey;">
Dieses Lehrbuch bietet eine umfassende Einführung in Grundlagen und Methoden der Computerlinguistik und stellt die wichtigsten Anwendungsgebiete in der Sprachtechnologie vor. Es richtet sich gleichermaßen an Studierende der Computerlinguistik und verwandter Fächer mit Bezug zur Verarbeitung natürlicher Sprache wie an Entwickler sprachverarbeitender Systeme. Für die dritte Auflage wurden sämtliche Kapitel überarbeitet und aktualisiert sowie zum Teil zu eigenständigen, neuen Kapiteln zusammengeführt. Insbesondere trägt die dritte Auflage der rasanten Entwicklung in der Computerlinguistik und Sprachtechnologie durch eine stärkere Fokussierung auf statistische Grundlagen und Methoden Rechnung.
</div>
<div style="position:relative">						
	<div id="bib_8d4d2b37870f459aea01f14df5d328delepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>The handbook of computational linguistics and natural language processing</b>.<br/>
2010. 
<br/>Alexander Clark, Chris Fox and Shalom Lappin.
<br/>

<a onclick="toggleAbstract('lepsky', '4e4386131cdae6c78f360d9bf9d1396b'); return false;" href="https://www.bibsonomy.org/bibtex/24e4386131cdae6c78f360d9bf9d1396b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '4e4386131cdae6c78f360d9bf9d1396b', 'https://www.bibsonomy.org/bibtex/24e4386131cdae6c78f360d9bf9d1396b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/24e4386131cdae6c78f360d9bf9d1396b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_4e4386131cdae6c78f360d9bf9d1396blepsky" style="display:none;border:1px dotted grey;">
This comprehensive reference work provides an overview of the concepts, methodologies, and applications in computational linguistics and natural language processing (NLP). Features contributions by the top researchers in the field, reflecting the work that is driving the discipline forwardIncludes an introduction to the major theoretical issues in these fields, as well as the central engineering applications that the work has producedPresents the major developments in an accessible way, explaining the close connection between scientific understanding of the computational properties of natural language and the creation of effective language technologiesServes as an invaluable state-of-the-art reference source for computational linguists and software engineers developing NLP applications in industrial research and development labs of software companies
</div>
<div style="position:relative">						
	<div id="bib_4e4386131cdae6c78f360d9bf9d1396blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Building and evaluating domain ontologies : NLP contributions</b>.<br/>
2010. 
<br/>G Grigonytė.
<br/>
<a href="http://books.google.de/books?id=M8X1O5ips1gC">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'fe7c93b20e30a042562cc14648aab46b'); return false;" href="https://www.bibsonomy.org/bibtex/2fe7c93b20e30a042562cc14648aab46b/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'fe7c93b20e30a042562cc14648aab46b', 'https://www.bibsonomy.org/bibtex/2fe7c93b20e30a042562cc14648aab46b/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2fe7c93b20e30a042562cc14648aab46b/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_fe7c93b20e30a042562cc14648aab46blepsky" style="display:none;border:1px dotted grey;">
An ontology is a knowledge representation structure made up of concepts and their interrelations. It represents shared understanding delineated by some domain. The building of an ontology can be addressed from the perspective of natural language processing.This thesis discusses the validity and theoretical background of knowledge acquisition from natural language. It also presents the theoretical and experimental framework for NLP-driven ontology building and evaluation tasks.
</div>
<div style="position:relative">						
	<div id="bib_fe7c93b20e30a042562cc14648aab46blepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Comparison of Different lemmatization approaches through the means of information retrieval performance</b>.<br/>
In: 
P. Sojka, A. Horák, I. Kopeček and K. Pala, editors, 
<i>Text, Speech and Dialogue</i>, pages 93-100.
Springer, Berlin ; Heidelberg, 2010.

<br/>
Jakub Kanis and Lucie Skorkovská.
<br/>

<a href="http://dx.doi.org/10.1007/978-3-642-15760-8_13">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '50c037e802a5564b87d5753f5d1ce2b5'); return false;" href="https://www.bibsonomy.org/bibtex/250c037e802a5564b87d5753f5d1ce2b5/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '50c037e802a5564b87d5753f5d1ce2b5', 'https://www.bibsonomy.org/bibtex/250c037e802a5564b87d5753f5d1ce2b5/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/250c037e802a5564b87d5753f5d1ce2b5/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_50c037e802a5564b87d5753f5d1ce2b5lepsky" style="display:none;border:1px dotted grey;">
This paper presents a quantitative performance analysis of two different approaches to the lemmatization of the Czech text data. The first one is based on manually prepared dictionary of lemmas and set of derivation rules while the second one is based on automatic inference of the dictionary and the rules from training data. The comparison is done by evaluating the mean Generalized Average Precision (mGAP) measure of the lemmatized documents and search queries in the set of information retrieval (IR) experiments. Such method is suitable for efficient and rather reliable comparison of the lemmatization performance since a correct lemmatization has proven to be crucial for IR effectiveness in highly inflected languages. Moreover, the proposed indirect comparison of the lemmatizers circumvents the need for manually lemmatized test data which are hard to obtain and also face the problem of incompatible sets of lemmas across different systems.
</div>
<div style="position:relative">						
	<div id="bib_50c037e802a5564b87d5753f5d1ce2b5lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Computerlinguistik und Texttechnologie</b>.<br/>
2010. 
<br/>Henning Lobin.
<br/>

<a onclick="toggleAbstract('lepsky', 'e24b9b3fa56aefda63911dbbc0c0cfe9'); return false;" href="https://www.bibsonomy.org/bibtex/2e24b9b3fa56aefda63911dbbc0c0cfe9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'e24b9b3fa56aefda63911dbbc0c0cfe9', 'https://www.bibsonomy.org/bibtex/2e24b9b3fa56aefda63911dbbc0c0cfe9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e24b9b3fa56aefda63911dbbc0c0cfe9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e24b9b3fa56aefda63911dbbc0c0cfe9lepsky" style="display:none;border:1px dotted grey;">
Computerlinguistik (die Verarbeitung von Sprache mit dem Computer) und Texttechnologie (die automatisierte Handhabung elektronischer Texte) haben im letzten Jahrzehnt unterschiedliche Richtungen eingeschlagen. Beide Disziplinen speisen sich jedoch aus der gleichen Quelle: der formalen Grammatik. Deshalb ist eine gemeinsame Darstellung sinnvoll. Der Bezug auf die gemeinsamen Grundlagen und die kontrastierende Gegenüberstellung einzelner Teilbereiche fördern das Verständnis der jeweils anderen Disziplin und eröffnen interessante Querbezüge. Erstmals wird die Verknüpfung von Computerlinguistik und Texttechnologie mit dieser Einführung in knapper Form systematisch vollzogen, was sie insbesondere für Module im Bachelor-Studium geeignet macht.
</div>
<div style="position:relative">						
	<div id="bib_e24b9b3fa56aefda63911dbbc0c0cfe9lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2009" class="bibsonomy_quicknav_group"><a name="2009">2009</a></h3>
<div style="margin-bottom:1em"><b>Natural language processing with Python : [analyzing text with the natural language toolkit]</b>.<br/>
2009. 

<br/>


<a onclick="toggleBibtex('lepsky', 'c75b0f4f6de1d519807747688144710c', 'https://www.bibsonomy.org/bibtex/2c75b0f4f6de1d519807747688144710c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c75b0f4f6de1d519807747688144710c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c75b0f4f6de1d519807747688144710clepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_c75b0f4f6de1d519807747688144710clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Speech and Language Processing : an introduction to natural language processing, computational linguistics, and speech recognition</b>.<br/>
2009. 
<br/>Daniel Jurafsky and James H. Martin.
<br/>
<a href="http://books.google.de/books?id=crxYPgAACAAJ">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '602f1d1e3eb43f7c535b85ff938f494d'); return false;" href="https://www.bibsonomy.org/bibtex/2602f1d1e3eb43f7c535b85ff938f494d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '602f1d1e3eb43f7c535b85ff938f494d', 'https://www.bibsonomy.org/bibtex/2602f1d1e3eb43f7c535b85ff938f494d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2602f1d1e3eb43f7c535b85ff938f494d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_602f1d1e3eb43f7c535b85ff938f494dlepsky" style="display:none;border:1px dotted grey;">
For undergraduate or advanced undergraduate courses in Classical Natural Language Processing, Statistical Natural Language Processing, Speech Recognition, Computational Linguistics, and Human Language Processing. An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology at all levels and with all modern technologies this text takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. The authors cover areas that traditionally are taught in different courses, to describe a unified vision of speech and language processing. Emphasis is on practical applications and scientific evaluation. An accompanying Website contains teaching materials for instructors, with pointers to language processing resources on the Web. The Second Edition offers a significant amount of new and extended material.
</div>
<div style="position:relative">						
	<div id="bib_602f1d1e3eb43f7c535b85ff938f494dlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Language resources, taxonomies and metadata</b>. <br/>
, 2009.

<br/>
Lothar Lemnitzer, Erhard Hinrichs and Andreas Witt.
<br/>

<a onclick="toggleAbstract('lepsky', '797a21191688dfadbab7a436aacf5bfc'); return false;" href="https://www.bibsonomy.org/bibtex/2797a21191688dfadbab7a436aacf5bfc/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '797a21191688dfadbab7a436aacf5bfc', 'https://www.bibsonomy.org/bibtex/2797a21191688dfadbab7a436aacf5bfc/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2797a21191688dfadbab7a436aacf5bfc/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_797a21191688dfadbab7a436aacf5bfclepsky" style="display:none;border:1px dotted grey;">
Paper presented at: Text Mining and Services, Leipzig, März 2009
</div>
<div style="position:relative">						
	<div id="bib_797a21191688dfadbab7a436aacf5bfclepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2008" class="bibsonomy_quicknav_group"><a name="2008">2008</a></h3>
<div style="margin-bottom:1em">
<b>A joint topic and perspective model for ideological discourse</b>. <br/>
In: W. Daelemans, B. Goethals and K. Morik, editors, <i>Machine Learning and Knowledge Discovery in Databases</i>, series Lecture Notes in Computer Science, pages 17-32.
Springer Berlin Heidelberg, 2008.

<br/>
Wei-Hao Lin, Eric Xing and Alexander Hauptmann.
<br/>


<a onclick="toggleAbstract('lepsky', 'b55b1a101ef72b76efc04648ae86fa47'); return false;" href="https://www.bibsonomy.org/bibtex/2b55b1a101ef72b76efc04648ae86fa47/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b55b1a101ef72b76efc04648ae86fa47', 'https://www.bibsonomy.org/bibtex/2b55b1a101ef72b76efc04648ae86fa47/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b55b1a101ef72b76efc04648ae86fa47/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b55b1a101ef72b76efc04648ae86fa47lepsky" style="display:none;border:1px dotted grey;">
Polarizing discussions on political and social issues are common in mass and user-generated media. However, computer-based understanding of ideological discourse has been considered too difficult to undertake. In this paper we propose a statistical model for ideology discourse. By ideology we mean “a set of general beliefs socially shared by a group of people.” For example, Democratic and Republican are two major political ideologies in the United States. The proposed model captures lexical variations due to an ideological text’s topic and due to an author or speaker’s ideological perspective. To cope with the non-conjugacy of the logistic-normal prior we derive a variational inference algorithm for the model. We evaluate the proposed model on synthetic data as well as a written and a spoken political discourse. Experimental results strongly support that ideological perspectives are reflected in lexical variations.
</div>
<div style="position:relative">						
	<div id="bib_b55b1a101ef72b76efc04648ae86fa47lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Natural language processing and information retrieval</b>.<br/>
2008. 
<br/>Tanveer Siddiqui and US Tiwary.
<br/>

<a onclick="toggleAbstract('lepsky', '7ea8ab07ee13a3600f6ee5b259a9ab1d'); return false;" href="https://www.bibsonomy.org/bibtex/27ea8ab07ee13a3600f6ee5b259a9ab1d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '7ea8ab07ee13a3600f6ee5b259a9ab1d', 'https://www.bibsonomy.org/bibtex/27ea8ab07ee13a3600f6ee5b259a9ab1d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/27ea8ab07ee13a3600f6ee5b259a9ab1d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_7ea8ab07ee13a3600f6ee5b259a9ab1dlepsky" style="display:none;border:1px dotted grey;">
Natural Language Processing and Information Retrieval is a textbook designed to meet the requirements of engineering students pursuing undergraduate and postgraduate programs in computer science and information technology. The book attempts to bridge the gap between theory and practice and would also serve as a useful reference for professionals and researchers working on language-related projects.Integrating two rapidly developing and popular research fields of language processing and information retrieval, the book provides an extensive coverage of various concepts and widely used techniques in these areas. The text includes topics such as language modeling, lexical analysis, computational modeling, grammar and parsing, and semantic as well as knowledge-based analysis. The statistical and semantic approaches are explained with examples from Hindi, English, and Urdu. Besides presenting traditional applications of machine translation and natural language generation, the book discusses recent trends and practices of information retrieval, text summarization, and information extraction in sufficient detail.Written in easy-to-understand and student-friendly style, the textbook also provides ample practical applications based on hands-on research experience wherever appropriate.Uma Shanker Tiwary, currently a faculty member at International Institute of Information Technology, Allahabad, has over 20 years of experience in teaching and research. A PhD in electronics from BHU, Varanasi, he has published several research papers in conference proceedings as well as journals of national and international repute. During the course of his career, Dr Tiwary has undertaken teaching assignments at Gwangju Institute of Science and Technology (GIST), South Korea as well as a number of research projects sponsored by agencies such as MHRD and AICTE.Tanveer Siddiqui, also a faculty member at International Institute of Information Technology, Allahabad, did her PhD in intelligent techniques for effective information retrieval and has over eight years of teaching experience.
</div>
<div style="position:relative">						
	<div id="bib_7ea8ab07ee13a3600f6ee5b259a9ab1dlepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2007" class="bibsonomy_quicknav_group"><a name="2007">2007</a></h3>
<div style="margin-bottom:1em"><b>Natural language processing for online applications : text retrieval, extraction and categorization</b>.<br/>
2007. 
<br/>Peter Jackson and Isabelle Moulinier.
<br/>

<a onclick="toggleAbstract('lepsky', 'ac6fc2247e9a31aef8e690160277e956'); return false;" href="https://www.bibsonomy.org/bibtex/2ac6fc2247e9a31aef8e690160277e956/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'ac6fc2247e9a31aef8e690160277e956', 'https://www.bibsonomy.org/bibtex/2ac6fc2247e9a31aef8e690160277e956/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2ac6fc2247e9a31aef8e690160277e956/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_ac6fc2247e9a31aef8e690160277e956lepsky" style="display:none;border:1px dotted grey;">
This text covers the technologies of document retrieval, information extraction, and text categorization in a way which highlights commonalities in terms of both general principles and practical concerns. It assumes some mathematical background on the part of the reader, but the chapters typically begin with a non-mathematical account of the key issues. Current research topics are covered only to the extent that they are informing current applications; detailed coverage of longer term research and more theoretical treatments should be sought elsewhere. There are many pointers at the ends of the chapters that the reader can follow to explore the literature. However, the book does maintain a strong emphasis on evaluation in every chapter both in terms of methodology and the results of controlled experimentation.
</div>
<div style="position:relative">						
	<div id="bib_ac6fc2247e9a31aef8e690160277e956lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Computerlexikographie : eine Einführung</b>.<br/>
2007. 
<br/>Claudia Kunze and Lothar Lemnitzer.
<br/>


<a onclick="toggleBibtex('lepsky', 'bc24dd0dd803f59450e7ec937a17737f', 'https://www.bibsonomy.org/bibtex/2bc24dd0dd803f59450e7ec937a17737f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2bc24dd0dd803f59450e7ec937a17737f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_bc24dd0dd803f59450e7ec937a17737flepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_bc24dd0dd803f59450e7ec937a17737flepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2006" class="bibsonomy_quicknav_group"><a name="2006">2006</a></h3>
<div style="margin-bottom:1em"><b>Information und Sprache : Beiträge zu Informationswissenschaft, Computerlinguistik, Bibliothekswesen und verwandten Fächern ; Festschrift für Harald H. Zimmermann</b>.<br/>
2006. 

<br/>


<a onclick="toggleBibtex('lepsky', '89b6c6baf98144fe4f25e212033819fc', 'https://www.bibsonomy.org/bibtex/289b6c6baf98144fe4f25e212033819fc/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/289b6c6baf98144fe4f25e212033819fc/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_89b6c6baf98144fe4f25e212033819fclepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_89b6c6baf98144fe4f25e212033819fclepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Konzeption und prototypische Realisierung einer begriffsbasierten Texterschließung</b>.
<br/>
PhD thesis, Universität Trier, Trier, 2006.

<br/>
Sascha Lorenz.
<br/>
<a href="http://ubt.opus.hbz-nrw.de/volltexte/2006/377/pdf/LorenzSaschaDiss.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '81591f5127693d2d9a98f3f8edc421a9'); return false;" href="https://www.bibsonomy.org/bibtex/281591f5127693d2d9a98f3f8edc421a9/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '81591f5127693d2d9a98f3f8edc421a9', 'https://www.bibsonomy.org/bibtex/281591f5127693d2d9a98f3f8edc421a9/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/281591f5127693d2d9a98f3f8edc421a9/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_81591f5127693d2d9a98f3f8edc421a9lepsky" style="display:none;border:1px dotted grey;">
Menschen kommunizieren zu einem großen Teil durch ihre Sprache. Durch deren Aufzeichnung als Texte lassen sich Kenntnisse, Fertigkeiten und somit Wissen übertragen. Der maschinelle Zugriff auf das in Texten explizit gemachte Wissen stellt trotz der Unterstützung durch Rechentechnik einen zeit- und damit kostenintensiven Prozess dar, dem große wirtschaftliche Bedeutung zukommt [KöRe98;TsiLa02]. Der sich diesem Problem widmende Forschungszweig des Information Retrieval (IR) hat eine Vielzahl von Methoden hervorgebracht oder adaptiert, um dieser Herausforderung zu begegnen. Die Problematik der fehlenden inhaltlichen Erschließung mit Hilfe maschineller Verfahren besteht jedoch weiterhin. Bestehende IR-Systeme zielen auf die Unterstützung des Wiederfindens von Dokumenten oder Teilen davon. Sie orientieren sich an einzelnen, als Indexterme bezeichneten Worten. Diese werden entweder manuell einem Text zugeordnet oder maschinell auf Grund statistischer Maße ermittelt. Eine Suchanfrage liefert dann Verweise auf diejenigen Dokumente, denen diese Indexterme zugeordnet sind. Es werden also keine Informationen über den gesuchten Sachverhalt sondern lediglich Verweise darauf geliefert. Als Konsequenz bleibt es dem Nutzer überlassen festzustellen, ob und in welchem Ausmaß die gesuchten Informationen in den Dokumenten enthalten sind. Der Informationsbedarf wird folglich nur mittelbar befriedigt. Eine weitere Folge dieser am Wort orientierten Vorgehensweise ist, dass nicht bedeutungsbezogen gesucht werden kann. Da nicht bekannt ist, welchen Begriff ein Term symbolisiert, kann ein IR-System Fragen nach Personen, deren Funktionen, etc. nicht beantworten. Dies wird erst durch die als Information Extraction bezeichnete, inhaltliche Erschließung möglich. Deren Ziel ist es, unstrukturierte Daten aus Texten so zu strukturieren, dass ein gezielter Zugriff ermöglicht wird. Neben vielen Ansätzen innerhalb der Information Extraction, die sich sehr speziellen und tiefgehenden Detailfragen widmen, hat sich der Bereich der Erkennung von Eigennamen etabliert. Diese unter der Bezeichnung Named Entity Recognition zusammengefassten Vorgehensweisen zielen auf besonders häufig anzutreffende Namen wie die von Personen, Organisationen und Orten. Damit lassen sich ähnlich einer Datenbank auch direkte Anfragen nach diesen Eigennamen beantworten, die auf Grund ihrer Seltenheit kaum als Indexterme in Frage kämen. Allerdings zielen diese Verfahren nur auf echte Eigennamen oder durch äußerliche Regelmäßigkeiten eindeutig beschreibbare Größen wie beispielsweise Datumsangaben. Gattungsnamen oder Bezeichnungen abstrakter Sachverhalte werden nicht betrachtet. Allen diesen Vorgehensweisen gemeinsam ist deren Orientierung am Wort. Sie betrachten einen Text als eine Folge einzelner Worte, die mit einer bestimmten Wahrscheinlichkeit auftreten. Insbesondere die Erkennung von Eigennamen ist daher auf eine vorhergehende syntaktische Analyse der Texte angewiesen, weil dann auf Grund der ermittelten Wortarten auch auf seltenere und damit weniger wahrscheinliche Bezeichner geschlossen werden kann. Trotz der zusätzlichen syntaktischen Informationen werden große Mengen manuell aufbereiteter Trainingsdaten benötigt, um statistisch relevante Aussagen für maschinelle Lernverfahren treffen zu können. Daher findet die inhaltliche Erschließung im IR praktisch keine Verwendung. Stattdessen werden immer mehr Worte zur Indexierung benutzt, ohne damit jedoch Bedeutungen erschließen zu können. Der Grund für die Probleme der inhaltlichen Texterschließung ist die historisch gewachsene Orientierung am Wort. Dementsprechend verspricht der Übergang vom Wort zu dem dadurch bezeichneten Begriff ein Weg zur Überwindung dieser Schwierigkeiten zu sein. Basis dieser Überlegungen ist, dass Kommunikation in natürlicher Sprache einem Protokoll folgt, also strukturiert abläuft. Dieses findet sich in Form typischer Kommunikationsmuster auch in der geschriebenen Sprache wieder. Da diese Muster aus zueinander in Beziehung stehenden Begriffen bestehen, erlauben die resultierenden Begriffsnetze die Identifikation gesuchter Bedeutungen sowie die Erschließung begrifflicher Zusammenhänge.
</div>
<div style="position:relative">						
	<div id="bib_81591f5127693d2d9a98f3f8edc421a9lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2005" class="bibsonomy_quicknav_group"><a name="2005">2005</a></h3>
<div style="margin-bottom:1em"><b>The Oxford handbook of computational linguistics</b>.<br/>
2005. 
<br/>Ruslan Mitkov.
<br/>

<a onclick="toggleAbstract('lepsky', '26493726c2b1d2c774906962c1ebe9a6'); return false;" href="https://www.bibsonomy.org/bibtex/226493726c2b1d2c774906962c1ebe9a6/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '26493726c2b1d2c774906962c1ebe9a6', 'https://www.bibsonomy.org/bibtex/226493726c2b1d2c774906962c1ebe9a6/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/226493726c2b1d2c774906962c1ebe9a6/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_26493726c2b1d2c774906962c1ebe9a6lepsky" style="display:none;border:1px dotted grey;">
Thirty-eight chapters, commissioned from experts all over the world, describe major concepts, methods, and applications in computational linguistics. Part I, Linguistic Fundamentals, provides an overview of the field suitable for senior undergraduates and non-specialists from other fields of linguistics and related disciplines. Part II describes current tasks, techniques, and tools in Natural Language Processing and aims to meet the needs of post-doctoral workers and others embarking on computational language research. Part III surveys current Applications. The book is a state-of-the-art reference to one of the most active and productive fields in linguistics. It will be of interest and practical use to a wide range of linguists, as well as to researchers in such fields as informatics, artificial intelligence, language engineering, and cognitive science.
</div>
<div style="position:relative">						
	<div id="bib_26493726c2b1d2c774906962c1ebe9a6lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2004" class="bibsonomy_quicknav_group"><a name="2004">2004</a></h3>
<div style="margin-bottom:1em">
<b>A resource-light approach to Russian morphology : tagging Russian using Czech resources.</b>. <br/>
In: <i>EMNLP</i>, pages 222-229.
2004.

<br/>
Jiri Hana, Anna Feldman and Chris Brew.
<br/>

<a href="http://www.aclweb.org/old_anthology/W/W04/W04-3229.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b28ebf154ae2d003b5fb6ea83a6c38f6'); return false;" href="https://www.bibsonomy.org/bibtex/2b28ebf154ae2d003b5fb6ea83a6c38f6/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b28ebf154ae2d003b5fb6ea83a6c38f6', 'https://www.bibsonomy.org/bibtex/2b28ebf154ae2d003b5fb6ea83a6c38f6/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b28ebf154ae2d003b5fb6ea83a6c38f6/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b28ebf154ae2d003b5fb6ea83a6c38f6lepsky" style="display:none;border:1px dotted grey;">
In this paper, we describe a resource-light system for the automatic morphological analysis and tagging of Russian. We eschew the use of extensive resources (particularly, large annotated corpora and lexicons), exploiting instead (i) pre-existing annotated corpora of Czech; (ii) an unannotated corpus of Russian. We show that our approach has benefits, and present what we believe to be one of the first full evaluations of a Russian tagger in the openly available literature.
</div>
<div style="position:relative">						
	<div id="bib_b28ebf154ae2d003b5fb6ea83a6c38f6lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Integration of russian language resources</b>. <br/>
In: .
2004.

<br/>
Serge Yablonsky.
<br/>

<a href="http://hnk.ffzg.hr/bibl/lrec2004/pdf/682.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '898f40f0dbddddfbb6232daf70f6d376'); return false;" href="https://www.bibsonomy.org/bibtex/2898f40f0dbddddfbb6232daf70f6d376/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '898f40f0dbddddfbb6232daf70f6d376', 'https://www.bibsonomy.org/bibtex/2898f40f0dbddddfbb6232daf70f6d376/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2898f40f0dbddddfbb6232daf70f6d376/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_898f40f0dbddddfbb6232daf70f6d376lepsky" style="display:none;border:1px dotted grey;">
In this paper we describe the creation of large scale linguistic resources for Russian language. Internet/intranet system architecture was developed to make a large volume of Russian language lexical information, corpora (texts) and knowledge base (Russian WordNet) available to the system at development and/or run time. There are four linguistic counterparts, corresponding to the major categories of lexical information developed in our system: lexicon, knowledge base, corpora and Russian language processing software.
</div>
<div style="position:relative">						
	<div id="bib_898f40f0dbddddfbb6232daf70f6d376lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2003" class="bibsonomy_quicknav_group"><a name="2003">2003</a></h3>
<div style="margin-bottom:1em">
<b>Feature extraction languages for propositionalized relational learning</b>. <br/>
<i>IJCAI Workshop on Learning Statistical Models from Relational Data</i>, 2003.

<br/>
C Cumby and D Roth.
<br/>
<a href="http://cogcomp.cs.illinois.edu/papers/CumbyRo03a.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '631873abf74b2b2c1ae2d0c040dd1805'); return false;" href="https://www.bibsonomy.org/bibtex/2631873abf74b2b2c1ae2d0c040dd1805/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '631873abf74b2b2c1ae2d0c040dd1805', 'https://www.bibsonomy.org/bibtex/2631873abf74b2b2c1ae2d0c040dd1805/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2631873abf74b2b2c1ae2d0c040dd1805/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_631873abf74b2b2c1ae2d0c040dd1805lepsky" style="display:none;border:1px dotted grey;">
Kernel methods have gained a great deal of popularity in the machine learning community as a method to learn indirectly in highdimensional feature spaces. Those interested in relational learning have recently begun to cast learning from structured and relational data in terms of kernel operations. We describe a general family of kernel functions built up from a description language of limited expressivity and use it to study the benefits and drawbacks of kernel learning in relational domains. Learning with kernels in this family directly models learning over an expanded feature space constructed using the same description language. This allows us to examine issues of time complexity in terms of learning with these and other relational kernels, and how these relate to generalization ability. The tradeoffs between using kernels in a very high dimensional implicit space versus a restricted feature space, is highlighted through two experiments, in bioinformatics and in natural language processing.
</div>
<div style="position:relative">						
	<div id="bib_631873abf74b2b2c1ae2d0c040dd1805lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Background to Framenet</b>. <br/>
<i>International Journal of Lexicography</i>, 16(3):235-250, 2003.

<br/>
Charles Fillmore, Christopher Johnson and Miriam Petruck.
<br/>
<a href="http://dx.doi.org/10.1093/ijl/16.3.235">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '6e16e330198e715de9b7917e6c451b71'); return false;" href="https://www.bibsonomy.org/bibtex/26e16e330198e715de9b7917e6c451b71/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6e16e330198e715de9b7917e6c451b71', 'https://www.bibsonomy.org/bibtex/26e16e330198e715de9b7917e6c451b71/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26e16e330198e715de9b7917e6c451b71/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6e16e330198e715de9b7917e6c451b71lepsky" style="display:none;border:1px dotted grey;">
This article presents general background information about the FrameNet project, including an introduction to its basic assumptions and goals, a description of its precursors, and information about its evolution during the six years of the project. The companion articles in this special issue of IJL describe various aspects of the project in greater detail.
</div>
<div style="position:relative">						
	<div id="bib_6e16e330198e715de9b7917e6c451b71lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>"I'm sorry Dave, I'm afraid I can't do that" : linguistics, statistics, and natural language processing circa 2001</b>. <br/>
, 2003.

<br/>
Lillian Lee.
<br/>
<a href="http://arxiv.org/abs/cs/0304027">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '49261145d71f848ea68e05720c2bb881'); return false;" href="https://www.bibsonomy.org/bibtex/249261145d71f848ea68e05720c2bb881/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '49261145d71f848ea68e05720c2bb881', 'https://www.bibsonomy.org/bibtex/249261145d71f848ea68e05720c2bb881/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/249261145d71f848ea68e05720c2bb881/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_49261145d71f848ea68e05720c2bb881lepsky" style="display:none;border:1px dotted grey;">
A brief, general-audience overview of the history of natural language processing, focusing on data-driven approaches.Topics include "Ambiguity and language analysis", "Firth things first", "A 'C' change", and "The empiricists strike back".
</div>
<div style="position:relative">						
	<div id="bib_49261145d71f848ea68e05720c2bb881lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Phrasenet : towards context sensitive lexical semantics</b>. <br/>
In: .
2003.

<br/>
X Li, D Roth and Y Tu.
<br/>

<a href="http://cogcomp.cs.illinois.edu/papers/LiRoTu.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '38235477d28349c53ea5947f65e17423'); return false;" href="https://www.bibsonomy.org/bibtex/238235477d28349c53ea5947f65e17423/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '38235477d28349c53ea5947f65e17423', 'https://www.bibsonomy.org/bibtex/238235477d28349c53ea5947f65e17423/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/238235477d28349c53ea5947f65e17423/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_38235477d28349c53ea5947f65e17423lepsky" style="display:none;border:1px dotted grey;">
This paper introduces PhraseNet, a contextsensitive lexical semantic knowledge base system. Based on the supposition that semantic proximity is not simply a relation between two words in isolation, but rather a relation between them in their context, English nouns and verbs, along with contexts they appear in, are organized in PhraseNet into Consets; Consets capture the underlying lexical concept, and are connected with several semantic relations that respect contextually sensitive lexical information. PhraseNet makes use of WordNet as an important knowledge source. It enhances a WordNet synset with its contextual information and refines its relational structure by maintaining only those relations that respect contextual constraints. The contextual information allows for supporting more functionalities compared with those of WordNet. Natural language researchers as well as linguists and language learners can gain from accessing PhraseNet with a word token and its context, to retrieve relevant semantic information. We describe the design and construction of PhraseNet and give preliminary experimental evidence to its usefulness for NLP researches.
</div>
<div style="position:relative">						
	<div id="bib_38235477d28349c53ea5947f65e17423lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>The Oxford handbook of computational linguistics</b>.<br/>
2003. 

<br/>


<a onclick="toggleBibtex('lepsky', 'd96b8fa679fe730db1ad1e13301bd47c', 'https://www.bibsonomy.org/bibtex/2d96b8fa679fe730db1ad1e13301bd47c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d96b8fa679fe730db1ad1e13301bd47c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d96b8fa679fe730db1ad1e13301bd47clepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_d96b8fa679fe730db1ad1e13301bd47clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Russian morphology : resources and Java software applications</b>. <br/>
In: , pages 87-94.
Association for Computational Linguistics, 2003.

<br/>
Serge Yablonsky.
<br/>

<a href="http://portal.acm.org/citation.cfm?id=1613212">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '050c5f257c4cca93f73a9fe73523d52c'); return false;" href="https://www.bibsonomy.org/bibtex/2050c5f257c4cca93f73a9fe73523d52c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '050c5f257c4cca93f73a9fe73523d52c', 'https://www.bibsonomy.org/bibtex/2050c5f257c4cca93f73a9fe73523d52c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2050c5f257c4cca93f73a9fe73523d52c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_050c5f257c4cca93f73a9fe73523d52clepsky" style="display:none;border:1px dotted grey;">
This paper deals with development and application of Russian morphology software and resources. The approach is particularly dependent on advanced morphological analysis. The paper presents the structure, formats and content of Russian dictionaries and corpora. Relevant aspects of the UML data models, XML format and related technologies are surveyed. We introducee the system based on Java and Oracle 9i DBMS.
</div>
<div style="position:relative">						
	<div id="bib_050c5f257c4cca93f73a9fe73523d52clepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2002" class="bibsonomy_quicknav_group"><a name="2002">2002</a></h3>
<div style="margin-bottom:1em">
<b>Lexikonfreie Lemmatisierung für Substantive des Deutschen</b>. <br/>
In: .
2002.

<br/>
Tanja Becker and Esther König.
<br/>

<a href="http://konvens2002.dfki.de/cd/pdf/17V-Becker.pdf">[doi]</a>&nbsp;

<a onclick="toggleBibtex('lepsky', '2358fcd8e70d20df9dc561a8a7eb414c', 'https://www.bibsonomy.org/bibtex/22358fcd8e70d20df9dc561a8a7eb414c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22358fcd8e70d20df9dc561a8a7eb414c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2358fcd8e70d20df9dc561a8a7eb414clepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_2358fcd8e70d20df9dc561a8a7eb414clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Anmerkungen zur scheinbaren Konkurrenz von numerischen und symbolischen Verfahren in der Computerlinguistik</b>.<br/>
In: 

<i>Computerlinguistik: Was geht, was kommt? = Computational linguistics : achievements and perspectives : Festschrift für Winfried Lenders</i>, pages 163-171.
2002.

<br/>
Tibor Kiss.
<br/>

<a href="http://www.linguistics.ruhr-uni-bochum.de/~kiss/publications/fs_lenders.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '6af04f6d6ffe3b57c2e911c806dcba2d'); return false;" href="https://www.bibsonomy.org/bibtex/26af04f6d6ffe3b57c2e911c806dcba2d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6af04f6d6ffe3b57c2e911c806dcba2d', 'https://www.bibsonomy.org/bibtex/26af04f6d6ffe3b57c2e911c806dcba2d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26af04f6d6ffe3b57c2e911c806dcba2d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6af04f6d6ffe3b57c2e911c806dcba2dlepsky" style="display:none;border:1px dotted grey;">
Veränderungen in der Mode vollziehen sich schrittweise, allmählich, oftmals unmerklich. Manchmal kommt es dann doch zu einem plötzlichen Bruch, zu einer eindeutig bestimmten Opposition zwischen dem modischen Jetzt und seinem Vorgänger. Deutlich erkennbar etwa, als um die Wende von den 70er zu den 80er Jahren die weite Karotte die enge Schlaghose verdrängte und im Gefolge dieses Wechsels taillierte Sakkos und Hemden big suits und Polohemden weichen mussten. Etwas Ähnliches hat sich vor einigen Jahren in der Computerlinguistik ereignet: Wurde diese seit Beginn der 80er Jahre durch deduktive, regelbasierte Verfahren beherrscht, so gab es seit Beginn der 90er Jahre zunächst eine Hinwendung, schließlich Mitte der 90er Jahre eine Flucht zu induktiven, numerisch basierten Verfahren, die mittlerweile die Computerlinguistik nahezu vollständig zu beherrschen scheinen.
</div>
<div style="position:relative">						
	<div id="bib_6af04f6d6ffe3b57c2e911c806dcba2dlepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Computerlinguistik : was geht, was kommt? ; Festschrift für Winfried Lenders</b>.<br/>
2002. 

<br/>


<a onclick="toggleBibtex('lepsky', '99d37f7a33dbb2246378c2e1b5f94a59', 'https://www.bibsonomy.org/bibtex/299d37f7a33dbb2246378c2e1b5f94a59/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/299d37f7a33dbb2246378c2e1b5f94a59/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_99d37f7a33dbb2246378c2e1b5f94a59lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_99d37f7a33dbb2246378c2e1b5f94a59lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-2000" class="bibsonomy_quicknav_group"><a name="2000">2000</a></h3>
<div style="margin-bottom:1em"><b>Handbook of natural language processing</b>.<br/>
2000. 
<br/>Robert Dale, Hermann Moisl and HL Somers.
<br/>

<a onclick="toggleAbstract('lepsky', 'c9424d4aed4fc18a538124799cfe6bc0'); return false;" href="https://www.bibsonomy.org/bibtex/2c9424d4aed4fc18a538124799cfe6bc0/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'c9424d4aed4fc18a538124799cfe6bc0', 'https://www.bibsonomy.org/bibtex/2c9424d4aed4fc18a538124799cfe6bc0/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2c9424d4aed4fc18a538124799cfe6bc0/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_c9424d4aed4fc18a538124799cfe6bc0lepsky" style="display:none;border:1px dotted grey;">
Annotation Contributors split nearly equally between scholars of linguistics and of technical matters such as computer science and information discuss recent developments in designing and implementing computational machinery that communicates with humans using natural language. Emphasizing practical tools and techniques and minimizing speculation and polemic, they cover symbolic approaches that have their origins in generative linguistics, approaches based on empirical corpus analysis, and artificial neural network approaches. Among the topics are discourse structure and intentional recognition, generating multimedia presentations, creating a corpus for data-intensive linguistics, example-based machine translation, character recognition with syntactic neural networks, knowledge representation, and text data mining. Annotation c. Book News, Inc., Portland, OR (booknews.com).
</div>
<div style="position:relative">						
	<div id="bib_c9424d4aed4fc18a538124799cfe6bc0lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Morphological tagging : data vs. dictionaries</b>. <br/>
In: , pages 94-101.
Association for Computational Linguistics, 2000.

<br/>
Jan Hajič.
<br/>

<a href="http://portal.acm.org/citation.cfm?id=974318">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '6f7f4fd56dfb579714b3fc556f5923ef'); return false;" href="https://www.bibsonomy.org/bibtex/26f7f4fd56dfb579714b3fc556f5923ef/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '6f7f4fd56dfb579714b3fc556f5923ef', 'https://www.bibsonomy.org/bibtex/26f7f4fd56dfb579714b3fc556f5923ef/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26f7f4fd56dfb579714b3fc556f5923ef/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6f7f4fd56dfb579714b3fc556f5923eflepsky" style="display:none;border:1px dotted grey;">
Part of Speech tagging for English seems to have reached the human levels of error, but full morphological tagging for inflectionally rich languages, such as Romanian, Czech, or Hungarian, is still an open problem, and the results are far from being satisfactory. This paper presents results obtained by using a universalized exponential feature-based model for five such languages. It focuses on the data sparseness issue, which is especially severe for such languages (the more so that there are no extensive annotated data for those languages). In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances.
</div>
<div style="position:relative">						
	<div id="bib_6f7f4fd56dfb579714b3fc556f5923eflepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Grundlagen der Computerlinguistik : Mensch-Maschine-Kommunikation in natürlicher Sprache</b>.<br/>
2000. 
<br/>Roland Hausser.
<br/>

<a onclick="toggleAbstract('lepsky', '1781cc3cd0be2d6d6b0f3002ae0433b0'); return false;" href="https://www.bibsonomy.org/bibtex/21781cc3cd0be2d6d6b0f3002ae0433b0/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '1781cc3cd0be2d6d6b0f3002ae0433b0', 'https://www.bibsonomy.org/bibtex/21781cc3cd0be2d6d6b0f3002ae0433b0/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/21781cc3cd0be2d6d6b0f3002ae0433b0/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_1781cc3cd0be2d6d6b0f3002ae0433b0lepsky" style="display:none;border:1px dotted grey;">
Die zentrale Aufgabe einer zukunftsorientierten Computerlinguistik ist die Entwicklung kognitiver Maschinen, mit denen Menschen in ihrer jeweiligen Sprache frei reden können. Langfristig umfaßt diese Zielsetzung eine funktional ausgerichtete Theoriebildung, eine objektive Verifikationsmethode und eine Fülle praktischer Anwendungen. Für die natürlichsprachliche Kommunikation wird nicht nur Sprachverarbeitung, sondern auch nichtsprachliche Wahrnehmung und Handlung benötigt. Deshalb ist der Inhalt dieses Lehrbuchs als Sprachtheorie für die Konstruktion sprechender Roboter organisiert. Sein zentrales Thema ist die Kommunikationsmechanik natürlicher Sprachen - beim Sprecher und beim Hörer. Der Inhalt ist in folgende vier Teile mit je sechs Kapiteln gegliedert: Sprachtheorie; Formale Grammatik; Morphologie und Syntax; Semantik und Pragmatik. Insgesamt 772 Übungsaufgaben dienen der Verständniskontrolle und -vertiefung.
</div>
<div style="position:relative">						
	<div id="bib_1781cc3cd0be2d6d6b0f3002ae0433b0lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1999" class="bibsonomy_quicknav_group"><a name="1999">1999</a></h3>
<div style="margin-bottom:1em"><b>Foundations of statistical natural language processing</b>.<br/>
1999. 
<br/>Christopher Manning and Hinrich Schütze.
<br/>

<a onclick="toggleAbstract('lepsky', 'd2cba020df503d0b5d6f1ca22092b1ba'); return false;" href="https://www.bibsonomy.org/bibtex/2d2cba020df503d0b5d6f1ca22092b1ba/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd2cba020df503d0b5d6f1ca22092b1ba', 'https://www.bibsonomy.org/bibtex/2d2cba020df503d0b5d6f1ca22092b1ba/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d2cba020df503d0b5d6f1ca22092b1ba/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d2cba020df503d0b5d6f1ca22092b1balepsky" style="display:none;border:1px dotted grey;">
Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.
</div>
<div style="position:relative">						
	<div id="bib_d2cba020df503d0b5d6f1ca22092b1balepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Translation engines : techniques for machine translation</b>.<br/>
1999. 
<br/>Arturo Trujillo.
<br/>

<a onclick="toggleAbstract('lepsky', '814a1f83ab603ee8186ff1356f8d307c'); return false;" href="https://www.bibsonomy.org/bibtex/2814a1f83ab603ee8186ff1356f8d307c/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '814a1f83ab603ee8186ff1356f8d307c', 'https://www.bibsonomy.org/bibtex/2814a1f83ab603ee8186ff1356f8d307c/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2814a1f83ab603ee8186ff1356f8d307c/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_814a1f83ab603ee8186ff1356f8d307clepsky" style="display:none;border:1px dotted grey;">
Machine translation (MT) is the area of computer science and applied linguistics dealing with the translation of human languages such as English and German. MT on the Internet has become an important tool by providing fast, economical and useful translations. With globalisation and expanding trade, demand for translation is set to grow. Translation Engines covers theoretical and practical aspects of MT, both classic and new, including: - Character sets and formatting languages - Translation memory - Linguistic and computational foundations - Basic computational linguistic techniques - Transfer and interlingua MT - Evaluation Software accompanies the text, providing readers with hands on experience of the main algorithms.
</div>
<div style="position:relative">						
	<div id="bib_814a1f83ab603ee8186ff1356f8d307clepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Russian morphological analysis</b>. <br/>
In: , pages 83-90.
1999.

<br/>
Serge Yablonsky.
<br/>

<a href="http://project.cgm.unive.it/events/papers/yablonski.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'b03078db036b498bea6e2f6f1d148605'); return false;" href="https://www.bibsonomy.org/bibtex/2b03078db036b498bea6e2f6f1d148605/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'b03078db036b498bea6e2f6f1d148605', 'https://www.bibsonomy.org/bibtex/2b03078db036b498bea6e2f6f1d148605/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2b03078db036b498bea6e2f6f1d148605/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_b03078db036b498bea6e2f6f1d148605lepsky" style="display:none;border:1px dotted grey;">
In this paper the approach to the organization of Russian inflexion morphologic model and its application for the Russian language morphological analysis and disambiguation are described. We are concerned with the pos tagging of 150-million-word Russian corpora. The approach is particularly dependent on the language processor Russicon, and on wide usage of Russicon's electronic dictionaries.
</div>
<div style="position:relative">						
	<div id="bib_b03078db036b498bea6e2f6f1d148605lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1997" class="bibsonomy_quicknav_group"><a name="1997">1997</a></h3>
<div style="margin-bottom:1em"><b>Survey of the state of the art in human language technology</b>.<br/>
1997. 

<br/>
<a href="https://books.google.de/books?id=WlHD141lKw4C">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'a8fceecbe6aca0cd117b5428f57ae6dc'); return false;" href="https://www.bibsonomy.org/bibtex/2a8fceecbe6aca0cd117b5428f57ae6dc/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'a8fceecbe6aca0cd117b5428f57ae6dc', 'https://www.bibsonomy.org/bibtex/2a8fceecbe6aca0cd117b5428f57ae6dc/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2a8fceecbe6aca0cd117b5428f57ae6dc/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_a8fceecbe6aca0cd117b5428f57ae6dclepsky" style="display:none;border:1px dotted grey;">
Languages, in all their forms, are the more efficient and natural means for people to communicate. Enormous quantities of information are produced, distributed and consumed using languages. Human language technology's main purpose is to allow the use of automatic systems and tools to assist humans in producing and accessing information, to improve communication between humans, and to assist humans in communicating with machines. This book, sponsored by the Directorate General XIII of the European Union and the Information Science and Engineering Directorate of the National Science Foundation, USA, offers the first comprehensive overview of the human language technology field.
</div>
<div style="position:relative">						
	<div id="bib_a8fceecbe6aca0cd117b5428f57ae6dclepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1996" class="bibsonomy_quicknav_group"><a name="1996">1996</a></h3>
<div style="margin-bottom:1em">
<b>A morphology-system and part-of-speech tagger for German</b>. <br/>
<i>arXiv:cmp-lg/9610006</i>, 1996.
arXiv: cmp-lg/9610006
<br/>
Wolfgang Lezius, Reinhard Rapp and Manfred Wettler.
<br/>
<a href="http://arxiv.org/abs/cmp-lg/9610006">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '22b9a0a8b02b0bbf1ccfc3e0f927840d'); return false;" href="https://www.bibsonomy.org/bibtex/222b9a0a8b02b0bbf1ccfc3e0f927840d/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '22b9a0a8b02b0bbf1ccfc3e0f927840d', 'https://www.bibsonomy.org/bibtex/222b9a0a8b02b0bbf1ccfc3e0f927840d/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/222b9a0a8b02b0bbf1ccfc3e0f927840d/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_22b9a0a8b02b0bbf1ccfc3e0f927840dlepsky" style="display:none;border:1px dotted grey;">
This paper presents an integrated tool for German morphology and statistical part-of-speech tagging which aims at making some well established methods widely available. The software is very user friendly, runs on any PC and can be downloaded as a complete package (including lexicon and documentation) from the World Wide Web. Compared with the performance of other tagging systems the tagger produces similar results.
</div>
<div style="position:relative">						
	<div id="bib_22b9a0a8b02b0bbf1ccfc3e0f927840dlepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1995" class="bibsonomy_quicknav_group"><a name="1995">1995</a></h3>
<div style="margin-bottom:1em"><b>Natural language understanding</b>.<br/>
1995. 
<br/>James Allen.
<br/>

<a onclick="toggleAbstract('lepsky', '0aa22ffd60c35b82e3c7edbd48c5bcb3'); return false;" href="https://www.bibsonomy.org/bibtex/20aa22ffd60c35b82e3c7edbd48c5bcb3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '0aa22ffd60c35b82e3c7edbd48c5bcb3', 'https://www.bibsonomy.org/bibtex/20aa22ffd60c35b82e3c7edbd48c5bcb3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/20aa22ffd60c35b82e3c7edbd48c5bcb3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_0aa22ffd60c35b82e3c7edbd48c5bcb3lepsky" style="display:none;border:1px dotted grey;">
This long-awaited revision offers a comprehensive introduction to natural language understanding with developments and research in the field today. Building on the effective framework of the first edition, the new edition gives the same balanced coverage of syntax, semantics, and discourse, and offers a uniform framework based on feature-based context-free grammars and chart parsers used for syntactic and semantic processing. Thorough treatment of issues in discourse and context-dependent interpretation is also provided. In addition, this title offers coverage of two entirely new subject areas. First, the text features a new chapter on statistically-based methods using large corpora. Second, it includes an appendix on speech recognition and spoken language understanding. Also, the information on semantics that was covered in the first edition has been largely expanded in this edition to include an emphasis on compositional interpretation.
</div>
<div style="position:relative">						
	<div id="bib_0aa22ffd60c35b82e3c7edbd48c5bcb3lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Russian morphology : an engineering approach</b>. <br/>
<i>Natural Language Engineering</i>, 1(03):235-260, 1995.

<br/>
Andrei Mikheev and Liubov Liubushkina.
<br/>
<a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1279148&amp;fileId=S135132490000019X">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '46bc1e56f3a3dedd36ec8423ea3293eb'); return false;" href="https://www.bibsonomy.org/bibtex/246bc1e56f3a3dedd36ec8423ea3293eb/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '46bc1e56f3a3dedd36ec8423ea3293eb', 'https://www.bibsonomy.org/bibtex/246bc1e56f3a3dedd36ec8423ea3293eb/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/246bc1e56f3a3dedd36ec8423ea3293eb/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_46bc1e56f3a3dedd36ec8423ea3293eblepsky" style="display:none;border:1px dotted grey;">
Morphological analysis, which is at the heart of the processing of natural language requires computationally effective morphological processors. In this paper an approach to the organization of an inflectional morphological model and its application for the Russian language are described. The main objective of our morphological processor is not the classification of word constituents, but rather an efficient computational recognition of morpho-syntactic features of words and the generation of words according to requested morpho-syntactic features. Another major concern that the processor aims to address is the ease of extending the lexicon. The templated word-paradigm model used in the system has an engineering flavour: paradigm formation rules are of a bottom-up (word specific) nature rather than general observations about the language, and word formation units are segments of words rather than proper morphemes. This approach allows us to handle uniformly both general cases and exceptions, and requires extremely simple data structures and control mechanisms which can be easily implemented as a finite-state automata. The morphological processor described in this paper is fully implemented for a substantial subset of Russian (more then 1,500,000 word-tokens – 95,000 word paradigms) and provides an extensive list of morpho-syntactic features together with stress positions for words utilized in its lexicon. Special dictionary management tools were built for browsing, debugging and extension of the lexicon. The actual implementation was done in C and C++, and the system is available for the MS-DOS, MS-Windows and UNIX platforms.
</div>
<div style="position:relative">						
	<div id="bib_46bc1e56f3a3dedd36ec8423ea3293eblepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em">
<b>Probabilistic part-of-speech tagging using decision trees</b>. <br/>
, 1995.

<br/>
Helmut Schmid.
<br/>


<a onclick="toggleBibtex('lepsky', '15dea522cd0f9b08a245f3086b8c7309', 'https://www.bibsonomy.org/bibtex/215dea522cd0f9b08a245f3086b8c7309/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/215dea522cd0f9b08a245f3086b8c7309/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_15dea522cd0f9b08a245f3086b8c7309lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_15dea522cd0f9b08a245f3086b8c7309lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1994" class="bibsonomy_quicknav_group"><a name="1994">1994</a></h3>
<div style="margin-bottom:1em">
<b>Verb semantics and lexical selection</b>. <br/>
In: , pages 133-138.
Association for Computational Linguistics, 1994.

<br/>
Zhibiao Wu and Martha Palmer.
<br/>

<a href="http://dl.acm.org/citation.cfm?id=981751">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', '229a52ab90ba89509ed5c34abc44390f'); return false;" href="https://www.bibsonomy.org/bibtex/2229a52ab90ba89509ed5c34abc44390f/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '229a52ab90ba89509ed5c34abc44390f', 'https://www.bibsonomy.org/bibtex/2229a52ab90ba89509ed5c34abc44390f/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2229a52ab90ba89509ed5c34abc44390f/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_229a52ab90ba89509ed5c34abc44390flepsky" style="display:none;border:1px dotted grey;">
This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentences as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.
</div>
<div style="position:relative">						
	<div id="bib_229a52ab90ba89509ed5c34abc44390flepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1992" class="bibsonomy_quicknav_group"><a name="1992">1992</a></h3>
<div style="margin-bottom:1em">
<b>Class-based n-gram models of natural language</b>. <br/>
<i>Comput. Linguist.</i>, 18(4):467-479, 1992.

<br/>
Peter Brown, Peter deSouza, Robert Mercer, Vincent Pietra and Jenifer Lai.
<br/>
<a href="http://dl.acm.org/citation.cfm?id=176313.176316">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd63f17c069884cec194a920170d6dcdf'); return false;" href="https://www.bibsonomy.org/bibtex/2d63f17c069884cec194a920170d6dcdf/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd63f17c069884cec194a920170d6dcdf', 'https://www.bibsonomy.org/bibtex/2d63f17c069884cec194a920170d6dcdf/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d63f17c069884cec194a920170d6dcdf/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d63f17c069884cec194a920170d6dcdflepsky" style="display:none;border:1px dotted grey;">
We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
</div>
<div style="position:relative">						
	<div id="bib_d63f17c069884cec194a920170d6dcdflepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Morphology and computation</b>.<br/>
1992. 
<br/>Richard Sproat.
<br/>

<a onclick="toggleAbstract('lepsky', '341257075dc6f579b0fe453b5894dcb3'); return false;" href="https://www.bibsonomy.org/bibtex/2341257075dc6f579b0fe453b5894dcb3/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', '341257075dc6f579b0fe453b5894dcb3', 'https://www.bibsonomy.org/bibtex/2341257075dc6f579b0fe453b5894dcb3/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2341257075dc6f579b0fe453b5894dcb3/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_341257075dc6f579b0fe453b5894dcb3lepsky" style="display:none;border:1px dotted grey;">
This book provides the first broad yet thorough coverage of issues in morphological theory. It includes a wide array of techniques and systems in computational morphology (including discussion of their limitations), and describes some unusual applications.Sproat motivates the study of computational morphology by arguing that a computational natural language system, such as a parser or a generator, must incorporate a model of morphology. He discusses a range of applications for programs with knowledge of morphology, some of which are not generally found in the literature. Sproat then provides an overview of some of the basic descriptive facts about morphology and issues in theoretical morphology and (lexical) phonology, as well as psycholinguistic evidence for human processing of morphological structure. He take up the basic techniques that have been proposed for doing morphological processing and discusses at length various systems (such as DECOMP and KIMMO) that incorporate part or all of those techniques, pointing out the inadequacies of such systems from both a descriptive and a computational point of view. He concludes by touching on interesting peripheral areas such as the analysis of complex nominals in English, and on the main contributions of Rumelhart and McClelland's connectionism to the computational analysis of words.Richard Sproat is Member of the Technical Staff at the AT&amp;TBell Laboratories.
</div>
<div style="position:relative">						
	<div id="bib_341257075dc6f579b0fe453b5894dcb3lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1989" class="bibsonomy_quicknav_group"><a name="1989">1989</a></h3>
<div style="margin-bottom:1em"><b>Die Verwendung hierarchisch strukturierter Sprachnetzwerke zur redundanzarmen Codierung von Texten</b>.
<br/>
PhD thesis, Technische Hochschule Darmstadt, Darmstadt, 1989.

<br/>
Jochen Meyer.
<br/>


<a onclick="toggleBibtex('lepsky', '79fcf9fc22c79176709327e82c2007ed', 'https://www.bibsonomy.org/bibtex/279fcf9fc22c79176709327e82c2007ed/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/279fcf9fc22c79176709327e82c2007ed/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_79fcf9fc22c79176709327e82c2007edlepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_79fcf9fc22c79176709327e82c2007edlepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1986" class="bibsonomy_quicknav_group"><a name="1986">1986</a></h3>
<div style="margin-bottom:1em"><b>Computational linguistics : an introduction</b>.<br/>
1986. 
<br/>Ralph Grishman.
<br/>


<a onclick="toggleBibtex('lepsky', '2cddd9bcd7975c62185966372507455a', 'https://www.bibsonomy.org/bibtex/22cddd9bcd7975c62185966372507455a/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/22cddd9bcd7975c62185966372507455a/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_2cddd9bcd7975c62185966372507455alepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_2cddd9bcd7975c62185966372507455alepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1982" class="bibsonomy_quicknav_group"><a name="1982">1982</a></h3>
<div style="margin-bottom:1em"><b>Sprachen und Computer : Festschrift zum 75. Geburtstag von Hans Eggers, 9. Juli 1982</b>.<br/>
1982. 

<br/>


<a onclick="toggleBibtex('lepsky', '80316941ee0cab2ff05990f3ee5515ec', 'https://www.bibsonomy.org/bibtex/280316941ee0cab2ff05990f3ee5515ec/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/280316941ee0cab2ff05990f3ee5515ec/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_80316941ee0cab2ff05990f3ee5515eclepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_80316941ee0cab2ff05990f3ee5515eclepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1980" class="bibsonomy_quicknav_group"><a name="1980">1980</a></h3>
<div style="margin-bottom:1em">
<b>An algorithm for suffix stripping</b>. <br/>
<i>Program</i>, 14(3):130-137, 1980.

<br/>
Martin F. Porter.
<br/>


<a onclick="toggleBibtex('lepsky', '6af4b9d4719da38058784912d014bb15', 'https://www.bibsonomy.org/bibtex/26af4b9d4719da38058784912d014bb15/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/26af4b9d4719da38058784912d014bb15/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_6af4b9d4719da38058784912d014bb15lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_6af4b9d4719da38058784912d014bb15lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1977" class="bibsonomy_quicknav_group"><a name="1977">1977</a></h3>
<div style="margin-bottom:1em"><b>Experimentelle Morphologie in der Informationswissenschaft</b>.<br/>
1977. 
<br/>Rainer Kuhlen.
<br/>


<a onclick="toggleBibtex('lepsky', 'e05b4aad2c87f8672e7da8e76b0b9929', 'https://www.bibsonomy.org/bibtex/2e05b4aad2c87f8672e7da8e76b0b9929/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2e05b4aad2c87f8672e7da8e76b0b9929/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_e05b4aad2c87f8672e7da8e76b0b9929lepsky" style="display:none;border:1px dotted grey;">

</div>
<div style="position:relative">						
	<div id="bib_e05b4aad2c87f8672e7da8e76b0b9929lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<h3 id="bib:year-1972" class="bibsonomy_quicknav_group"><a name="1972">1972</a></h3>
<div style="margin-bottom:1em">
<b>Understanding natural language</b>. <br/>
<i>Cognitive Psychology</i>, 3(1):1-191, 1972.

<br/>
Terry Winograd.
<br/>
<a href="http://www.sciencedirect.com/science/article/pii/0010028572900023">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'd1fe18b1c77572c1b56cac574dd8b1f5'); return false;" href="https://www.bibsonomy.org/bibtex/2d1fe18b1c77572c1b56cac574dd8b1f5/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'd1fe18b1c77572c1b56cac574dd8b1f5', 'https://www.bibsonomy.org/bibtex/2d1fe18b1c77572c1b56cac574dd8b1f5/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2d1fe18b1c77572c1b56cac574dd8b1f5/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_d1fe18b1c77572c1b56cac574dd8b1f5lepsky" style="display:none;border:1px dotted grey;">
This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language—syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.
</div>
<div style="position:relative">						
	<div id="bib_d1fe18b1c77572c1b56cac574dd8b1f5lepsky" style="display:inline;position:absolute;"></div>
</div></div>

<div style="margin-bottom:1em"><b>Das Lexikon in der maschinellen Sprachanalyse</b>.<br/>
1972. 
<br/>Harald Zimmermann.
<br/>
<a href="http://scidok.sulb.uni-saarland.de/volltexte/2007/715/pdf/1972c.pdf">[doi]</a>&nbsp;
<a onclick="toggleAbstract('lepsky', 'f7af0293e12852fdd62fd62c7dda85e7'); return false;" href="https://www.bibsonomy.org/bibtex/2f7af0293e12852fdd62fd62c7dda85e7/lepsky?format=bibtex">[abstract]</a>&nbsp;
<a onclick="toggleBibtex('lepsky', 'f7af0293e12852fdd62fd62c7dda85e7', 'https://www.bibsonomy.org/bibtex/2f7af0293e12852fdd62fd62c7dda85e7/lepsky?format=bibtex'); return false;" href="https://www.bibsonomy.org/bibtex/2f7af0293e12852fdd62fd62c7dda85e7/lepsky?format=bibtex">[BibTeX]</a>&nbsp;
<div id="abs_f7af0293e12852fdd62fd62c7dda85e7lepsky" style="display:none;border:1px dotted grey;">
Zimmermanns Dissertation führt ein in die Verwendung von Lexika bei der Lösung von sprachlichen Problemen mit dem Computer. Der lexikalische Ansatz bedeutete die Abkehr von den in den frühen Projekten zur maschinellen Sprachverarbeitung (bis Mitte der 60er Jahre) verfolgten "praxisnahen" Ansätzen hin zu einer umfassenden, an der Behandlung der Lexikoneintragung in der Transformationsgrammatik orientierten Beschreibung des Sprachsystems, wozu vor Allem das Lexikon gehört. Im Einzelnen geht es um Funktion und Aufbau sowie Struktur und Verwendung des Lexikons, Abwägung von theoretischen Anforderungen und Effizienzgesichtspunkten für die praktische Anwendung, Adäquatheit der Beschreibung, automatische Wortklassifikation und Wörterbucherweiterung
</div>
<div style="position:relative">						
	<div id="bib_f7af0293e12852fdd62fd62c7dda85e7lepsky" style="display:inline;position:absolute;"></div>
</div></div>
<!-- 
	This software is distributed under a Creative Commons Attribution 3.0 License
	http://creativecommons.org/licenses/by/3.0/

	*Attribution*
	JavaScript by Mark Schenk, Dominik Benz and Michael Domhardt
	JabRef export filter and css by Michael Domhardt http://mensch-maschine-systemtechnik.de/
	BibSonomy and Typo3 integration by Dominik Benz
	Content by BibSonomy - Lesezeichen und Referenzen teilen - in blau! http://bibsonomy.org/
-->

<script type="text/javascript">
<!--
function toggleAbstract(user,hash) {
	var abs = document.getElementById('abs_'+hash+user);	
	if (abs) {
		if(abs.id.indexOf('abs_') != -1) {
			abs.style.display = ( abs.style.display == 'none' ? '' : 'none' );
		}
	} 
	return;
}

function toggleBibtex(user,hash,biburl) {
    var f = document.getElementById('bib_' + hash + user + '_src');
	if (undefined != f) {
		f.parentNode.removeChild(f);
		return;
	}
	var el = document.getElementById('bib_' + hash + user);
    iframe = document.createElement("iframe");
    iframe.setAttribute("src", biburl);
	iframe.setAttribute("id", 'bib_' + hash + user + '_src');
    iframe.style.width = 500+"px";
    iframe.style.height = 200+"px";
	iframe.style.background = "#eee";
    el.appendChild(iframe);
	return;
}
-->
</script>
